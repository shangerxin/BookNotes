Web scraping with Python 3rd=Ryan;Note=Erxin


# Web scraping 
- package 
BeautifulSoup


# web crawling models  
- package 
$ pip install Scrapy

- document 
https://doc.scrapy.org/en/latest/news.html


# store data
- media files 
- csv 
- mysql 
- email 
```
import smtplib
from email.mime.text import MIMEText

msg = MIMEText('The body of the email is here')

msg['Subject'] = 'An Email Alert'
msg['From'] = 'ryan@pythonscraping.com'
msg['To'] = 'webmaster@pythonscraping.com'

s = smtplib.SMTP('localhost')
s.send_message(msg)
s.quit()
```


# Working with dirty data 
- OpenRefine
OpenRefine is an open source project started by a company called Metaweb in 2009. Google acquired Metaweb in 2010, changing the name of the project from Freebase Gridworks to Google Refine

working with messy data: cleaning it; transforming it from one format into another


# Crawing through forms and logins 
- request library 

```
import requests

params = {'firstname': 'Ryan', 'lastname': 'Mitchell'}
r = requests.post("http://pythonscraping.com/pages/files/processing.php", data=params)
print(r.text)
```

- submit files 
```

import requests

files = {'uploadFile': open('files/python.png', 'rb')}
r = requests.post('http://pythonscraping.com/pages/files/processing2.php', 
                   files=files)
print(r.text)
```

- http basic access authentication 
http://pythonscraping.com/pages/auth/login.phpthat has this type of authentication


```
import requests
from requests.auth import AuthBase
from requests.auth import HTTPBasicAuth

auth = HTTPBasicAuth('ryan', 'password')
r = requests.post(url='https://pythonscraping.com/pages/auth/login.php', auth=auth)
print(r.text)
```


# Crawling through apis 
- REST, GraphQL, JSON, and XML APIs

- US National Weather Service provides a weather API (https://www.weather.gov/documentation/services-web-api)

- Google has dozens of APIs in its Developers section (https://console.developers.google.com)

- http methods and apis 

a web server via HTTP:

GET

POST

PUT

DELETE

- finding undocumented api with developer tool 
They often have JSON or XML in them. You can filter the list of requests by using the search/filter field.

With GET requests, the URL will contain the parameter values passed to them.


# Image processing and CAPTCHAs handling 
- Pillow
Although Pillow might not be the most fully featured image-processing library

- Tesseract ORC 

Training Tesseract
Whether you’re training for CAPTCHAs or any other text, there are a few factors to consider that greatly impact the Tesseract’s performance

Do characters overlap in the image

multiple variations of the font or style of writing

Are there any background images, lines

Is there high contrast with clear boundaries

Is the font a fairly standard serif or sans-serif font


I’ve created a project at https://github.com/REMitchell/tesseract-trainer that contains, among other things, a web app that assists in creating these box files


```
CLEANED_DIR = 'cleaned'
BOX_DIR = 'box'
EXP_DIR = 'exp'
class TesseractTrainer():
    def __init__(self, languageName, fontName, directory='data'):
        self.languageName = languageName
        self.fontName = fontName
        self.directory = directory

    def runAll(self):
        os.chdir(self.directory)
        self.createDirectories()
                self.createFontProperties()
        prefixes = self.renameFiles()
        self.createTrainingFiles(prefixes)
        self.extractUnicode()
        self.runShapeClustering()
        self.runMfTraining()
        self.runCnTraining()
        self.createTessData()
        
trainer = TesseractTrainer('captcha', 'captchaFont')
trainer.runAll()


$ tesseract -l captcha U8DG.png -
```
- numpy 




# avoding scraping traps 
- cookies 
 (http://pythonscraping.com, in this example) and calling get_cookies()
 

```
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
chrome_options = Options()
chrome_options.add_argument('--headless')
driver = webdriver.Chrome(executable_path='drivers/chromedriver',
                          options=chrome_options)
driver.get('http://pythonscraping.com')
driver.implicitly_wait(1)
print(driver.get_cookies())
```


# Test with unittest 


# Web scraping in parallel 
- Python’s global interpreter lock (or GIL) acts to prevent threads from executing the same line of code at once. The GIL ensures that the common memory shared by all processes does not become corrupted 

- example 
```
import threading
import time

def print_time(threadName, delay, iterations):
    start = int(time.time())
    for i in range(0,iterations):
    time.sleep(delay)
    print(f'{int(time.time() - start)} - {threadName}')

    threads = [
    threading.Thread(target=print_time, args=('Fizz', 3, 33)),
    threading.Thread(target=print_time, args=('Buzz', 5, 20)),
    threading.Thread(target=print_time, args=('Counter', 1, 100))
    ]

    [t.start() for t in threads]
    [t.join() for t in threads]
```

- Threading Module
The Python threading module is a higher-level interface built on the lower-level_thread module.

```
threading.Thread(target=crawler)
t.start()

while True:
    time.sleep(1)
    if not t.isAlive():
    t = threading.Thread(target=crawler)
    t.start()
```


```

import threading
import time

class Crawler(threading.Thread):
def __init__(self):
    threading.Thread.__init__(self)
    self.done = False

def isDone(self):
    return self.done

def run(self):
    time.sleep(5)
    self.done = True
    raise Exception('Something bad happened!')

    t = Crawler()
    t.start()

    while True:
        time.sleep(1)
        if t.isDone():
        print('Done')
        break
    if not t.isAlive():
        t = Crawler()
        t.start()
```

- multi processing 

```

from multiprocessing import Process
import time

def print_time(threadName, delay, iterations):
    start = int(time.time())
    for i in range(0,iterations):
        time.sleep(delay)
        seconds_elapsed = str(int(time.time()) - start)
        print (threadName if threadName else seconds_elapsed)

processes = [
    Process(target=print_time, args=('Counter', 1, 100)),
    Process(target=print_time, args=('Fizz', 3, 33)),
    Process(target=print_time, args=('Buzz', 5, 20))
]

[p.start() for p in processes]
[p.join() for p in processes]
```


# Web scraping proxies 
- Tor The Onion Router network, better known by the acronym Tor, is a network of volunteer servers set up to route and reroute traffic through many layers

- PySocks is a remarkably simple Python module that is capable of routing traffic through proxy servers and that works fantastically in conjunction with Tor

```
import socks
import socket
from urllib.request import urlopen


socks.set_default_proxy(socks.PROXY_TYPE_SOCKS5, "localhost", 9150)
socket.socket = socks.socksocket
print(urlopen('http://icanhazip.com').read())



from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

CHROMEDRIVER_PATH = ChromeDriverManager().install()
driver = webdriver.Chrome(service=Service(CHROMEDRIVER_PATH))
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--proxy-server=socks5://127.0.0.1:9150')
driver = webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=chrome_options)

driver.get('http://icanhazip.com')
print(driver.page_source)
driver.close()
```

- ScrapingBee is the smallest of the companies in this list. 

It has a strong focus on JavaScript automation, headless browsers, and innocuous-looking IP addresses.

ScrapingBee also has a Python package (https://pypi.org/project/scrapingbee/)

```
from scraper_api import ScraperAPIClient

client = ScraperAPIClient(SCRAPER_API_KEY)
start = time.time()
result = client.get('https://www.target.com/p/-/A-83650487')
print(f'Time: {time.time() - start}')
print(f'HTTP status: {response.status_code}')
print(f'Response body: {response.content}')
```

- Most small web-hosting providers come with software called cPanel, used to provide basic administration services and information about your website and related services

- CGI-script. CGI, which stands for Common Gateway Interface, is any program that can be run on a server and dynamically generate content that is displayed on a website.

- This is a Software Development Kit (SDK) that let you use various features of the API in a slightly more convenient way

```
$ pip install scrapingbee
```

- ScraperAPI, true to its name, has a mostly clean and REST-ful API with tons of features. It supports asynchronous requests

```
import requests
import time

start = time.time()
params = {
    'api_key': SCRAPER_API_KEY,
    'url': 'https://www.target.com/p/-/A-83650487'
}
response = requests.get('http://api.scraperapi.com', params=params)
print(f'Time: {time.time() - start}')
print(f'HTTP status: {response.status_code}')
print(f'Response body: {response.content}')
```

- Oxylabs is a large Lithuanian-based company with a focus on search engine results page (SERP) and product page scraping

- Zyte, formerly Scrapinghub, is another large web scraping proxy and API service company. It’s also one of the oldest, founded in 2010.

- Google Compute Engine by Marc Cohen, Kathryn Hurley, and Paul Newson (O’Reilly) is a straightforward resource on using Google Cloud Computing with both Python and JavaScript.






