Practical machine learning=Himanshu;Note=Erxin

# introduction 
- reference 
https://learning.oreilly.com/library/view/practical-machine-learning/9781484241493/


- concepts 
pixels

PPI means “pixels per inch” 

DPI means “dots per inch.” 

PPI looks more smooth whereas DPI is crispier.

Lossy compression prioritizes saving space, rather than the accuracy of the retrieved file

- image file format 

JPEG: Joint Photographic Experts Group

JPEG2000: New JPEG format developed in 2000

TIFF: Tagged Image File Format

GIF: Graphics Interchange Format

BMP: Bitmap

PNG: Portable Network Graphics

WebP: Format developed by Google

SVG: Scalable Vector Graphics

- color space 

RGB: red, green, blue

XYZ: color in the x, y, and z dimensions

HSV/HSL: hue, saturation, and value/hue, saturation, and lightness

LAB: luminance, and green–red and blue–yellow color components

LCH: lightness, chroma, and hue

YPbPr: green, blue, and red cables

YUV: brightness and chroma, or color

YIQ: luminance, in-phase parameter, and quadrature

- RGB
Using the RGB color space, red, green, and blue are mixed in different ways to make different color combinations. Good to human 

RGB colors have a threshold of saturation. They cannot go beyond what we can see. 

- The XYZ color space helps us go beyond this threshold. Now, you may wonder why we would want to go beyond the threshold. Well, it may not be possible for our human eyes to perceive certain colors, but in the digital world, you may need these colors to be used. For example, XYZ can be used for color matching;

- HSV/HSL is an alternative representation of the RGB color space. It consists of the following components:

Hue is a property that describes three colors: green, red, and magenta. It can also be a mixture of two pure colors: red and yellow, and yellow and green

Saturation measures the intensity of an image. It tells us how far a color is from gray. A lower value means the color is approaching gray.

Lightness refers to the intensity of color with respect to white. It tells us how far a color is from white.


- LAB
The LAB color space has three components:
1.	
Luminance

 
2.	
a*, which is the green and red color component

 
3.	
b*, which is the blue and yellow color component

Together a point and the distance to it has cylindrical coordinates. Anything that does not have cylindrical coordinates cannot be perceived by humans.

 
The best part about the LAB color space is that it is not device dependent; it can be used in printing, textiles, and a host of other applications. The LAB color space is one of the most exact means of representing a color


- LCH
The LCH is similar to the LAB color space, but instead of using cylindrical coordinates, it uses rectangular coordinates. This makes the coordinates similar to how our human eye sees

- YPbPr
The YPbPr color space is used in video electronics, such as DVD players. It consists of following three components:
1.	
Y: the green cable

 
2.	
Pb: the blue cable

 
3.	
Pr: the red cable

The three components are derived from the RGB color space only. Y refers to brightness; Pb and Pr are the two different color signals. 

using computers, the digital color components are derived from the RGB color space. 

However, when we talk about electronic devices (such as DVD players), we need to use the analog counterpart of the RGB color space, which is YPbPr

-YUV

The YUV color space is somewhat similar to YPbPr, because both are used in video electronics. The difference is that YUV supports black-and-white television as well

1.	Y: the luminance in an image

2.	I: the in-phase parameter
 
3.	Q: the quadrature representing the color information

 
- advance image processing 

image processing:
Bezier curve

Ellipsoid, A circle is a two-dimensional figure with a constant diameter or radius. A sphere is a three-dimensional circle that also has a constant radius or diameter

Gamma correction, Gamma correction, which is used to display an image accurately onscreen, controls the brightness of an image and can be used to change the red-to-green-to-blue ratio.

Structural Similarity Index, The Structural Similarity Index, or SSIM, is used for measuring the quality of an image. It tells how much one image is structurally similar to other

Deconvolution, In general, deconvolution is used to correct blurry images, which helps restore contrast. With blurred images, it is difficult to determine pixel intensity. To make this correction, we use what is called the point spread function (PSF).

Homography, Homography has multiple uses in image processing: the generation of mosaic and panoramic images, image stitching, image registration, image alignment, and more. 

Convolution, Convolution is a simple process during which we apply a matrix (also called a kernel or a filter) to an image so that we can downsize it, or add several padding layers to keep the size the same

- Scikit Image and Python

Uploading and Viewing an Image

Getting Image Resolution

Looking at Pixel Values

Converting Color Space

Saving an Image

Creating Basic Drawings

Doing Gamma Correction
```
from skimage import exposure
from skimage import io
from pylab import *
img = io.imread('puppy.jpg')
gamma_corrected1 = exposure.adjust_gamma(img, 0.5)
gamma_corrected2 = exposure.adjust_gamma(img, 5)
figure(0)
io.imshow(gamma_corrected1)
figure(1)
io.imshow(gamma_corrected2)
Output:

```

Rotating, Shifting, and Scaling Images

Determining Structural Similarity


```
#Import libraries
from skimage import io
from skimage import color
from skimage import data
#Read image
img = io.imread('puppy.jpg')
#Convert to XYZ
img_xyz = color.rgb2xyz(img)
#Convert back to RGB
img_rgb = color.xyz2rgb(img_xyz)
#Show both figures
figure(0)
io.imshow(img_xyz)
figure(1)
io.imshow(img_rgb)
Output:

```

- advanced image processing using opencv 

Blending two images
```
determine whether there are any changes. Let’s look at the code:
#import required packages
import cv2
#Read image 1
img1 = cv2.imread('cat_1.jpg')
#Read image 2
img2 = cv2.imread('cat_2.jpg')
#Define alpha and beta
alpha = 0.30
beta = 0.70
#Blend images
final_image = cv2.addWeighted(img1, alpha, img2, beta, 0.0)
#Show image
io.imshow(final_image)
cv2.DestroyAllWindows()# After we have clicked Close or pressed Escape, this function destroys all the windows 
```

Changing the contrast and brightness of an image
```
#import required packages
import cv2
import numpy as np
#Read image
image = cv2.imread("cat_1.jpg")
#Create a dummy image that stores different contrast and brightness
new_image = np.zeros(image.shape, image.dtype)
#Brightness and contrast parameters
contrast = 3.0
bright = 2
#Change the contrast and brightness
for y in range(image.shape[0]):
    for x in range(image.shape[1]):
        for c in range(image.shape[2]):
            new_image[y,x,c] = np.clip(contrast*image[y,x,c] + bright, 0, 255)
figure(0)
io.imshow(image)
figure(1)
io.imshow(new_image)
```

Adding text to images

Smoothing images

Changing the shape of images
```
#DILATION CODE :
#Import package
import cv2
#Read image
image = cv2.imread("cat_1.jpg")
#Define erosion size
s1 = 0
s2 = 10
s3 = 10
#Define erosion type
t1 = cv2.MORPH_RECT
t2 = cv2.MORPH_CROSS
t3 = cv2.MORPH_ELLIPSE
#Define and save the erosion template
tmp1 = cv2.getStructuringElement(t1, (2*s1 + 1, 2*s1+1), (s1, s1))
tmp2= cv2.getStructuringElement(t2, (2*s2 + 1, 2*s2+1), (s2, s2))
tmp3 = cv2.getStructuringElement(t3, (2*s3 + 1, 2*s3+1), (s3, s3))
#Apply the erosion template to the image and save in different variables
final1 = cv2.erode(image, tmp1)
final2 = cv2.erode(image, tmp2)
final3 = cv2.erode(image, tmp3)
#Show all the images with different erosions
figure(0)
io.imshow(final1)
figure(1)
io.imshow(final2)
figure(2)
io.imshow(final3)
#EROSION CODE :
#Import packages
import cv2
#Read images
image = cv2.imread("cat_1.jpg")
#Define dilation size
d1 = 0
d2 = 10
d3 = 20
#Define dilation type
t1 = cv2.MORPH_RECT
t2 = cv2.MORPH_CROSS
t3 = cv2.MORPH_ELLIPSE
#Store the dilation templates
tmp1 = cv2.getStructuringElement(t1, (2*d1 + 1, 2*d1+1), (d1, d1))
tmp2 = cv2.getStructuringElement(t2, (2*d2 + 1, 2*d2+1), (d2, d2))
tmp3 = cv2.getStructuringElement(t3, (2*d3 + 1, 2*d3+1), (d3, d3))
#Apply dilation to the images
final1 = cv2.dilate(image, tmp1)
final2 = cv2.dilate(image, tmp2)
final3 = cv2.dilate(image, tmp3)
#Show the images
figure(0)
io.imshow(final1)
figure(1)
io.imshow(final2)
figure(2)
io.imshow(final3)
```

Effecting image thresholding

Calculating gradients to detect edges

Performing histogram equalization

- image processing using machine learning 

scale-invariant feature transform (SIFT) algorithm

Image classification using convolutional neural networks (CNNs)













