Observability with Grafana=Rob Chapman;Note=Erxin


# Introducing observability and the grafana stack 
- telemetry types 

Metrics can be thought of as numeric data that is recorded at a point in time and enriched with labels 

Logs are considered to be unstructured string data types

Distributed traces show the end-to-end journey of an action. They are captured from every step that is taken to complete the action.

Profiling data (stack traces) can give us a very detailed technical view of the system’s use of resources such as CPU cycles 

- Other Grafana tools
Grafana Labs continues to be a leader in observability and has acquired several companies

Other Grafana tools
Grafana Labs continues to be a leader in observability and has acquired several companies

k6 is a load testing tool that provides both a packaged tool to run in your own infrastructure and a cloud Software as a Service

Pyroscope is a recent acquisition of Grafana Labs, joining in March 2023. Pyroscope is a tool that enable teams to engage in the continuous profiling of system resource

Grafana Labs fully embraces its history as an open source software provider. The LGTM stack


# Instrumenting applications and infrastructure 
- Common Event Format (CEF)

CEF is an open logging and auditing format from ArcSight that aims to provide a simple interface to record security-related events.

NCSA Common Log Format (CLF)

The NCSA CLF is historically used on web servers to record information about requests made to the server. This format has been extended by the CLF to include additional information about the browser (user-agent) and the referer.

W3C Extended Log File Format

W3C Extended Log File Format is a log format commonly used by Windows Internet Information Services servers (web servers).

Windows Event Log

Windows Event Log is the standard log format used by the Windows operating system. These logs record events that occur on the system and are categorized System, Application, Security, Setup, and Forwarded events.

JavaScript Object Notation (JSON)

JSON is an open standard file format that is very useful for easily parsing structured log events.

Syslog

Syslog is a standard that’s used across many hardware devices such as networking, compute, and storage, and is used by the Linux kernel for logging.

Logfmt

Logfmt does not have a defined standard but is a widely used form of human-readable structured logging.


- CEF
Developed by ArcSight to fulfill the Security Information and Event Management (SIEM) use case, the CEF is a structured text-based log forma

- Metric protocols
Metric protocols are collections of tools and libraries for instrumenting applications

StatsD 

DogStatsD 

OpenTelemetry Protocol 

Prometheus 

- Tracing, or as it is more commonly referred to, distributed tracing, tracks application requests as they are made between services of a system

The trace record is the parent object that represents the data flow or execution path through the system being observed

OTLP 

Zipkin 

Jaeger 

    + possible impacts to consider:

Increased latency
Memory overhead
Slower startup time

- common infrastructure components 

    + capacity in any area:

System temperature
CPU utilization percent
Overall disk space used and remaining
Memory usage and free memory

    + network devices 
    
Latency
Throughput
Packet loss
Bandwidth

    + powre components 
    
Power supply state
Backup power supply state
Voltage
Wattage
Current


# Grafana Cloud 


# Updating the OpenTelemetry 
- Grafana Loki was designed from the ground up to be a highly scalable multi-tenant logging solution

- LogQL
Grafana developed LogQL as the query language for Loki using the Prometheus Query Language (PromQL) for inspiratio

- Grafana Loki has a full microservices architecture that can be run as a single binary and a simple scalable deployment to a full microservices deployment 

- PromQL offers three data types, which are important, as the functions and operators in PromQL


# Grafana in Practice 
-  Grafana website: https://grafana.com/grafana/plugins/panel-plugins/. A great place to try them out and get ideas you can use in your own dashboards is https://play.grafana.org, 

- OpenTelemetry demo system architecture diagram


# Managing incidents using alerts 
- Golden signals: Golden signals were introduced in the Google SRE book, and they overlap very strongly with RED and USE


# Auotmating collection infrastructure with Helm or Ansible 
- production > collection > storage > visualize 


# Real User Monitoring with Grafana 
- RUM is the term used to describe the collection and processing of telemetry that describes the health of the frontend of your web applications.



# Application performance with grafana pyroscope and k6 
- Smoke tests

These are designed to validate that the system works. They can also be known as sanity or confidence tests. They are called smoke tests after testing a device by powering it on and checking for smoke.

These are designed to quickly say that things look as expected or that something is wrong

These should run quickly, in minutes not hours.

They should be low volume.

Average load tests

These tests show how the system is used in most conditions.

These are designed to simulate the most frequent level of load on the system.

These should run relatively quickly, but slower than smoke tests.

They should simulate average volumes of traffic.

Stress tests

These tests stress the system with higher-than-average peak traffic.

These are designed to simulate what would happen if peak traffic were experienced for an extended duration.

These should run in less than a day.

They should simulate high volumes of traffic.

Spike tests

These tests should show how the system behaves with a sudden, short, massive increase in traffic, as might be seen during a denial of service (DoS) attack.

These are designed to test how the system would handle a sudden overwhelming spike in traffic, such as a DoS attack.

These should run quickly.

They should simulate unrealistic amounts of traffic.

Breakpoint tests

These tests gradually increase traffic until the system breaks down.

These are designed to understand when the system will fail with added load.

These can run for extended periods.

They should simulate steadily increasing rates of traffic.

Soak tests

These tests assess the performance of the system over extended periods. They are like an average load test over a significantly longer period.

These are designed to demonstrate how the system will function during real operations for extended periods. They are good for identifying issues such as memory leaks.

These will run over extended periods such as 48 hours.

They should simulate average volumes of traffic.

- k6 is the load testing tool developed by Grafana Labs after they acquired LoadImpact. k6 offers several key features:


# Using Grafana for fast feedback during the development cycle 
-  life cycle:

Code: This is where new code is written in line with the specification given during the planning phase
Build: This phase is where new code is built
Test: New code is tested in various ways during this phase
Release: The code is verified as ready to be deployed to production in this phase; any final checks or assurances will be performed here
Deploy: The code is deployed to a production environment
Operate: This phase is a continuous phase; the latest deployed release is run in a production environment
Monitor: Any data collected from the release that is currently operating in production is gathered, as well as any feedback or user research, and is collated together to be used in the next planning phase
Plan: During this phase, the team plans what future iterations of the product will contain
Security: This is a continuous concern for the team in a DevSecOps approach and is the responsibility of all members of the team

- The test phase can cover a lot of different test types. While tests are typically managed by the CI/CD platform

- The operate phase is where the product is live in front of customers. The most important aspect of this phase is ensuring customers are getting a great service.

- monitor phase is the phase in which using Grafana can really shine. The two biggest challenges are knowing what telemetry to use to answer a question about the product and whether the telemetry is being made available.

- CI platforms cover a lot of different tools, such as Github Actions, GitLab CI/CD, Jenkins, Azure DevOps, Google Cloud Build, and similar.

- platforms use tools such as Jenkins, GitLab CI/CD, AWS CodeDeploy, ArgoCD, FluxCD

- monitoring these systems:

OpenTelemetry Collector contributed receivers

Prometheus modules: These are modules that allow Prometheus to scrape data from a lot of resource platforms.

- Data collection decicions 

those factors together to help you process them:

Logs:
Choose a log format that can be extended so you can deliver quickly and enhance later.
Select labels carefully, considering Loki’s performance and cardinality. You can always extract additional fields as labels during querying.
Consider whether valuable metrics can be created from logs (to maximize the value).
Metrics:
Identify important metrics, and drop what you don’t need. This can help with metric cardinality, too. If you cannot drop the whole metric, just dropping some of the highest cardinality labels can help a lot.
Choose the protocols that provide the data you need (remember, there are variations, so read the documentation for each carefully).
If using verbose metric protocols, ensure protection is in place (e.g., histogram bucketing) to restrict the ability to flood your system.
Add context so you can correlate metrics with traces.
Traces:
Ensure that the accuracy of spans and traces is implemented and validated
Balance the performance and cost impact with a mitigation strategy (sampling, filtering, and retention)
Instrumentation libraries:
Research them well. If you are using a library, you want it to be maintained and supported going forward.
Telemetry collector:
Run proofs of concept to validate what works with your technology. You don’t want to fall foul of permission constraints restricting the choice of collector on your route to production.
Consider the support model that comes with the collector technology, if any.
What are your business needs from a collector?

- Grafana cloud dashborads 

Top 10 dashboards with errors: This lets you know which dashboards are encountering errors of some form.
Top 10 data sources with errors: This reports the Grafana data sources that have issues. This is useful to diagnose errors with queries, or in communicating to the backend data source.
Top 10 users seeing errors: This identifies your platform users who are encountering problems inside Grafana. This is helpful when investigating platform stability.

- Databases created or altered on a SQL Server instance by using CONTAINMENT = PARTIAL allow the creation of database principals referred to as contained users.

- buildint roles 

db_owner

db_accessadmin. This role has the right not only to create and manage database users and custom database roles, but also to create schemas and to grant permissions on all database objects

db_backupoperator. This role has BACKUP DATABASE (including full and differential backups), BACKUP LOG, and CHECKPOINT permissions for the database.

db_datareader. This role has rights to execute a SELECT statement using any object in the database.

db_datawriter. Members of this role can execute INSERT, UPDATE, and DELETE statements on any table or view in the database. 

db_ddladmin. This role has the rights to perform DDL statements to alter objects in the database. It has no permission to create or modify permissions


- permissions 

Data Definition Language (DDL). DDL statements are used to define structures in the database, such as tables, stored procedures, or functions.

Data Manipulation Language (DML). DML statements are used to fetch data from a table or to modify the contents of a table.

    + DDL
Sample DDL statements include things like CREATE TABLE, ALTER TABLE, UPDATE STATISTICS, CREATE PROCEDURE, CREATE OR ALTER PROCEDURE

    + DLM 
following statements access and modify data in tables and are commonly used: DELETE, INSERT, BULK INSERT, MERGE, SELECT, UPDATE, TRUNCATE TABLE, EXECUTE PROCEDURE.

- DBAs should change the AUTHORIZATION databases to either a known high-level, noninteractive service account or to the built-in sa principal (SID 0x01)

-  two values:

ORIGINAL_LOGIN(). The name of the login with which you connected to the instance. This will not change even after you use EXECUTE AS USER or EXECUTE AS LOGIN.

SUSER_SNAME(). The login you are executing as. It could be the name from EXECUTE AS LOGIN or it could be the login of the user if you executed EXECUTE AS USER.

CURRENT_USER. The name of the user whose security content you have assumed. This is the equivalent of USER_NAME(). It can be either the user you have executed as or the username in the database for a login you are impersonating.

- Windows PowerShell project, which is available at http://www.dbatools.io.

- DAC connections are only allowed locally. You can use the Surface Area Configuration dialog box in the Facets section of SSMS 



# Protect data through classification, encryption and auditing 
-  General Data Protection Regulation (GDPR) came into effect in the European Union (EU). It provides for the protection of any personal data associated with data subjects (EU residents) 

- SSMS can help identify possible security vulnerabilities. Read more about it at https://learn.microsoft.com/sql/relational-databases/security/sql-vulnerability-assessment.

- Most network traffic uses these two protocols. For example:

TCP ports 1433 and 1434. SQL Server

TCP ports 2382 and 2383. SQL Server Analysis Services

TCP port 80. HTTP

TCP port 443. HTTPS

TCP port 22. Secure Shell (SSH)

UDP port 53. Domain Name System (DNS)

- You can read the IPV6 Specification, known as Internet Engineering Task Force Request For Comments #8200, at https://tools.ietf.org/html/rfc8200.

- You can read the IPV4 specification, known as Internet Engineering Task Force Request For Comments #791, at https://tools.ietf.org/html/rfc791.


- In SQL Server, the SMK is at the top of the encryption hierarchy. It is automatically generated the first time the SQL Server instance starts

- The DMK is used to protect asymmetric keys and private keys for digital certificates stored in the database. A copy of the DMK is stored in the database for which it is used as well as in the master database

-  direct-attached storage (DAS) or storage area network (SAN) as a file-system solution or at the physical storage layer

- Host Guardian Service (HGS). Install HGS in a Windows Server 2022 failover cluster with three computers in its own AD forest. These computers must not be connected to an existing AD


- Tools for client and development. Install the requisite tools on your client machine:

Microsoft .NET Data Provider for SQL Server 2.1.0 or later

SSMS (SSMS) 19.0 or later

SQL Server PowerShell module version 21.1 or later

Visual Studio 2017 or later

Developer pack (SDK) for .NET Standard 2.1 or later

Microsoft.SqlServer.Management.AlwaysEncrypted.EnclaveProviders NuGet package

Microsoft.SqlServer.Management.AlwaysEncrypted.AzureKeyVaultProvider NuGet package version 2.2.0 or later, if you plan to use Azure Key Vault for storing your column master keys

- Trusted Platform Module (TPM) enclave attestation, which is hardware-based.

Image You can read more about TPM attestation at https://learn.microsoft.com/windows-server/identity/ad-ds/manage/component-updates/tpm-key-attestation.