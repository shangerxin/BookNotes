Jenkins Document;Note=Erxin

# Reference 
https://jenkins.io/doc/

# Using jenkins 
- aborting a build 

Pipeline jobs can be stopped by sending an HTTP POST request to URL endpoints of a build.

BUILD ID URL/stop - aborts a Pipeline.

BUILD ID URL/term - forcibly terminates a build (should only be used if stop does not work).

BUILD ID URL/kill - hard kill a pipeline. This is the most destructive way to stop a pipeline and should only be used as a last resort.

- relevant projects need to be configured to Record fingerprints of the jar files

- go to jenkins_server/systemInfo and see the user.timezone system property.

- Jenkins installation sits at https://ci.jenkins.io, visiting https://ci.jenkins.io/api/ will show just the top-level API features available                   


# Using jenkins agents 
- To generate the SSH key pair, you have to execute a command line tool named ssh-keygen on a machine you have access to. It could be:



# Guide Tour
- download and run 
http://mirrors.jenkins.io/war-stable/latest/jenkins.war

Run java -jar jenkins.war --httpPort=8080.

Browse to http://localhost:8080

- Jenkins Pipeline is typically written into a text file (called a Jenkinsfile) which in turn is checked into a project’s source control repository.

https://jenkins.io/doc/book/pipeline/jenkinsfile/

pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                echo 'Building..'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing..'
            }
        }
        stage('Deploy') {
            steps {
                echo 'Deploying....'
            }
        }
    }
}

- pipeline made of multiple steps 
    + "step" like a single command which performs a single action. When a step succeeds it moves onto the next step. 

    + all the steps in the Pipeline have successfully completed, the Pipeline is considered to have successfully executed

        * linux use sh to execute shell 
pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                sh 'echo "Hello World"'
                sh '''
                    echo "Multiline shell steps works too"
                    ls -lah
                '''
            }
        }
    }
}

        * window use bat to execute shell 
pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                bat 'set'
            }
        }
    }
}

    + timeout and retries and more 
pipeline {
    agent any
    stages {
        stage('Deploy') {
            steps {
                retry(3) {
                    sh './flakey-deploy.sh'
                }

                timeout(time: 3, unit: 'MINUTES') {
                    sh './health-check.sh'
                }
            }
        }
    }
}

The "Deploy" stage retries the flakey-deploy.sh script 3 times, and then waits for up to 3 minutes for the health-check.sh script to execute. If the health check script does not complete in 3 minutes, the Pipeline will be marked as having failed in the "Deploy" stage.

- finishing up pipeline with post section 
Jenkinsfile (Declarative Pipeline)
pipeline {
    agent any
    stages {
        stage('Test') {
            steps {
                sh 'echo "Fail!"; exit 1'
            }
        }
    }
    post {
        always {
            echo 'This will always run'
        }
        success {
            echo 'This will run only if successful'
        }
        failure {
            echo 'This will run only if failed'
        }
        unstable {
            echo 'This will run only if the run was marked as unstable'
        }
        changed {
            echo 'This will run only if the state of the Pipeline has changed'
            echo 'For example, if the Pipeline was previously failing but is now successful'
        }
    }
}

- define execution environment, This approach allows you to use practically any tool which can be packaged in a Docker container.

Jenkinsfile (Declarative Pipeline)
pipeline {
    agent {
        docker { image 'node:7-alpine' }
    }
    stages {
        stage('Test') {
            steps {
                sh 'node --version'
            }
        }
    }
}

Pipeline executes, Jenkins will automatically start the specified container and execute the defined steps within it:

- environment variables 
Jenkinsfile (Declarative Pipeline)
pipeline {
    agent {
        label '!windows'
    }

    environment {
        DISABLE_AUTH = 'true'
        DB_ENGINE    = 'sqlite'
    }

    stages {
        stage('Build') {
            steps {
                echo "Database engine is ${DB_ENGINE}"
                echo "DISABLE_AUTH is ${DISABLE_AUTH}"
                sh 'printenv'
            }
        }
    }
}

- cleaning up and notifications 
pipeline {
    agent any
    stages {
        stage('No-op') {
            steps {
                sh 'ls'
            }
        }
    }
    post {
        always {
            echo 'One way or another, I have finished'
            deleteDir() /* clean up our workspace */
        }
        success {
            echo 'I succeeeded!'
        }
        unstable {
            echo 'I am unstable :/'
        }
        failure {
            echo 'I failed :('
        }
        changed {
            echo 'Things were different before...'
        }
    }
}

post {
    failure {
        mail to: 'team@example.com',
             subject: "Failed Pipeline: ${currentBuild.fullDisplayName}",
             body: "Something is wrong with ${env.BUILD_URL}"
    }
}

also support hipchat and slack message 

- deployment 
pipeline {
    agent any
    options {
        skipStagesAfterUnstable()
    }
    stages {
        stage('Build') {
            steps {
                echo 'Building'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing'
            }
        }
        stage('Deploy') {
            steps {
                echo 'Deploying'
            }
        }
    }
}

stage('Deploy - Staging') {
    steps {
        sh './deploy staging'
        sh './run-smoke-tests'
    }
}
stage('Deploy - Production') {
    steps {
        sh './deploy production'
    }
}

    + ask for human input with input step 
Jenkinsfile (Declarative Pipeline)
pipeline {
    agent any
    stages {
        /* "Build" and "Test" stages omitted */

        stage('Deploy - Staging') {
            steps {
                sh './deploy staging'
                sh './run-smoke-tests'
            }
        }

        stage('Sanity check') {
            steps {
                input "Does the staging environment look ok?"
            }
        }

        stage('Deploy - Production') {
            steps {
                sh './deploy production'
            }
        }
    }
}

- add test stage for jenkinsfile
        stage('Deliver for development') {
            when {
                branch 'development'
            }
            steps {
                sh './jenkins/scripts/deliver-for-development.sh'
                input message: 'Finished using the web site? (Click "Proceed" to continue)'
                sh './jenkins/scripts/kill.sh'
            }
        }
        stage('Deploy for production') {
            when {
                branch 'production'
            }
            steps {
                sh './jenkins/scripts/deploy-for-production.sh'
                input message: 'Finished using the web site? (Click "Proceed" to continue)'
                sh './jenkins/scripts/kill.sh'
            }
        }


# Install jenkins in docker 
- docker image 

use is the jenkinsci/blueocean image (from the Docker Hub repository)

A new jenkinsci/blueocean image is published each time a new release of Blue Ocean is published

Jenkins Docker images you can use (accessible through jenkins/jenkins on Docker Hub)
- install docker 
+ create a bridge network in docker using docker network 
$ docker network create jenkins

- Create the following volumes to share the Docker client TLS certificates needed to connect to the Docker daemon and persist the Jenkins data 

$ docker volume create jenkins-docker-certs
$ docker volume create jenkins-data

- download and run the docker:dind docker image using the following docker container run command 

$ docker container run \
  --name jenkins-docker \ 
  --rm \ 
  --detach \ 
  --privileged \ 
  --network jenkins \ 
  --network-alias docker \ 
  --env DOCKER_TLS_CERTDIR=/certs \ 
  --volume jenkins-docker-certs:/certs/client \ 
  --volume jenkins-data:/var/jenkins_home \ 
  --publish 2376:2376 \ 
  docker:dind 
 
--env Maps the /certs/client directory inside the container to a Docker volume named jenkins-docker-certs as created above.

Maps the /var/jenkins_home directory inside the container to the Docker volume named jenkins-data as created above.

    
- Download the jenkinsci/blueocean image and run it as a container in Docker
docker container run \
  --name jenkins-blueocean \ 
  --rm \ 
  --detach \ 
  --network jenkins \ 
  --env DOCKER_HOST=tcp://docker:2376 \ 
  --env DOCKER_CERT_PATH=/certs/client \
  --env DOCKER_TLS_VERIFY=1 \
  --publish 8080:8080 \ 
  --publish 50000:50000 \ 
  --volume jenkins-data:/var/jenkins_home \ 
  --volume jenkins-docker-certs:/certs/client:ro \ 
  jenkinsci/blueocean 
  
- on window 
$ docker network create jenkins
$ docker volume create jenkins-docker-certs
$ docker volume create jenkins-data
  
download docker:dind docker image using docker container run command 

$ docker container run --name jenkins-docker --rm --detach ^
  --privileged --network jenkins --network-alias docker ^
  --env DOCKER_TLS_CERTDIR=/certs ^
  --volume jenkins-docker-certs:/certs/client ^
  --volume jenkins-data:/var/jenkins_home ^
  docker:dind

download blue ocean image jenkinsci/blueocean 
docker container run --name jenkins-blueocean --rm --detach ^
  --network jenkins --env DOCKER_HOST=tcp://docker:2376 ^
  --env DOCKER_CERT_PATH=/certs/client --env DOCKER_TLS_VERIFY=1 ^
  --volume jenkins-data:/var/jenkins_home ^
  --volume jenkins-docker-certs:/certs/client:ro ^
  --publish 8080:8080 --publish 50000:50000 jenkinsci/blueocean
  
accessing jenkins/blue ocean docker container 
$ docker container exec -it jenkins-blueocean bash 

accessing the jenkins console log through docker logs 
$ docker container logs <docker-container-name>
$ docker container logs jenkins-blueocean


# WAR file
- WAR file
The Web application ARchive (WAR) file version of Jenkins can be installed on any operating system or platform that supports Java.

Download the latest stable Jenkins WAR file to an appropriate directory on your machine.

Open up a terminal/command prompt window to the download directory.

Run the command java -jar jenkins.war.

Browse to http://localhost:8080 and wait until the Unlock Jenkins page appears.

Continue on with the Post-installation setup wizard below.


# Pipeline
Jenkins pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into jenkins

- A continuous delivery (CD) pipeline is an automated expression of your process for getting software from version control right through to your users and customers. 

The definition of a Jenkins Pipeline is written into a text file (called a Jenkinsfile) which in turn can be committed to a project’s source control repository. pipeline as code, a part of the application to be version and reviewed like any other code 

- can use the Declarative Directive Generator to help you get started with configuring the directives and sections in your Declarative Pipeline.

- create a jenkinsfile and committing it to source control provides a number of immediate benefits 

automatically creates a pipeline build process for all branches and pull requests 
code review/itegration on the pipeline 
audit trail fro the pipeline 
single source of truth for the pipeline which can be viewed and edited by mutliple members of the project 

- jenkinsfile pipeline can be written in two syntax 
    + declarative 
    provides richer syntactical features over Scripted Pipeline syntax
    
    + scripted 
    
- why pipeline 
code 
durable 
pausable 
versatile 
extensible 

- concepts 
pipeline, a user-defined model of a CD, defines your entire build process, typically includes stages for building an application, testing it and then delivering it 

node is a machine to executing a pipeline 

stage, a block defines a conceptually distinct subset of tasks performed through the entire pipeline 

step, a single task fundamentally, a step tells jenkins what to do at a particular point in time. It can execute a shell use the sh step, bat for windows 

    + declarative pipeline fundamentals, jenkinsfile example 
    
pipeline {
    agent any 
    stages {
        stage('Build') { 
            steps {
                // 
            }
        }
        stage('Test') { 
            steps {
                // 
            }
        }
        stage('Deploy') { 
            steps {
                // 
            }
        }
    }
}

follow the same rules as Groovy’s syntax with the following exceptions
top level pipeline must be pipeline {}
no semicolons 
block must only consist of sections, directives, steps or assignment statement 

	agent is Declarative Pipeline-specific syntax that instructs Jenkins to allocate an executor (on a node) and workspace for the entire Pipeline.

    + scripted pipeline fundamentals 
    
    Schedules the steps contained within the block to run by adding an item to the Jenkins queue. As soon as an executor is free on a node, the steps will run
    
    Creates a workspace (a directory specific to that particular Pipeline) where work can be done on files checked out from source control
    
node {  
    stage('Build') { 
        // 
    }
    stage('Test') { 
        // 
    }
    stage('Deploy') { 
        // 
    }
}

blocks in a Scripted Pipeline provides clearer visualization of each `stage’s subset of tasks/steps in the Jenkins UI.

- pipeline example 
pipeline { 
    agent any 
    options {
        skipStagesAfterUnstable()
    }
    stages {
        stage('Build') { 
            steps { 
                sh 'make' 
            }
        }
        stage('Test'){
            steps {
                sh 'make check'
                junit 'reports/**/*.xml' 
            }
        }
        stage('Deploy') {
            steps {
                sh 'make publish'
            }
        }
    }
}

agent is Declarative Pipeline-specific syntax that instructs Jenkins to allocate an executor (on a node) and workspace for the entire Pipeline.

https://plugins.jenkins.io/workflow-durable-task-step

https://plugins.jenkins.io/pipeline-stage-view

- Sections

agent, specifies where the entire pipeline or specific stage will execute in the jenkins environment 

    + top level agents, outer most level of the pipeline, options are invoked after entering the agent, when using timeout 

node("myAgent") {
    timeout(unit: 'SECONDS', time: 5) {
        stage("One"){
            sleep 10
            echo 'hello'
        }
    }
}

    + stage agents 
    
agents declared within a stage, the options are invoked before entering the agent and before checking any when conditions.

    + agent paremeters, agent section supports a few different types of parameters.
    
any, none, label, node, docker, dockerfile, kubernetes 

agent any

agent none

agent { label 'my-defined-label' }

agent { node { label 'labelName' } },  node allows for additional options (such as customWorkspace).

agent {
    docker {
        image 'maven:3.8.1-adoptopenjdk-11'
        label 'my-defined-label'
        args  '-v /tmp:/tmp'
        registryUrl 'https://myregistry.com/'
        registryCredentialsId 'myPredefinedCredentialsInJenkins'
    }
}


//Execute the Pipeline, or stage, with a container built from a Dockerfile contained in the source repository. In order to use this option, the Jenkinsfile must be loaded from either a Multibranch Pipeline or a Pipeline from SCM. 
agent {
    // Equivalent to "docker build -f Dockerfile.build --build-arg version=1.0.2 ./build/
    dockerfile {
        filename 'Dockerfile.build'
        dir 'build'
        label 'my-defined-label'
        additionalBuildArgs  '--build-arg version=1.0.2'
        args '-v /tmp:/tmp'
        registryUrl 'https://myregistry.com/'
        registryCredentialsId 'myPredefinedCredentialsInJenkins'
    }
}

//a pod with a Kaniko container inside it, you would define it as follows
agent {
    kubernetes {
        defaultContainer 'kaniko'
        yaml '''
kind: Pod
spec:
  containers:
  - name: kaniko
    image: gcr.io/kaniko-project/executor:debug
    imagePullPolicy: Always
    command:
    - sleep
    args:
    - 99d
    volumeMounts:
      - name: aws-secret
        mountPath: /root/.aws/
      - name: docker-registry-config
        mountPath: /kaniko/.docker
  volumes:
    - name: aws-secret
      secret:
        secretName: aws-secret
    - name: docker-registry-config
      configMap:
        name: docker-registry-config
'''
   }
//This secret should contain the contents of ~/.aws/credentials.
{
      "credHelpers": {
        "<your-aws-account-id>.dkr.ecr.eu-central-1.amazonaws.com": "ecr-login"
      }
}

- common options 

label, a string on which to run the pipeline, valid for node, docker and dockerfile 

customWorkspace, custom workspace will be under the workspace root on the node, or an absolute path, valid for node, docker, and dockerfile.
agent {
    node {
        label 'my-defined-label'
        customWorkspace '/some/other/path'
    }
}

args, Runtime arguments to pass to docker run., valid for docker and dockerfile 
pipeline {
    agent { docker 'maven:3.8.1-adoptopenjdk-11' } 
    stages {
        stage('Example Build') {
            steps {
                sh 'mvn -B clean verify'
            }
        }
        //Stage-level Agent Section
        stage('Example Test') {
            agent { docker 'openjdk:8-jre' } 
            steps {
                echo 'Hello, JDK'
                sh 'java -version'
            }
        }
    }
}

agent none at the top-level of the Pipeline ensures that an Executor will not be assigned unnecessarily. Using agent none also forces each stage section to contain its own agent section.

- The post section defines one or more additional steps that are run upon the completion of a Pipeline’s or stage’s run (depending on the location of the post section within the Pipeline). post can support any of the following 

post-condition blocks: always, changed, fixed, regression, aborted, failure, success, unstable, unsuccessful, and cleanup.

changed, Only run the steps in post if the current Pipeline’s or stage’s run has a different completion status from its previous run.

fixed, Only run the steps in post if the current Pipeline’s or stage’s run is successful and the previous run failed or was unstable.

regression, Only run the steps in post if the current Pipeline’s or stage’s run’s status is failure, unstable, or aborted and the previous run was successful.

failure, Only run the steps in post if the current Pipeline’s or stage’s run has a "failed" status, typically denoted by red in the web UI.

unstable Only run the steps in post if the current Pipeline’s or stage’s run has an "unstable" status, usually caused by test failures, code violations, etc. This is typically denoted by yellow in the web UI.

unsuccessful, Only run the steps in post if the current Pipeline’s or stage’s run has not a "success" status

pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                echo 'Hello World'
            }
        }
    }
    post { 
        always { 
            echo 'I will always say Hello again!'
        }
    }
}

- stages, a sequence of one or more stage directives, the stages section is where the bulk of the "work" described by a Pipeline, the stages section will typically follow the directives such as agent, options, etc.


pipeline {
    agent any
    stages { 
        stage('Example') {
            steps {
                echo 'Hello World'
            }
        }
    }
}

- steps, The steps section defines a series of one or more steps to be executed

pipeline {
    agent any
    stages {
        stage('Example') {
            steps { 
                echo 'Hello World'
            }
        }
    }
}

- The environment directive specifies a sequence of key-value pairs which will be defined as environment variables for all steps

This directive supports a special helper method credentials() which can be used to access pre-defined Credentials, Inside the pipeline block, or within stage directives.

pipeline {
    agent any
    environment { 
        CC = 'clang'
    }
    stages {
        stage('Example') {
            environment { 
                AN_ACCESS_KEY = credentials('my-predefined-secret-text') 
            }
            steps {
                sh 'printenv'
            }
        }
    }
}

An environment directive used in the top-level pipeline block will apply to all steps within the Pipeline.

An environment directive defined within a stage will only apply the given environment variables to steps
    
pipeline {
    agent any
    stages {
        stage('Example Username/Password') {
            environment {
                SERVICE_CREDS = credentials('my-predefined-username-password')
            }
            steps {
                sh 'echo "Service user is $SERVICE_CREDS_USR"'
                sh 'echo "Service password is $SERVICE_CREDS_PSW"'
                sh 'curl -u $SERVICE_CREDS https://myservice.example.com'
            }
        }
        stage('Example SSH Username with private key') {
            environment {
                SSH_CREDS = credentials('my-predefined-ssh-creds')
            }
            steps {
                sh 'echo "SSH private key is located at $SSH_CREDS"'
                sh 'echo "SSH user is $SSH_CREDS_USR"'
                sh 'echo "SSH passphrase is $SSH_CREDS_PSW"'
            }
        }
    }
}

- options, The options directive allows configuring Pipeline-specific options from within the Pipeline itself.

buildDiscarder, persist artifacts and console output for specific number of runs 
options { buildDiscarder(logRotator(numToKeepStr: '1')) }

checkoutToSubdirectory Perform the automatic source control checkout in a subdirectory of the workspace. 

disableConcurrentBuilds Disallow concurrent executions of the Pipeline. 

disableResume Do not allow the pipeline to resume if the controller restarts.

overrideIndexTriggers Allows overriding default treatment of branch indexing triggers. 

preserveStashes Preserve stashes from completed builds, for use with stage restarting.

quietPeriod Set the quiet period, in seconds, for the Pipeline, overriding the global default. 

retry On failure, retry the entire Pipeline the specified number of times
options { retry(3) }

skipDefaultCheckout Skip checking out code from source control by default in the agent directive.
options { skipDefaultCheckout() }

skipStagesAfterUnstable Skip stages once the build status has gone to UNSTABLE.

timeout Set a timeout period for the Pipeline run, after which Jenkins should abort the Pipeline. For example: options { timeout(time: 1, unit: 'HOURS') }

timestamps Prepend all console output generated by the Pipeline run with the time at which the line was emitted. For example: options { timestamps() }

parallelsAlwaysFailFast Set failfast true for all subsequent parallel stages in the pipeline.

    + stage options,  similar to the options directive at the root of the Pipeline. However, the stage-level options can only contain steps like retry, timeout, or timestamps.  

- parameters directive provides a list of parameters that a user should provide when triggering the Pipeline. parameters are made available to Pipeline steps via the params object 

string A parameter of a string type, for example: 
parameters { string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') }

text A text parameter, which can contain multiple lines, for example: 
parameters { text(name: 'DEPLOY_TEXT', defaultValue: 'One\nTwo\nThree\n', description: '') }

booleanParam A boolean parameter, for example: 
parameters { booleanParam(name: 'DEBUG_BUILD', defaultValue: true, description: '') }

choice A choice parameter, for example: 
parameters { choice(name: 'CHOICES', choices: ['one', 'two', 'three'], description: '') }

password  A password parameter, for example: 
parameters { password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'A secret password') }
 
 
pipeline {
    agent any
    parameters {
        string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?')

        text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person')

        booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value')

        choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something')

        password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password')
    }
    stages {
        stage('Example') {
            steps {
                echo "Hello ${params.PERSON}"

                echo "Biography: ${params.BIOGRAPHY}"

                echo "Toggle: ${params.TOGGLE}"

                echo "Choice: ${params.CHOICE}"

                echo "Password: ${params.PASSWORD}"
            }
        }
    }
}

- triggers, webhooks-based integration will likely already be present. The triggers currently available are cron, pollSCM and upstream.

    + acron
a cron-style string to define a regular interval, triggers { cron('H */4 * * 1-5') }

    + pollSCM 
Accepts a cron-style string to define a regular interval at which Jenkins should check for new source changes.
triggers { pollSCM('H */4 * * 1-5') }
    
    + upstream 
Accepts a comma-separated string of jobs and a threshold. 
triggers { upstream(upstreamProjects: 'job1,job2', threshold: hudson.model.Result.SUCCESS) }

- cron time string is five values 

[ * (MINUTE TIME FORMAT) ] [ * (HOUR TIME FORMAT) ] [ * (DAY OF THE MONTH TIME FORMAT) ] [ * (REPRESENT MONTH TIME FORMAT) ] [ * (DAY OF WEEK TIME FORMAT) ] [ PATH OF THE SCRIPT OR JOBS or APPLICATION ]

    + examples 
    
//run automatically every next 7 minutes 
*/7 * * * * /root/time.sh

//every 4 hour 
* */4 * * * /root/time.sh

//weekly 
@weekly /root/time.sh

* specifies all valid values

M-N specifies a range of values

M-N/X or */X steps by intervals of X through the specified range or whole valid range

A,B,…​,Z enumerates multiple values

- The stage directive goes in the stages section and should contain a steps section, an optional agent section
// Declarative //
pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                echo 'Hello World'
            }
        }
    }
}

- tool A section defining tools to auto-install and put on the PATH. This is ignored if agent none is specified.

pipeline {
    agent any
    tools {
        maven 'apache-maven-3.0.1' 
    }
    stages {
        stage('Example') {
            steps {
                sh 'mvn --version'
            }
        }
    }
}

tool name must be pre-configured in Jenkins under Manage Jenkins → Global Tool Configuration.

- input, The input directive on a stage allows you to prompt for input, using the input step. will be available in the environment for the rest of the stage

pipeline {
    agent any
    stages {
        stage('Example') {
            input {
                message "Should we continue?"
                ok "Yes, we should."
                submitter "alice,bob"
                parameters {
                    string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?')
                }
            }
            steps {
                echo "Hello, ${PERSON}, nice to meet you."
            }
        }
    }
}

- The when directive allows the Pipeline to determine whether the stage should be executed depending on the given condition. 

nesting conditions: not, allOf, or anyOf. Nesting conditions may be nested to any arbitrary depth.

    + built-in condition 
    
the branch being built matches the branch pattern, when { branch 'master' }. Note that this only works on a multibranch Pipeline.
when { branch pattern: "release-\\d+", comparator: "REGEXP"}

build is building a tag. Example: when { buildingTag() }

changelog Execute the stage if the build’s SCM changelog contains a given regular expression pattern
when { changelog '.*^\\[DEPENDENCY\\] .+$' }

changeset Execute the stage if the build’s SCM changeset contains one or more files matching the given pattern. 
when { changeset pattern: ".TEST\\.java", comparator: "REGEXP" } or when { changeset pattern: "*/*TEST.java", caseSensitive: true }

changeRequest, executes the stage if the current build is for a "change request" (a.k.a. Pull Request on GitHub and Bitbucket 
when { changeRequest() }. Possible attributes are id, target, branch, fork, url, title, author, authorDisplayName, and authorEmail
when { changeRequest target: 'master' }.
when { changeRequest authorEmail: "[\\w_-.]+@example.com", comparator: 'REGEXP' }

environment, when { environment name: 'DEPLOY_TO', value: 'production' }

equals, the expected value is equal to the actual value, for example: when { equals expected: 2, actual: currentBuild.number }

expression, when the specified Groovy expression evaluates to true, for example
when { expression { return params.DEBUG_BUILD } }

tag Execute the stage if the TAG_NAME variable matches the given pattern.  when { tag "release-*" }. If an empty pattern is provided the stage will execute if the TAG_NAME variable exists (same as buildingTag()).
when { tag pattern: "release-\\d+", comparator: "REGEXP"}

not,  Execute the stage when the nested condition is false. 
when { not { branch 'master' } }

allOf Execute the stage when all of the nested conditions are true

anyOf Execute the stage when at least one of the nested conditions is true.

triggeredBy Execute the stage when the current build has been triggered by the param given
when { triggeredBy 'SCMTrigger' }
when { triggeredBy 'TimerTrigger' }
when { triggeredBy 'BuildUpstreamCause' }
when { triggeredBy cause: "UserIdCause", detail: "vlinde" }

beforeAgent evaluating when before entering agent in a stage by specifying the beforeAgent option within the when block. If beforeAgent is set to true 

beforeInput the when condition for a stage will not be evaluated before the input, if one is defined. However, this can be changed by specifying the beforeInput option

beforeOptions the when condition for a stage will be evaluated after entering the options for that stage, if any are defined

    + when examples 
pipeline {
    agent any
    stages {
        stage('Example Build') {
            steps {
                echo 'Hello World'
            }
        }
        stage('Example Deploy') {
            when {
                allOf {
                    branch 'production'
                    environment name: 'DEPLOY_TO', value: 'production'
                }
            }
            steps {
                echo 'Deploying'
            }
        }
    }
}

pipeline {
    agent any
    stages {
        stage('Example Build') {
            steps {
                echo 'Hello World'
            }
        }
        stage('Example Deploy') {
            when {
                branch 'production'
                anyOf {
                    environment name: 'DEPLOY_TO', value: 'production'
                    environment name: 'DEPLOY_TO', value: 'staging'
                }
            }
            steps {
                echo 'Deploying'
            }
        }
    }
}

pipeline {
    agent any
    stages {
        stage('Example Build') {
            steps {
                echo 'Hello World'
            }
        }
        stage('Example Deploy') {
            when {
                expression { BRANCH_NAME ==~ /(production|staging)/ }
                anyOf {
                    environment name: 'DEPLOY_TO', value: 'production'
                    environment name: 'DEPLOY_TO', value: 'staging'
                }
            }
            steps {
                echo 'Deploying'
            }
        }
    }
}

- sequential stages, a stages section containing a list of nested stages to be run in sequential order. a stage directive within a parallel or matrix block can use all other functionality of a stage, including agent, tools, when, etc.

pipeline {
    agent none
    stages {
        stage('Non-Sequential Stage') {
            agent {
                label 'for-non-sequential'
            }
            steps {
                echo "On Non-Sequential Stage"
            }
        }
        stage('Sequential') {
            agent {
                label 'for-sequential'
            }
            environment {
                FOR_SEQUENTIAL = "some-value"
            }
            stages {
                stage('In Sequential 1') {
                    steps {
                        echo "In Sequential 1"
                    }
                }
                stage('In Sequential 2') {
                    steps {
                        echo "In Sequential 2"
                    }
                }
                stage('Parallel In Sequential') {
                    parallel {
                        stage('In Parallel 1') {
                            steps {
                                echo "In Parallel 1"
                            }
                        }
                        stage('In Parallel 2') {
                            steps {
                                echo "In Parallel 2"
                            }
                        }
                    }
                }
            }
        }
    }
}

- parallel, a parallel section containing a list of nested stages to be run in parallel. Note that a stage must have one and only one of steps, stages, parallel, or matrix. However, a stage directive within a parallel or matrix block can use all other functionality of a stage, including agent, tools, when, etc. 

you can force your parallel stages to all be aborted when any one of them fails, by adding failFast true to the stage containing the parallel. or an option to the pipeline definition: parallelsAlwaysFailFast() in the pipeline. 

pipeline {
    agent any
    stages {
        stage('Non-Parallel Stage') {
            steps {
                echo 'This stage will be executed first.'
            }
        }
        stage('Parallel Stage') {
            when {
                branch 'master'
            }
            failFast true
            parallel {
                stage('Branch A') {
                    agent {
                        label "for-branch-a"
                    }
                    steps {
                        echo "On Branch A"
                    }
                }
                stage('Branch B') {
                    agent {
                        label "for-branch-b"
                    }
                    steps {
                        echo "On Branch B"
                    }
                }
                stage('Branch C') {
                    agent {
                        label "for-branch-c"
                    }
                    stages {
                        stage('Nested 1') {
                            steps {
                                echo "In stage Nested 1 within Branch C"
                            }
                        }
                        stage('Nested 2') {
                            steps {
                                echo "In stage Nested 2 within Branch C"
                            }
                        }
                    }
                }
            }
        }
    }
}

pipeline {
    agent any
    options {
        parallelsAlwaysFailFast()
    }
    stages {
        stage('Non-Parallel Stage') {
            steps {
                echo 'This stage will be executed first.'
            }
        }
        stage('Parallel Stage') {
            when {
                branch 'master'
            }
            parallel {
                stage('Branch A') {
                    agent {
                        label "for-branch-a"
                    }
                    steps {
                        echo "On Branch A"
                    }
                }
                stage('Branch B') {
                    agent {
                        label "for-branch-b"
                    }
                    steps {
                        echo "On Branch B"
                    }
                }
                stage('Branch C') {
                    agent {
                        label "for-branch-c"
                    }
                    stages {
                        stage('Nested 1') {
                            steps {
                                echo "In stage Nested 1 within Branch C"
                            }
                        }
                        stage('Nested 2') {
                            steps {
                                echo "In stage Nested 2 within Branch C"
                            }
                        }
                    }
                }
            }
        }
    }
}

- Matrix stages in Declarative Pipeline may have a matrix section defining a multi-dimensional matrix of name-value combinations to be run in parallel.

"cells" in a matrix. Each cell in a matrix can include one or more stages to be run sequentially using the configuration for that cell. must have only one of steps, stages, parallel, or matrix. parallel and matrix are not allow to nest. a stage in a parallel or matrix can use all other functionality. 

failFast, parallelsAlwaysFailFast()

The matrix section must include an axes section and a stages section. he axes section defines the values for each axis in the matrix. The stages section defines a list of stages to run sequentially. n excludes section to remove invalid cells from the matrix 

    + axes, Each axis consists of a name and a list of values
    
matrix {
    axes {
        axis {
            name 'PLATFORM'
            values 'linux', 'mac', 'windows'
        }
        axis {
            name 'BROWSER'
            values 'chrome', 'edge', 'firefox', 'safari'
        }
    }
    stages {
        stage('build-and-test') {
            // ...
        }
    }
    //optional 
    excludes {
        exclude {
            axis {
                name 'PLATFORM'
                notValues 'mac'
            }
            axis {
                name 'ARCHITECTURE'
                values '32-bit', '64-bit'
            }
        }
    }
}

exclude axis directives can use notValues instead of values. These will exclude cells that do not match one of the values passed to notValues.

    + Matrix cell-level directives, optional. The axis and exclude directives define the static set of cells that make up the matrix. 

agent

environment

input

options

post

tools

when

    + matrix example 
pipeline {
    parameters {
        choice(name: 'PLATFORM_FILTER', choices: ['all', 'linux', 'windows', 'mac'], description: 'Run on specific platform')
    }
    agent none
    stages {
        stage('BuildAndTest') {
            matrix {
                agent {
                    label "${PLATFORM}-agent"
                }
                when { anyOf {
                    expression { params.PLATFORM_FILTER == 'all' }
                    expression { params.PLATFORM_FILTER == env.PLATFORM }
                } }
                axes {
                    axis {
                        name 'PLATFORM'
                        values 'linux', 'windows', 'mac'
                    }
                    axis {
                        name 'BROWSER'
                        values 'firefox', 'chrome', 'safari', 'edge'
                    }
                }
                excludes {
                    exclude {
                        axis {
                            name 'PLATFORM'
                            values 'linux'
                        }
                        axis {
                            name 'BROWSER'
                            values 'safari'
                        }
                    }
                    exclude {
                        axis {
                            name 'PLATFORM'
                            notValues 'windows'
                        }
                        axis {
                            name 'BROWSER'
                            values 'edge'
                        }
                    }
                }
                stages {
                    stage('Build') {
                        steps {
                            echo "Do Build for ${PLATFORM} - ${BROWSER}"
                        }
                    }
                    stage('Test') {
                        steps {
                            echo "Do Test for ${PLATFORM} - ${BROWSER}"
                        }
                    }
                }
            }
        }
    }
}

- Steps, Declarative Pipelines may use all the available steps documented in the Pipeline Steps reference

- The script step takes a block of Scripted Pipeline and executes that in the Declarative Pipeline

pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                echo 'Hello World'

                script {
                    def browsers = ['chrome', 'firefox']
                    for (int i = 0; i < browsers.size(); ++i) {
                        echo "Testing the ${browsers[i]} browser"
                    }
                }
            }
        }
    }
}

- flow control 

node {
    stage('Example') {
        if (env.BRANCH_NAME == 'master') {
            echo 'I only execute on the master branch'
        } else {
            echo 'I execute elsewhere'
        }
    }
}

node {
    stage('Example') {
        try {
            sh 'exit 1'
        }
        catch (exc) {
            echo 'Something failed, I should sound the klaxons!'
            throw
        }
    }
}

- Scripted Pipeline must serialize data back to the controller. Due to this design requirement, some Groovy idioms such as collection.each { item → /* perform operation */ } are not fully supported.




## Getting started with pipeline 
- define a pipeline 
    + through blue ocean, after setting up a pipeline in blue ocean, it will helps you create pipeline's jenkinsfile into source control 
    
    + through the classic UI 
    Jenkins > New Item > Pipeline > enter your pipeline code into the script text area 
    
    + In SCM, Choose pipeline script from SCM option, Jenkins can then check out your Jenkinsfile from source control as part of your Pipeline project’s build process and then proceed to execute your Pipeline.
    
    create pipeline from classic UI till the step 5. Definition field, choose the Pipeline script from SCM option
    
    In the script path field, specify the location of your Jenkinsfile. The default value of this field assumes that your jenkinsfile is named Jenkinsfile and located at the root of the repository 

    if your IDE is not correctly syntax highlighting your Jenkinsfile, try inserting the line #!/usr/bin/env groovy at the top of the Jenkinsfile
    
- The built-in documentation can be found globally at ${YOUR_JENKINS_URL}/pipeline-syntax
- Select the desired step in the Sample Step dropdown menu
    + ${YOUR_JENKINS_URL}/pipeline-syntax.
    + desired step in the Sample Step dropdown menu
    + Generate Pipeline Script to create a snippet of Pipeline 
    
- Global variable reference 
env, environment variables,  reference at ${YOUR_JENKINS_URL}/pipeline-syntax/globals
params, all aprameters read-only Map, for example: params.MY_PARAM_NAME
currentBuild,  such as currentBuild.result, currentBuild.displayName
scm, represents the configuration in a multibranch build 
complete global variable list, ${YOUR_JENKINS_URL}/pipeline-syntax/globals for a complete,



- declarative directive generator
    + directly to ${YOUR_JENKINS_URL}/directive-generator.
    + Click Generate Directive to create the directive’s configuration to copy into your Pipeline

- a Jenkins Pipeline is written into a text file (called a Jenkinsfile) which in turn can be committed to a project’s source control repository    

Automatically creates a Pipeline build process for all branches and pull requests.

Code review/iteration on the Pipeline (along with the remaining source code).

Audit trail for the Pipeline.

Single source of truth [3] for the Pipeline, which can be viewed and edited by multiple members of the project.

- the Blue Ocean UI helps you set up your Pipeline project, and automatically creates and writes your Pipeline (i.e. the Jenkinsfile) for you through the graphical Pipeline editor.

- The built-in "Snippet Generator" utility is helpful for creating bits of code for individual steps,






# Pipeline example 
- reference 
https://jenkins.io/doc/pipeline/examples/
- ansi color build wrapper 
- rchive build output 
- artifactory generic upload download 
- artifactory maven build 
- configure provider plugin 
- external workspace manager 
- get build cause 
- gitcommit 
- gitcommit_changeset 
- ircnotify commandline 
- jobs in parallel 
- load from file 
- maven and jdk specific version 
- parallel from grep 
- parallel from list 
- parallel multiple nodes 
- push git repo 
- slacknotify 
- timestamper wrapper 
- trigger job on all nodes 
- unstash different dir


# Blue Ocean 
Blue Ocean is a new project that rethinks the user experience of Jenkins. Designed from the ground up for Jenkins Pipeline and compatible with Freestyle jobs

- sophisticated visualization of CD pipelines 
- pipeline editor 
- personalization UI 
- pinpoint precision 
- native integration for branch and pull requests 

- Jenkins is installed on most platforms, the Blue Ocean plugin and all its other dependent plugins (which form the Blue Ocean "suite of plugins") are not installed by default. blueocean is part of docker jenkins image https://hub.docker.com/

- Alternatively, you can access Blue Ocean directly by appending /blue to the end of your Jenkins server’s URL - e.g. https://jenkins-server-url/blue


# Using Credentials 
- credentials stored in jenkins 
Secret text - a token such as an API token (e.g. a GitHub personal access token),

Username and password - which could be handled as separate components or as a colon separated string in the format username:password (read more about this in Handling credentials),

Secret file - which is essentially secret content in a file,

SSH Username with private key - an SSH public/private key pair,

Certificate - a PKCS#12 certificate file and optional password, or

Docker Host Certificate Authentication credentials.

- adding new global credentials 

System, click the Global credentials (unrestricted) link to access this default domain.


# Using a jenkinsfile 
- benefits 

Code review/iteration on the Pipeline

Audit trail for the Pipeline

Single source of truth [2] for the Pipeline, which can be viewed and edited by multiple members of the project

- example jenkinsfile 

Jenkinsfile (Declarative Pipeline)
pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                echo 'Building..'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing..'
            }
        }
        stage('Deploy') {
            steps {
                echo 'Deploying....'
            }
        }
    }
}

-  create a new Jenkinsfile in the root directory of the project.

Jenkinsfile (Scripted Pipeline)
node {
    checkout scm 
    /* .. snip .. */
}

The checkout step will checkout code from source control;

- build, *nix can use bash, window can use batch  

pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                sh 'make' 
                archiveArtifacts artifacts: '**/target/*.jar', fingerprint: true 
            }
        }
    }
}

- test 

Jenkinsfile (Declarative Pipeline)
pipeline {
    agent any

    stages {
        stage('Test') {
            steps {
                /* `make check` returns non-zero on test failures,
                * using `true` to allow the Pipeline to continue nonetheless
                */
                sh 'make check || true' 
                junit '**/target/*.xml' 
            }
        }
    }
}

sh step always sees a zero exit code, giving the junit step the opportunity to capture and process the test reports

- deploy, Deployment can imply a variety of steps, depending on the project or organization requirements

pipeline {
    agent any

    stages {
        stage('Deploy') {
            when {
              expression {
                currentBuild.result == null || currentBuild.result == 'SUCCESS' 
              }
            }
            steps {
                sh 'make publish'
            }
        }
    }
}

- using environment variable 

Pipeline is documented at ${YOUR_JENKINS_URL}/pipeline-syntax/globals#env 

- setting environment variables dynamically, run time and can be used by shell scripts (sh), Windows batch scripts (bat) and PowerShell scripts (powershell). Each script can either returnStatus or returnStdout. 

pipeline {
    agent any 
    environment {
        // Using returnStdout
        CC = """${sh(
                returnStdout: true,
                script: 'echo "clang"'
            )}""" 
        // Using returnStatus
        EXIT_STATUS = """${sh(
                returnStatus: true,
                script: 'exit 1'
            )}"""
    }
    stages {
        stage('Example') {
            environment {
                DEBUG_FLAGS = '-g'
            }
            steps {
                sh 'printenv'
            }
        }
    }
}

- handling credentials 

Jenkins' declarative Pipeline syntax has the credentials() helper method (used within the environment directive) which supports secret text, username and password, as well as secret file credentials. 

respective credential IDs jenkins-aws-secret-key-id and jenkins-aws-secret-access-key.

Jenkinsfile (Declarative Pipeline)
pipeline {
    agent {
        // Define agent details here
    }
    environment {
        AWS_ACCESS_KEY_ID     = credentials('jenkins-aws-secret-key-id')
        AWS_SECRET_ACCESS_KEY = credentials('jenkins-aws-secret-access-key')
    }
    stages {
        stage('Example stage 1') {
            steps {
                // 
            }
        }
        stage('Example stage 2') {
            steps {
                // 
            }
        }
    }
}

Any sensitive information in credential IDs themselves (such as usernames) are also returned as “****” in the Pipeline run’s outpu

stage’s steps using the syntax $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY.

environment {
    BITBUCKET_COMMON_CREDS = credentials('jenkins-bitbucket-common-creds')
}

BITBUCKET_COMMON_CREDS - contains a username and a password separated by a colon in the format username:password.
BITBUCKET_COMMON_CREDS_USR - an additional variable containing the username component only.
BITBUCKET_COMMON_CREDS_PSW - an additional variable containing the password component only.

- jenkins pipeline 

pipeline {
    agent {
        // Define agent details here
    }
    stages {
        stage('Example stage 1') {
            environment {
                BITBUCKET_COMMON_CREDS = credentials('jenkins-bitbucket-common-creds')
            }
            steps {
                // 
            }
        }
        stage('Example stage 2') {
            steps {
                // 
            }
        }
    }
}

this stage’s steps and can be referenced using the syntax:
$BITBUCKET_COMMON_CREDS
$BITBUCKET_COMMON_CREDS_USR
$BITBUCKET_COMMON_CREDS_PSW

- secret files, uploaded to Jenkins. Secret files are used for credentials that are:

pipeline {
    agent {
        // Define agent details here
    }
    environment {
        // The MY_KUBECONFIG environment variable will be assigned
        // the value of a temporary file.  For example:
        //   /home/user/.jenkins/workspace/cred_test@tmp/secretFiles/546a5cf3-9b56-4165-a0fd-19e2afe6b31f/kubeconfig.txt
        MY_KUBECONFIG = credentials('my-kubeconfig')
    }
    stages {
        stage('Example stage 1') {
            steps {
                sh("kubectl --kubeconfig $MY_KUBECONFIG get pods")
            }
        }
    }
}

- other credential types 

Bindings, click Add and choose from the dropdown

SSH User Private Key - to handle SSH public/private key pair credentials

Certificate - to handle PKCS#12 certificates, from which you can specify

Keystore Variable

SSH User Private Key example

withCredentials(bindings: [sshUserPrivateKey(credentialsId: 'jenkins-ssh-key-for-abc', \
                                             keyFileVariable: 'SSH_KEY_FOR_ABC', \
                                             passphraseVariable: '', \
                                             usernameVariable: '')]) {
  // some block
}

    + jenkins file 
    
pipeline {
    agent {
        // define agent details
    }
    stages {
        stage('Example stage 1') {
            steps {
                withCredentials(bindings: [sshUserPrivateKey(credentialsId: 'jenkins-ssh-key-for-abc', \
                                                             keyFileVariable: 'SSH_KEY_FOR_ABC')]) {
                  // 
                }
                withCredentials(bindings: [certificate(credentialsId: 'jenkins-certificate-for-xyz', \
                                                       keystoreVariable: 'CERTIFICATE_FOR_XYZ', \
                                                       passwordVariable: 'XYZ-CERTIFICATE-PASSWORD')]) {
                  // 
                }
            }
        }
        stage('Example stage 2') {
            steps {
                // 
            }
        }
    }
}

node {
  withCredentials([string(credentialsId: 'mytoken', variable: 'TOKEN')]) {
    sh /* WRONG! */ """
      set +x
      curl -H 'Token: $TOKEN' https://some.api/
    """
    sh /* CORRECT */ '''
      set +x
      curl -H 'Token: $TOKEN' https://some.api/
    '''
  }
}

- string interpolation 

    + Groovy string interpolation should never be used with credentials.

    + interpolation with environment variable 
    
    sh("curl -u ${EXAMPLE_CREDS_USR}:${EXAMPLE_CREDS_PSW} https://example.com/")

- handling parameters, Declarative Pipeline supports parameters out-of-the-box, allowing the Pipeline to accept user-specified parameters at runtime

pipeline {
    agent any
    parameters {
        string(name: 'Greeting', defaultValue: 'Hello', description: 'How should I greet the world?')
    }
    stages {
        stage('Example') {
            steps {
                echo "${params.Greeting} World!"
            }
        }
    }
}

- handling failure 

a number of different "post conditions" such as: always, unstable, success, failure, and changed

pipeline {
    agent none
    stages {
        stage('Build') {
            agent any
            steps {
                checkout scm
                sh 'make'
                stash includes: '**/target/*.jar', name: 'app' 
            }
        }
        stage('Test on Linux') {
            agent { 
                label 'linux'
            }
            steps {
                unstash 'app' 
                sh 'make check'
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
        stage('Test on Windows') {
            agent {
                label 'windows'
            }
            steps {
                unstash 'app'
                bat 'make check' 
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
    }
}

- using multiple agents 

The stash step allows capturing files matching an inclusion pattern (**/target/*.jar) 
    
pipeline {
    agent none
    stages {
        stage('Build') {
            agent any
            steps {
                checkout scm
                sh 'make'
                stash includes: '**/target/*.jar', name: 'app' 
            }
        }
        stage('Test on Linux') {
            agent { 
                label 'linux'
            }
            steps {
                unstash 'app' 
                sh 'make check'
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
        stage('Test on Windows') {
            agent {
                label 'windows'
            }
            steps {
                unstash 'app'
                bat 'make check' 
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
    }
}

Once pipelines are completed its execution, stashed files are deleted from the Jenkins controller.

unstash will retrieve the named "stash" from the Jenkins controller into the Pipeline’s current workspace.

bat allow execute windows batch command 
    
- optional step arguments 

uses the syntax [key1: value1, key2: value2]. Making statements like the following functionally equivalent:








