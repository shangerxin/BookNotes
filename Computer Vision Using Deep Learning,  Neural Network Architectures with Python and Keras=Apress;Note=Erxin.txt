Computer Vision Using Deep Learning: Neural Network Architectures with Python and Keras=Apress;Note=Erxin


# Introduction 
- color detection using opencv 
```
import numpy as np
import cv2

image = cv2.imread('Color.png')

#covnert to HSV (Hue Saturation Value) format. It enables us to separate from saturation and pseudo-illumination. 

hsv_convert = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# define the upper and lower ranges of the color 
lower_range = np.array([110,50,50]) 
upper_range = np.array([130,255,255])

mask_toput = cv2.inRange(hsv_convert, lower_range, upper_range) 

cv2.imshow('image', image) cv2.imshow('mask', mask_toput)

while(True):
    k = cv2.waitKey(5)& 0xFF if k== 27: break


```

- shape detection 

```
import numpy as np
import cv2

shape_image = cv2.imread('shape.png')

gray_image = cv2.cvtColor(shape_image, cv2.COLOR_BGR2GRAY) 
ret,thresh = cv2.threshold(gray_image,127,255,1)

contours,h = cv2.findContours(thresh,1,2)


#  Then we decide the shape based on the number of elements in the contours.

for cnt in contours:
    approx = cv2.approxPolyDP(cnt,0.01*cv2.arcLength(cnt,True),True)
    print (len(approx))

if len(approx)==3:
    print ("triangle")
    cv2.drawContours(shape_image,[cnt],0,(0,255,0),-1)
    
elif len(approx)==4:
    print ("square")
    cv2.drawContours(shape_image,[cnt],0,(0,0,255),-1)
    
elif len(approx) > 15:
    print ("circle")
    cv2.drawContours(shape_image,[cnt],0,(0,255,255),-1)
    
cv2.imshow('shape_image',shape_image)   cv2.waitKey(0)
cv2.destroyAllWindows()

```

- face detection using opencv 

```
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')

img = cv2.imread('Vaibhav.jpg')

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)


faces = face_cascade.detectMultiScale(gray, 1.3, 5) for (x,y,w,h) in faces:
image = cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2) 
roi_gr = gray[y:y+h, x:x+w] 
roi_clr = img[y:y+h, x:x+w]
the_eyes = eye_cascade.detectMultiScale(roi_gr)

for (ex,ey,ew,eh) in the_eyes:   
    cv2.rectangle(roi_clr,(ex,ey),(ex+ew,ey+eh),(0,255,0),2) 
    cv2.imshow('img',image) cv2.waitKey(0) 
    cv2.destroyAllWindows()
```

- deep learning 

data discovery > data calibration > modeling > delivery 

- Artificial Neural Networks (ANNs) are said to be inspired by the way a human brain works

- layers 

inputs 

hidden layers 

output layers 

- neuron 

we have to initiate a few parameters for the process of training the network. They are referred to as hyperparameters 

- Hyperparameters

Hyperparameters are those variables and attributes which an Artificial Neural Network cannot learn by itself.

Tuning the hyperparameters is the process of choosing the best value for the hyperparameters based on its performance

- Bias is just like adding an intercept value to a linear equation.

y = mx + c

- activation functions  

    + Sigmoid function, [0,1]
    
    a sigmoid function is as shown in Equation 1-1.
    
    $$ S(x)=\frac{1}{1+{e}^{-x}}=\frac{e^x}{e^x+1} $$
    
        + positives 
        (1)Nonlinear

        (2)Easy to work with

        (3)Continuous differentiable

        (4)Monotonic and does not blow up the activations
    
        + challenges 
        
        (1) Output not zero centered

        (2) Problem of vanishing gradients

        (3) Is slow to train
    
    + tanh function, [-1, 1]
        + positive 
        
        (1)Similar to the sigmoid function

        (2)Gradient is stronger but is preferred over sigmoid
    
        + challenges 
        
        (1)Problem of vanishing gradient

    A tanh function can be represented as Figure 1-12 and can be seen in Equation 1-2.
    
    $$ Tanh(x)=\frac{e^x-{e}^{-x}}{e^x+{e}^{-x}} $$
    
    
    + most popular activation function – ReLU. Rectified Linear Unit or ReLU, [0, inf]
    
    ReLU is a simple function, least expensive to compute, and can be trained much faster. It is unbounded and not centered at zero.
    
    F(x) = max (0, x) i.e. will give output as x if positive else 0  
    
        + postive 
        
        (1)Not linear

        (2)Easy to compute and hence fast to train

        + challenges 
        
        (1)Used only in the hidden layers

        (2)Can blow up the activations

        (3)For the x<0 region, the gradient will be zero. Hence, weights do not get updated (dying ReLU problem)
    
    
    + Leaky ReLU, max(0, x)
        + positive 
        (1)A variant of ReLU

        (2)Fixes dying ReLU problem

        + challenges 
        
        (1)Cannot be used for complex classifications
    
    + ELU, [0, inf]
    
        + positive
        (1)Alternative to ReLU

        (2)The output is smoother
        
        + challenges 
        (1)Can blow up the activations
    
    + The softmax function is used in the final layer of the Neural Network to generate the output from the network., RANGE is calculates probabilities 
    
    The softmax function calculates the probabilities for each of the target classes over all the possibilities. It is an activation function that is useful for multiclass classification problems. output the sum of 1.
    
- Learning rate will directly impact the amount of time it will take for the network to converge
   
- Backpropagation,  backwards as compared to forward propagation where the information flows from left to right.
   
- whereas on the testing/validation dataset, the accuracy drops. This is called overfitting.

    + Batch normalization and Dropout are two other techniques to mitigate the problem of overfitting.
    
    process of normalizing the output from a layer with zero mean and a standard deviation of one.
    
    + Dropout is another technique to fight the problem of overfitting
    
- Gradient descent, Gradient descent is used to find the global minimum or global maximum of a function. 

we have Stochastic Gradient Descent. the coefficients are updated for each training instance, and hence it takes less time.

- Loss is the measure of our model’s accuracy. In simple terms

    + Cross entropy, classification 
    -y(log(p) + (1-y) log(1-p))
    
    + Hinge loss, Classification
    
    max(0, 1- y *f(x))
    
    + Absolute error, Regression
	
    |y - f(x)|
    
    + Squared error, Regression

    (y - f(x))2
    
    + Huber loss, Regression
    L𝛿 = ½ (y - f(x))2 , if |y-f(x) <= 𝛿

    else 𝛿|y-f(x)| - ½ 𝛿2

- libraries 

    + tensorflow 
    
```
import tensorflow as tf

a = tf.constant([1,1,1,1])
b = tf.constant([2,2,2,2])

product_results = tf.multiply(a, b)

print(product_results)
```

    + keras 
    
    + pytorch 
    
    + sonnet 
    
    + MXNet 
    
    + matlab 
    
- Convolutional Neural Network or CNN is able to replicate this astounding capability using Deep Learning.    

CNNs are very powerful in image classification, object detection, object tracking, image captioning, face recognition, and so on.


# Nuts and Bots of Deep Learning for Computer Vision 
- TensorFlow (TF) is a platform for Machine Learning by Google. Keras is a framework developed on top of other DL toolkits like TF, Theano, CNTK, and so on.

- tensor, Recall scalars and vectors from your high-school mathematics. Vectors can be visualized as scalars with a direction.

In terms of a mathematical definition, a tensor is an object that can provide a linear mapping between two algebraic objects. These objects can themselves be scalars or vectors or even tensors.

- CNN 

a filter is just a matrix with values called weights. These weights are trained and updated during the model training process

dimensions of the input image are (n,n) and the dimensions of filter are (x,x).

So, the output after the CNN layer is ((n-x+1), (n-x+1)).

padding, we add some pixels to an image which is getting processed.

- pooling layer

A Pooling Layer is added after the Convolutional Layer.

A pooling layer is generally applied after the convolutional layer. A pooling layer with 3x3 pixels and a stride diminishes feature maps’ size by a factor of 2 which means that each dimension is halved

- Fully Connected Layer, a horse, a fully connected layer will have high-level features like tail, legs,

- Deep Learning solution using CNN. The “Hello World” of Deep Learning is generally called an MNIST dataset.

- the steps to followed 
```
from keras.models import Sequential
from keras.layers import Conv2D,Activation,MaxPooling2D,Dense,Flatten,Dropout
import numpy as np

catDogImageclassifier = Sequential()
catDogImageclassifier.add(Conv2D(32,(3,3),input_shape=(64,64,3)))
catDogImageclassifier.add(Activation('relu'))
catDogImageclassifier.add(MaxPooling2D(pool_size =(2,2)))

catDogImageclassifier.add(Conv2D(32,(3,3))) 
catDogImageclassifier.add(Activation('relu')) 
catDogImageclassifier.add(MaxPooling2D(pool_size =(2,2))) 

catDogImageclassifier.add(Conv2D(32,(3,3))) 
catDogImageclassifier.add(Activation('relu')) 
catDogImageclassifier.add(MaxPooling2D(pool_size =(2,2))) 

catDogImageclassifier.add(Conv2D(32,(3,3)))
catDogImageclassifier.add(Activation('relu')) 
catDogImageclassifier.add(MaxPooling2D(pool_size =(2,2)))
catDogImageclassifier.add(Flatten())

catDogImageclassifier.add(Dense(64)) 
catDogImageclassifier.add(Activation('relu'))

catDogImageclassifier.add(Dropout(0.5))
catDogImageclassifier.add(Dense(1))
catDogImageclassifier.add(Activation('sigmoid'))
catDogImageclassifier.summary()


catDogImageclassifier.compile(optimizer ='rmsprop', loss ='binary_crossentropy',metrics =['accuracy'])

from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale =1./255, shear_range =0.25, zoom_range = 0.25, horizontal_flip =True)
test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('/Users/DogsCats/train',target_size=(64,6 4),batch_size= 32,class_mode='binary')

test_set = test_datagen.flow_from_directory('/Users/DogsCats/test', target_size = (64,64),
batch_size = 32,
class_mode ='binary')

from IPython.display import display
from PIL import Image 

catDogImageclassifier.fit_generator(training_set, 
steps_per_epoch =625, 
epochs = 10, 
validation_data = test_set, 
validation_steps = 1000)

# Steps per epoch are 625, and the number of epochs is 10. If we have 1000 images and a batch size of 10, the number of steps required will be 100 (1000/10).

#The model will be saved as an HDF5 file, and it can be reused later.
catDogImageclassifier.save('catdog_cnn_model.h5')


# load_model:
from keras.models import load_model
catDogImageclassifier = load_model('catdog_cnn_model.h5')


# Load the library and the image from the folder. You would have to change the location of the file in code snippet below.

import numpy as np
from keras.preprocessing import image

an_image = image.load_img('/Users/vaibhavverdhan/BookWriting/2.jpg',target_size =(64,64))

#getting converted to an array of numbers:
an_image =image.img_to_array(an_image)

# expanded array shape.
an_image =np.expand_dims(an_image, axis =0)

# check the corresponding accuracy.
verdict = catDogImageclassifier.predict(an_image) 

if verdict[0][0] >= 0.5:
    prediction = 'dog'
else:
    prediction = 'cat'
```

- creating image classification models using LeNet 

Each image can be represented by height, width, and the number of channels or number of channels, height, and width. 

- MNIST classification using LeNet 
```
import keras
from keras.optimizers import SGD
from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn import datasets
from keras import backend as K
import matplotlib.pyplot as plt
import numpy as np

from keras.datasets import mnist ## Data set is imported here
from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dense
from keras import backend as K

image_rows, image_cols = 28, 28 batch_size = 256
num_classes = 10
epochs = 10

(x_train, y_train), (x_test, y_test) = mnist.load_data()
Convert the image data to float and then normalize it.
x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255
x_test /= 255


print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

if K.image_data_format() == 'channels_first':
   x_train = x_train.reshape(x_train.shape[0], 1, image_rows, image_cols)
   x_test = x_test.reshape(x_test.shape[0], 1, image_rows, image_cols)
   input_shape = (1, image_rows, image_cols)
else:
   x_train = x_train.reshape(x_train.shape[0], image_rows, image_cols, 1)
   x_test = x_test.reshape(x_test.shape[0], image_rows, image_cols, 1)
   input_shape = (image_rows, image_cols, 1)
   
model = Sequential()
model.add(Conv2D(20, (5, 5), padding="same",input_shape=input_shape))
model.add(Activation("relu")) 
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Conv2D(50, (5, 5), padding="same")) 
model.add(Activation("relu")) 
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Flatten()) 
model.add(Dense(500)) 
model.add(Activation("relu"))

model.add(Dense(num_classes)) 
model.add(Activation("softmax"))
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])

theLeNetModel = model.fit(x_train, y_train, batch_size=batch_size,
epochs=epochs,
verbose=1, validation_data=(x_test, y_test))

#output statistic data 
import matplotlib.pyplot as plt

f, ax = plt.subplots()
ax.plot([None] + theLeNetModel.history['acc'], 'o-') 
ax.plot([None] + theLeNetModel.history['val_acc'], 'x-') 
ax.legend(['Train acc', 'Validation acc'], loc = 0) 
ax.set_title('Training/Validation acc per Epoch') 
ax.set_xlabel('Epoch')
ax.set_ylabel('acc')

import matplotlib.pyplot as plt f,
ax = plt.subplots()
ax.plot([None] + theLeNetModel.history['loss'], 'o-') ax.plot([None] + theLeNetModel.history['val_loss'], 'x-') ax.legend(['Train loss', 'Validation loss'], loc = 0) ax.set_title('Training/Validation loss per Epoch') ax.set_xlabel('Epoch')
ax.set_ylabel('acc')

```



# Image Classification suing LeNet
- deep learning architectures, The most popular architectures are LeNet-5, AlexNet, VGGNet, GoogLeNet, ResNet, R-CNN (Region-based CNN), YOLO (You Only Look Once), SqueezeNet, SegNet, GAN (Generative Adversarial Network), and so on. 

- LeNet is the first architecture we are discussing in the book. It is one of the simpler CNN architectures. 

a few forms – LeNet-1, LeNet-4, and LeNet-5, which is the most cited and celebrated one.

    + architecture, LeNet-1 
    
First layer: 28x28 input image

Second layer: Four 24x24 Convolutional Layer (5x5 size)

Third layer: Average pooling layer (2×2 size)

Fourth layer: Eight 12×12 convolutional layer (5×5 size)

Fifth layer: Average pooling layers (2×2 size)

    + LeNet-4 
    
First layer: 32x32 input image

Second layer: Four 24x24 Convolutional Layer (5x5 size)

Third layer: Average pooling layer (2×2 size)

Fourth layer: Sixteen 12×12 convolutional layer (5×5 size)

Fifth layer: Average pooling layers (2×2 size)

    + LeNet-5, quoted architecture is LeNet-5, and it is the one which is generally used while solving the business problems
    
First layer: The first layer of LeNet-5 is a 32x32 input image layer. It is a grayscale image that passes through a convolutional block with six filters of size 5x5. The resulting dimensions are 28x28x1 from 32x32x1. Here, 1 represents the channel; it’s 1 because it’s a grayscale image. If it was RGB, it would have been three channels.

Second layer: The pooling layer also called the subsampling layer has a filter size of 2x2 and a stride of 2. Image dimensions are reduced to 14x14x6.

Third layer: This is again a Convolutional Layer with 16 feature maps, a size of 5x5, and a stride of 1. Note that in this layer, only 10 out of 16 feature maps are connected to 6 feature maps of the previous layer. 

Fourth layer: It is a pooling layer with a filter size of 2x2 and a stride of 2 with output as 5x5x16.

Fifth layer: It is a Fully Connected Convolutional layer with 120 feature maps and size being 1x1 each. Each of the 120 units is connected to 400 nodes in the previous layer.

Sixth layer: It is a Fully Connected layer with 84 units.

Finally, the output layer is a softmax layer with ten possible values corresponding to each digit.

        * table 
        
layer       operation          feature map      input size      kernel size         stride      activation function 
input                           1               32x32    
1           convolutional       6               28x28           5x2                 1           tanh 
2           average pooling     6               14x14           2x2                 2           tanh 
3           convolution         16              10x10           5x2                 1           tanh 
4           average pooling     16              5x5             2x2                 2           tanh 
5           convolution         120             1x1             5x2                 1           tanh 
6           fully connected                     84                                              tanh 
output      fully connected                     10                 


- Generate traffic sign identification using LeNet 
```
import keras
from keras.optimizers import SGD
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn import datasets
from keras import backend as K
import matplotlib.pyplot as plt
import numpy as np

from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dense
from keras import backend as K

import glob
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import random
import matplotlib.image as mpimg
import cv2
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.utils import shuffle
import warnings
from skimage import exposure
# Load pickled data
import pickle
%matplotlib inline matplotlib.style.use('ggplot')
%config InlineBackend.figure_format = 'retina'

# downloaded from Kaggle at www.kaggle.com/ meowmeowmeowmeowmeow/gtsrb-german-traffic-sign.
training_file = "train.p"
testing_file = "test.p"

with open(training_file, mode="rb") as f: train = pickle.load(f)
with open(testing_file, mode="rb") as f: test = pickle.load(f)

X, y = train['features'], train['labels']
x_train, x_valid, y_train, y_valid = train_test_split(X, y, stratify=y,
test_size=4000, random_state=0)
x_test,y_test=test['features'],test['labels']

figure, axiss = plt.subplots(2,5, figsize=(15, 4))
figure.subplots_adjust(hspace = .2, wspace=.001)
axiss = axiss.ravel()
for i in range(10):
    index = random.randint(0, len(x_train))
    image = x_train[index]
    axiss[i].axis('off')
    axiss[i].imshow(image)
    axiss[i].set_title( y_train[index])
    
# check the performance with different values of epochs. The same is true for batch size too.
image_rows, image_cols = 32, 32
batch_size = 256
num_classes = 43
epochs = 10

histogram, the_bins = np.histogram(y_train, bins=num_classes) the_width = 0.7 * (the_bins[1] - the_bins[0])
center = (the_bins[:-1] + the_bins[1:]) / 2
plt.bar(center, histogram, align="center", width=the_width) 
plt.show()

train_hist, train_bins = np.histogram(y_train, bins=num_classes)
test_hist, test_bins = np.histogram(y_test, bins=num_classes)
train_width = 0.7 * (train_bins[1] - train_ bins[0])
train_center = (train_bins[:-1] + train_bins[1:]) / 2
test_width = 0.7 * (test_ bins[1] - test_bins[0])
test_center = (test_ bins[:-1] + test_bins[1:]) / 2

plt.bar(train_center, train_hist, align="center", color="red", width=train_width)
plt.bar(test_center, test_hist, align="center", color="green", width=test_width)
plt.show()

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_ train.shape[0], 'train samples')
print(x_test. shape[0], 'test samples')

y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes)

if K.image_data_format() == 'channels_first':
    input_shape = (1, image_rows, image_cols)
else:
    input_shape = (image_rows, image_cols, 1) 
    
model = Sequential() model.add(Conv2D(16,(3,3),input_shape=(32,32,3)))

model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Conv2D(50, (5, 5), padding="same")) 
model.add(Activation("relu")) 
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Flatten()) 
model.add(Dense(500)) model.add(Activation("relu"))
model.add(Dense(num_classes)) 
model.add(Activation("softmax"))
model.summary()

model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
theLeNetModel = model.fit(x_train, y_train, batch_size=batch_size,
epochs=epochs,
verbose=1,
validation_data=(x_test, y_test))


# We will first plot the training and testing accuracy for the network.
import matplotlib.pyplot as plt
f, ax = plt.subplots()
ax.plot([None] + theLeNetModel.history['acc'], 'o-')
ax.plot([None] + theLeNetModel. history['val_acc'], 'x-')
ax.legend(['Train acc', 'Validation acc'], loc = 0)
ax.set_ title('Training/Validation acc per Epoch')
ax.set_xlabel('Epoch')
ax.set_ylabel('acc')

import matplotlib.pyplot as plt
f, ax = plt.subplots()
ax.plot([None] + theLeNetModel.history['loss'], 'o-')
ax.plot([None] + theLeNetModel. history['val_loss'], 'x-')
ax.legend(['Train loss', 'Validation loss'], loc = 0)
ax.set_ title('Training/Validation loss per Epoch')
ax.set_xlabel('Epoch')
ax.set_ylabel('acc')

predictions = theLeNetModel.model.predict(x_test)

from sklearn.metrics import confusion_matrix import numpy as np
confusion = confusion_matrix(y_test, np.argmax(predictions,axis=1))

cm = confusion_matrix(y_test, np.argmax(predictions,axis=1))

import seaborn as sn
df_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16})# font size

def plot_confusion_matrix(cm):
    cm = [row/sum(row) for row in cm]
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot(111)
    cax = ax.matshow(cm, cmap=plt.cm.Oranges) fig.colorbar(cax)
    plt.title('Confusion Matrix') plt.xlabel('Predicted Class IDs') plt.ylabel('True Class IDs')
    plt.show()
    
plot_confusion_matrix(cm)
```


# VGGNet and AlexNet Networks 
- AlexNet 

paper can be accessed at https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.

- activation functions 

layer           Operation           Feature map             Input Size          Kernel Size             Stride          Activation 
Input           Image               1                       227x227x3   
1               Convolution         96                      55x55x96            11x11                   4               ReLU
                Max Pooling         96                      27x27x96            3x3                     2               ReLU 
1               Convolution         256                     27x27x256           5x5                     1               ReLU 
                Max Pooling         256                     13x13x256           3x3                     2               ReLU 
3               Convolution         384                     13x13x384           3x3                     1               ReLU 
4               Convolution         384                     13x13x384           3x3                     1               ReLU
5               Convolution         256                     13x13x256           3x3                     1               ReLU
                Max Pooling         256                     6x6x256             3x3                     2               ReLU 
6               Fully Connected                             9216                                                        ReLU 
7               Fully Connected                             4096                                                        ReLU 
8               Fully Connected                             4096                                                        ReLU 
                Output                                      1000                                                        Softmax 

The inventors used data augmentation and dropout layers to fight overfitting in the network.

- VGGNet is a CNN architecture proposed by Karen Simonyan and Andrew Zisserman at the University of Oxford

VGG Neural Network model: VGG16 and VGG19.

It uses only 3x3 convolution and 2x2 pooling throughout the network. 

The convolutional layers use a very small kernel size (3x3).

There are 1x1 convolutions to linearly transform the input.

The stride is 1 pixel, and it helps to preserve the spatial resolution.

ReLU is used throughout all the hidden layers.

There are three Fully Connected layers with the first two layers having 4096 channels and the last one having 1000 channels. Finally we have a softmax layer 

- VGG19 is slightly more complex than VGG16. The difference between the two is being studied next.

VGG-16 contain 16 layers, VGG-19 contains 19 layers 

VGG-19 can be used for richer categories as hight as 1000 classes 

VGG-16 preferred for small dataset 

- developing solutions using AlexNet and VGG 

The CIFAR dataset can be accessed at www.cs.toronto.edu/~kriz/cifar.html.

it has 100 classes containing 600 images. each comes with a fine label and a coarse label 

    + using AlexNet
```
import keras
from keras.datasets import cifar10
from keras import backend as K
from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, MaxPooling2D
from keras.models import Model
from keras.layers import concatenate,Dropout,Flatten

from keras import optimizers,regularizers
from keras.preprocessing.image import ImageDataGenerator
from keras.initializers import he_normal
from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint


# preprocess the images by getting the mean and standard deviation and then standardizing them.
mean = np.mean(x_train,axis=(0,1,2,3))
std = np.std(x_train, axis=(0, 1, 2, 3))
x_train = (x_train-mean)/(std+1e-7)
x_test = (x_test-mean)/(std+1e-7)

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

fig = plt.figure(figsize=(18, 8))
columns = 5
rows = 5
for i in range(1, columns*rows + 1):
   fig.add_subplot(rows, columns, i)
   plt.imshow(X_train[i], interpolation="lanczos")


ef alexnet(img_input,classes=10):
    xnet = Conv2D(96,(11,11),strides=(4,4),padding='same',activation='relu',kernel_initializer='uniform')(img_input)
    xnet = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same',data_format=DATA_ FORMAT)(xnet)
    xnet = Conv2D(256,(5,5),strides=(1,1),padding='same', activation="relu",kernel_initializer='uniform')(xnet)
    xnet = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same',data_format=DATA_ FORMAT)(xnet)
    xnet = Conv2D(384,(3,3),strides=(1,1),padding='same', activation="relu",kernel_initializer='uniform')(xnet)
    xnet = Conv2D(384,(3,3),strides=(1,1),padding='same', activation="relu",kernel_initializer='uniform')(xnet)
    xnet = Conv2D(256,(3,3),strides=(1,1),padding='same', activation="relu",kernel_initializer='uniform')(xnet)
    xnet = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same',data_format=DATA_ FORMAT)(xnet)
    xnet = Flatten()(xnet)
    xnet = Dense(4096,activation='relu')(xnet)
    xnet = Dropout(0.25)(xnet)
    xnet = Dense(4096,activation='relu')(xnet)
    xnet = Dropout(0.25)(xnet)
    out_model = Dense(classes, activation="softmax")(xnet)
    return out_model
    
img_input=Input(shape=(32,32,3))
output = alexnet(img_input)
model=Model(img_input,output)
model.summary()

# using Stochastic Gradient Descent (SGD) here with a learning rate of 0.01 and a momentum of 0.8.
sgd = optimizers.SGD(lr=.01, momentum=0.8, nesterov=True) 
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# Using checkpoint, we can save the model only when the accuracy has increased and not with each subsequent epoch. 
filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor="val_acc", verbose=1, save_best_only=True, mode="max")
callbacks_list = [checkpoint]
epochs = 50

datagen = ImageDataGenerator(horizontal_flip=True, width_shift_range=0.115, height_shift_range=0.115, fill_mode="constant",cval=0.)
datagen.fit(x_train)

model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size), steps_per_epoch=iterations,
epochs=epochs,
callbacks=callbacks_list,
validation_data=(x_test, y_test))

import matplotlib.pyplot as plt
f, ax = plt.subplots()
ax.plot([None] + model.history.history['acc'], 'o-') ax.plot([None] + model.history.history['val_acc'], 'x-') ax.legend(['Train acc', 'Validation acc'], loc = 0)
ax.set_title('Training/Validation acc per Epoch')
ax.set_xlabel('Epoch')
ax.set_ylabel('acc')

import matplotlib.pyplot as plt
f, ax = plt.subplots()
ax.plot([None] + model.history.history['loss'], 'o-') ax.plot([None] + model.history.history['val_loss'], 'x-')
ax.legend(['Train loss', 'Validation loss'], loc = 0) ax.set_title('Training/Validation loss per Epoch') ax.set_xlabel('Epoch')
ax.set_ylabel('acc')

predictions = model.predict(x_test)

from sklearn.metrics import confusion_matrix import numpy as np
confusion_matrix(y_test, np.argmax(predictions,axis=1))

rounded_labels=np.argmax(y_test, axis=1) rounded_labels[1]
cm = confusion_matrix(rounded_labels, np.argmax(predictions,axis=1))


def plot_confusion_matrix(cm):
   cm = [row/sum(row) for row in cm]
   fig = plt.figure(figsize=(10, 10))
   ax = fig.add_subplot(111)
   cax = ax.matshow(cm, cmap=plt.cm.Oranges) fig.colorbar(cax)
   plt.title('Confusion Matrix') plt.xlabel('Predicted Class IDs')    plt.ylabel('True Class IDs')
   plt.show()
   plot_confusion_matrix(cm)

```

we used AlexNet to work on the CIFAR-10 dataset, and we got 74.80% validation accuracy

    + using VGG 
```
import keras
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint
from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras import optimizers
import numpy as np
from keras.layers.core import Lambda
from keras import backend as K
from keras import regularizers
import matplotlib.pyplot as plt
import warnings warnings.filterwarnings("ignore")

# hyperparameters 
number_classes = 10
wght_decay = 0.00005
x_shape = [32,32,3]
batch_size = 64
maxepoches = 30
learning_rate = 0.1
learning_decay = 1e-6
learning_drop = 20

# generate a few image 
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
fig = plt.figure(figsize=(18, 8))
columns = 5
rows = 5
for i in range(1, columns*rows + 1):
   fig.add_subplot(rows, columns, i)
   plt.imshow(x_train[i], interpolation="lanczos")
   
mean = np.mean(x_train,axis=(0,1,2,3))
std = np.std(x_train, axis=(0, 1, 2, 3))
x_train = (x_train-mean)/(std+1e-7)
x_test = (x_test-mean)/(std+1e-7)
y_train = keras.utils.to_categorical(y_train, number_classes)
y_test = keras.utils.to_categorical(y_test, number_classes)

# create vgg mode 
model = Sequential()
model.add(Conv2D(64, (3, 3), padding="same",
input_shape=x_shape,kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(64, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(128, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(256, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(256, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(512, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(512, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(512, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(512, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(512, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(512, (3, 3), padding="same",kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.5))
model.add(Flatten()) model.add(Dense(512,kernel_regularizer=regularizers.l2(wght_decay))) model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5)) model.add(Dense(number_classes)) model.add(Activation('softmax'))

model.summary()

# fit the model
image_augm = ImageDataGenerator( featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, rotation_range=12, width_shift_range=0.2, height_shift_range=0.1, horizontal_flip=True, vertical_flip=False)
image_augm.fit(x_train)
sgd = optimizers.SGD(lr=learning_rate, decay=learning_decay, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])
filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor="val_acc", verbose=1,
save_best_only=True, mode="max")
callbacks_list = [checkpoint]
trained_model = model.fit_generator(image_augm.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0]//batch_size,
epochs=maxepoches,
validation_data=(x_test, y_test),callbacks=callbacks_list,verbose=1)


```

- training accuracy to be higher than testing accuracy

Keras to generate Deep Learning solutions, there are two modes, namely, training and testing. During the testing phase, dropout or L1/L2 weight regularizations are turned off.

it is done using the model and is done at the end of the epoch; hence, the testing loss is lower.

hence the computed loss in initial batches is mostly higher than the final ones.




# Object Detection Using Deep Learning 
- a few architectures for Object Detection like R-CNN, Fast R-CNN, Faster R-CNN, SSD (Single Shot MultiBox Detector), and YOLO (You Only Look Once).

- object detection methods 

    + Image segmentation using simple attributes like shape, size, and color of an object.
    
    + We can use an aggregated channel feature (ACF), which is a variation of channel features. instead of calculate rectangle in different scale, it extracts features directly as pixel values.
    
    + Viola-Jones algorithm can be used for face detection.
    
    + other solutions like RANSAC (random sample consensus), Haar feature–based cascade classifier, SVM classification using HOG features

- Deep learning commonly being used for object detection 

R-CNN: Regions with CNN features. It combines Regional Proposals with CNN.
 

Fast R-CNN: A Fast Region–based Convolutional Neural Network.

Faster R-CNN: Object detection networks on Region Proposal algorithms to hypothesize object locations.
 
Mask R-CNN: This network extends Faster R-CNN by adding the prediction of segmentation masks on each region of interest.

YOLO: You Only Look Once architecture. It proposes a single Neural Network to predict bounding boxes and class probabilities from an image in a single evaluation.

SSD: Single Shot MultiBox Detector. It presents a model to predict objects in images using a single deep Neural Network.

- deep learning framework for object detection 

Sliding window approach for Object Detection

Bounding box approach

Intersection over Union (IoU)

Non-max suppression

Anchor boxes concept

- Sliding window approach for Object Detection
divide the image into regions or specific areas and then classify each one of them

these regions cropped, we can classify whether this region contains an object that interests us or not

then we increase the size of the sliding window and continue the process.

- Bounding box approach 

bounding boxes as it is dependent on the size of the window. And hence we have another approach wherein we divide the entire image into grids (x by x), and then for each grid, we define our target label. We can show a bounding box

Pc: Probability of having an object in the grid cell (0: no object, 1: an object).

Bx: If Pc is 1, it is the x coordinate of the bounding box.

By: If Pc is 1, it is the y coordinate of the bounding box.

Bh: If Pc is 1, it is the height of the bounding box.

Bw: If Pc is 1, it is the width of the bounding box.

C1: It is the class probability that the object belongs to Class 1.

C2: It is the class probability that the object belongs to Class 2.

- IOU, area of overlap / area of union 

IoU = Overlapping region/Combined entire region  

IoU values for different positions of the overlapping blocks. If the value is closer to 1.0, it means that the detection is more accurate 

- non-max suppression 

Get the respective probabilities for all the grids.
 
Set a threshold for the probability and threshold for IoU.
 
Discard the ones which are below that threshold.
 
Choose the box with the best probability.
 
Calculate the IoU of the remaining boxes.
 
Discard the ones which are below the IoU threshold.

- Anchor boxes 

Anchor boxes are used to capture the scaling and aspect ratio of the objects we wish to detect

They are of predefined size (height and width) and are sized based on the size of the object we want to detect

During the process of object detection, each anchor box is tiled across the image, and the Neural Network outputs a unique set of predictions for each of the anchor boxes. The output consists of the probability score, IoU, background, and offset for each anchor box. 

making faster real-time object detection possible. To be noted is that the network does not predict the bounding boxes. The network predicts the probability scores and refinements for the tiled anchor box.

- region based CNN, R-CNN 

R-CNN to address the problem of selecting a large number of regions. R-CNN is Region-based CNN architecture. Instead of classifying a huge number of regions, the solution suggests to use selective search and extract only 2000 regions from the image.

first step is to input an image, represented by step 1

These are the 2000 proposed region

    + create the initial segmentation for the image.
    + generate the various candidate regions
    + We combine similar regions into larger ones iteratively. 
    + use the generated regions to output the final region proposals.

reshape all the 2000 regions as per the implementation in the CNN.
    
pass through each region through CNN to get features for each region.

extracted features are now passed through a support vector machine to classify the presence of objects

we predict the bounding boxes for the objects using bounding box regression

- challenges with R-CNN 

R-CNN implements three algorithms (CNN for extracting the features, SVM for the classification of objects, and bounding box regression for getting the bounding boxes).

It extracts features using CNN for each image region makes it slower 

it takes 40–50 seconds to make a prediction for an image, and hence it becomes a problem for huge datasets.

the selective search algorithm is fixed, and not much improvements can be made.

- fast R-CNN 

The image is an input

passed to the convolutional network which returns the respective regions of interest.

apply the region of interest pooling layer. It results in reshaping all the regions as per the input of the convolution

the classification is done by the softmax layer. the coordinates of the bounding boxes are identified using a bounding box regressor

- fast R-CNN different with R-CNN 

Fast R-CNN does not require feeding of 2000 region proposals to the CNN every time

It uses only one convolution operation per image instead of three (extracting features, classification, and generating bounding boxes) no need to store a feature map, resulting in saving disk space.

softmax layers have better accuracy than SVM and have faster execution time.

- faster R-CNN 

The paper can be accessed at https://papers.nips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf

original paper at https://papers.nips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf

    + The first module is a deep fully convolutional network that proposes regions

    + second module is the Fast R-CNN detector that uses the proposed regions

        * the feature maps received, we apply Region Proposal Networks (RPNs). The substeps followed are

RPN takes the feature maps generated from the last step.
 
RPN applies a sliding window and generates k anchor boxes. We have discussed anchor boxes in the last section.
 
The anchor boxes generated are of different shapes and sizes.
 
RPN will also predict that an anchor is an object or not.
 
It will also give the bounding box regressor to adjust the anchors.
 
To be noted is RPN has not suggested the class of the object.
 
We will get object proposals and the respective objectness scores.

Apply ROI pooling to make the size of all the proposals the same.

    + Apply ROI pooling to make the size of all the proposals the same.

    + we feed them to the fully connected layers with softmax and linear regression.

    + We will receive the predicted Object Classification and respective bounding boxes.

- Fast R-CNN, still the algorithm does not analyze all the parts of the image simultaneously. 

- YOLO 

The actual paper can be accessed at https://arxiv.org/pdf/1506.02640v5.pdf.

    + YOLO divides the input image into an SxS grid. To be noted is that each grid is responsible for predicting only one object.

    If the center of an object falls in a grid cell, that grid cell is responsible for detecting that object. 
     
    + the grid cells, it predicts boundary boxes (B). Each of the boundary boxes has five attributes – the x coordinate, y coordinate, width, height, and a confidence score. In other words, it has (x, y, w, h) and a score
    
    + The width w and height h are normalized to the images’ width and height. The x and y coordinates represent the center relative to the bound of the grid cells.
     
    + The confidence is defined as Probability(Object) times IoU.
    
    + Each grid cell predicts C conditional class probabilities – Pr(Classi | Object). These probabilities are conditioned on the grid cell containing an object
    
    + At the test time, we multiply the conditional class probabilities and the individual class predictions. It gives us the class-specific confidence scores for each box
    
    $$ \left|\psi \left({t}_1\right)\right\rangle =U\left({t}_1,{t}_0\right)\mid \psi \left({t}_0\right)\Big\rangle $$
    
    phi(t_1) = U(t_1, t_0)|phi(t+0)
    
    + YOLO predicts multiple bounding boxes for each cell. YOLO optimizes for sum-squared error in the output in the model as sum-squared error is easy to optimize. 
    
    + loss function in yolo 
    
    The loss function is shown in Equation 5-3 and comprises localization loss, confidence loss, and classification loss
    
    confidence loss has two kinds of categories, object is detected, object is not detected in the box 
    
    Localization loss is to measure the errors for the predicted boundary boxes. 
    
    + The network design is shown in Figure 5-13 and is taken from the actual paper at https://arxiv.org/pdf/1506.02640v5.pdf.
    
    + Interested readers can get in-depth knowledge from the official website at https://pjreddie.com/darknet/yolo/.
    
    + Layers
    
    448x448, Conv layer 7x7x64-s-2 maxpool layer 2x2-s-2 
    
    112x112 Conv layer 3x3x192 maxpool layer 2x2-s-2 
    
    56x56 Conv layer 1x1x128 3x3x256 1x1x256 3x3x512 Maxpool layer 2x2-s-2 
    
    28x28 Conv layer 1x1x256x4 3x3x512x4 1x1x512 3x3x1024 Maxpool layer 2x2-s-2 
    
    14x14 Conv Layers 1x1x512x2 3x3x1024x2 3x3x1024 3x3x1024-s-2 
    
    7x7 Conv Layers 3x3x1024 3x3x1024 
    
    7x7 Conn layer 
    
    4096 Conn layer 
    
    7x7x30 
    
- Single shot multibox detector (SSD)

Regional Proposal Network (RPN)–based solutions like R-CNN, Fast R-CNN, need two shots – first one to get the region proposals and second to detect the object for each proposal

SSD uses the VGG16 architecture

The paper can be accessed at https://arxiv.org/pdf/1512.02325.pdf.

applying and passing through a series of convolutions, we obtain   a feature layer of size m x n and p channels.

For each of the locations, we will get k bounding boxes which might be of varying sizes and aspect ratios. And for each of these k bounding boxes, we calculate c class scores and four offsets relative to the original default bounding box to finally receive (c+4)kmn outputs.

SSD implements a smooth L1 norm to calculate the location loss.

multibox methods can be read at https://arxiv.org/abs/1412.1441.

A comparison of YOLO and SSD is shown here. The image is taken from the original paper at https://arxiv.org/pdf/1512.02325.pdf

SSD uses two loss functions to calculate the loss – confidence loss (Lconf) and localization loss (Lloc). Lconf is the loss in making a class prediction, while Lloc is the mismatch between the ground truth and the predicted box.

a few other important processes followed in SSD

    1. Data augmentation by flipping, cropping, and color distortion is done to improve the accuracy.
    2. SSD implements non-max suppression to remove the duplicate predictions. 
    3. SSD results in a higher number of predictions than the actual number of objects.
    4. Detection of small-sized objects can be a challenge
    5. To quote from the original paper: “The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters
    
The accuracy of the SSD can be further improved
    
- transfer learning 
    
    



# Face Recognition and Gesture Recognition 


# Video Analytics Using Deep Learning 


# End to End model development 


