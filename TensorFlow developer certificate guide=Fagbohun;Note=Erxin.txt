TensorFlow developer certificate guide=Fagbohun;Note=Erxin\

# Introduction to tensorflow
- requires 

pip install command:

tensorflow>=2.7.0
tensorflow-datasets==4.4.0
pillow==8.4.0
pandas==1.3.4
numpy==1.21.4
scipy==1.7.3

- Tensors are multi-dimensional arrays designed for numerical data representation
```
#Creating a tensor object using tf.constant
a_constant = tf.constant([1, 2, 3, 4 ,5, 6])
a_constant


# generate a similar tensor object using the tf.Variable function:

#Creating a tensor object using tf.Variable
a_variable = tf.Variable([1, 2, 3, 4 ,5, 6])
a_variable


# Tensors generated using tf.constant cannot be changed, whereas tf.Variable tensors can be reassigned in the future

#3-dimensional tensor
d=tf.constant([[[1,2],[3,4],[5,6]],[[7,8],[9,10],[11,12]]])
d

```

- rank of a tensor identifies the number of dimensions of the tensor

- Properties of tensors

tensor has a data type of float16. To have some fun, you can experiment with different configurations

- Changing data types

Let’s say we have a tensor and we want to change the data type from int32 to float32

```
a=tf.constant([1,2,3,4,5])
a =tf.cast(a,dtype=tf.float32)
a

# Indexing
a[0]


expand the dimension:
tf.expand_dims(a,axis=0)


```

- tensor aggregation 

```
import random
random.seed(22)
a = random.sample(range(1, 100), 50)
a = tf.constant(a)
```

- element wise operation 

```
let us try out a few element-wise operations and see what happens next:


#Addition operation
print((a+4).numpy())
print(" ")
#Subtraction operation
print((a-4).numpy())


# 3 X 2 MATRIX
a = tf.constant([[1, 2], [3, 4], [5, 6]])
#2 X 3 MATRIX
b = tf.constant([[7,8,9], [10,11,12]])
tf.matmul(a,b)
```

- hello world in tensorFlow 

```
import tensorflow as tf

from tensorflow import keras

from tensorflow.keras import Sequential

from tensorflow.keras.layers import Dense

print(tf.__version__)

#import additional libraries

import numpy as np

import matplotlib.pyplot as plt

# Hours of study

X = [20,23,25,28,30,37,40,43,46]

# Test Scores

y = [45, 51, 55, 61, 65, 79, 85, 91, 97]

plt.plot(X, y)

plt.title("Exam Performance graph")

plt.xlabel('Hours of Study')

plt.ylabel('Test Score')

plt.show()

# Keras API to build a simple model:

study_model = Sequential([Dense(units=1,input_shape=[1])])

study_model.compile(optimizer='adam', loss='mean_squared_error')

X= np.array(X, dtype=int)

y= np.array(y, dtype=int)

#fitting the model

history= study_model.fit(X, y, epochs=2500)

#Let us predict how well a student will perform based on their study time
n=38 #Hours of study
result =study_model.predict([n])[0][0] #Result
rounded_number = round(81.0729751586914, 2)
```



# Linear regression with TensorFlow
- requires

pip install command:

tensorflow>=2.7.0
tensorflow-datasets==4.4.0
pillow==8.4.0
pandas==1.3.4
numpy==1.21.4
scipy==1.7.3

- Linear regression is a supervised machine learning technique that models the linear relationship between the predicted output variable

    + MAE

Absolute error = |Ypred − Ytrue|

where Ypred = the predicted value and Ytrue = the ground truth.

The mean absolute error (MAE) of a model is the average of all absolute errors of the data points under consideration. MAE measures the average of the residuals and can be represented using the following equation:

MAE = 1_n ∑i=1n |Ypred − Ytrue|

it is difficult to compare MAE results across different datasets.


    + MSE, mean squared error (MSE). MSE, in contrast to MAE, squares the residuals, thus removing any negative values in the residuals. MSE is represented using the following equation:

MSE =1_N ∑i=1N( Ypred − Ytrue)2
    
    
    + RMSE, Another useful metric in regression modeling is the root mean square error (RMSE). As the name suggests, it is the square root of MSE
    
MSE =1_N ∑i=1N( Ypred − Ytrue)2

RMSE = √_MSE = √________________1_N ∑i=1N( Ypred − Ytrue)2

R2 = 1 − Rres_Rtot

where Rres is the sum of the square of residuals and Rtot is the total sum of squares. The closer the value of R2 is to 1

- salary prediction with tensorflow 

Build model -> Compile Model -> Fit Model 

```
# import tensorflow

import tensorflow as tf

from tensorflow import keras

from tensorflow.keras import Sequential

from tensorflow.keras.layers import Dense

print(tf.__version__)

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import MinMaxScaler

#Loading from the course GitHub account

df=pd.read_csv('https://raw.githubusercontent.com/oluwole-packt/datasets/main/salary_dataset.csv')

df.head()

#drop irrelevant columns
df =df.drop(columns =['Name', 'Phone_Number', 'Date_Of_Birth'])
df.head()

# We will use the drop function in pandas to drop the name, phone number, and date of birth columns

#check the data for any missing values
df.isnull().sum()

#drop the null values
df=df.dropna()

more missing values using the isnull() function:
#check for null values
df.isnull().sum()


# Converting categorical variables to numeric values
df = pd.get_dummies(df, drop_first=True)
df.head()

# use the corr() function to get the correlation of our refined dataset:
df.corr()

# model building 
# We split the attributes and labels into X and y variables
X = df.drop("Salary", axis=1)
y = df["Salary"]

# we use stochastic gradient descent (SGD) as our optimizer and MAE for our loss and evaluation metric:
#compile the model
Model_1.compile(loss=tf.keras.losses.mae,
optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])

#Fit the model
model_1.fit(X_train, y_train, epochs =50)


# We have displayed the last five tries (epochs 46–50). The error drops gradually; train with more epochs 
#create a model using the Keras API
model_2 = Sequential([Dense(units=1, activation='linear',
input_shape=[len(X_train.columns)])])
#compile the model
model_2.compile(loss=tf.keras.losses.mae,
optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])
#Fit the model
history=model_2.fit(X_train, y_train, epochs =500)


def visualize_model(history, ymin=None, ymax=None):
# Lets visualize our model
print(history.history.keys())
# Lets plot the loss
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Number of epochs')
plt.ylim([ymin,ymax]) # To zoom in on the y-axis
plt.legend(['loss plot'], loc='upper right')
plt.show()
```

Fully connected neurons 
mode = Sequential([Dense(units=1, input_shae=[len(x_train.columns)])])
The sequential is used for layer definition 
Dense, fully connected neurons 

use stochastic gradient descent (SGD) as our optimizer and MAE for our loss and evaluation metric

- build a more complex model and see whether we can push the loss lower and quicker than our initial model:


#Set random set
tf.random.set_seed(10)
# create a model
# use a Rectified Linear Unit (ReLU) activation function for this layer; its job is to help our model learn more complex patterns in our data and improve computational efficiency
model_3 =Sequential([Dense(units=64, activation='relu', input_shape=[len(X_train.columns)]),  Dense(units=1) ])
#compile the model
model_3.compile(loss="mae", optimizer="SGD",
metrics = ['mae'])
#Fit the model
history_3 =model_3.fit(X_train, y_train, epochs=500)
history_3 =model_3.fit(X_train, y_train, epochs=500)

visualize_model(history_3, ymin=0, ymax=10000)

- add another layer 

```
#Set random set
tf.random.set_seed(10)
#create a model
model_4 =Sequential([
    Dense(units=64, activation='relu',
    input_shape=[len(X_train.columns)]),
    Dense(units=64, activation='relu'),
    Dense(units=1)
])
#compile the model
model_4.compile(loss="mae", optimizer="SGD",
metrics = "mae")
#fit the model
history_4 =model_4.fit(X_train, y_train, epochs=500)
```

- Normalization is a technique applied to input features to ensure they are of a consistent scale, usually between 0 and 1.

X.describe()

 use the following equation to scale it:

Xnorm = X − Xmin_Xmax − Xmin
```
# create a scaler object
scaler = MinMaxScaler()
# fit and transform the data
X_norm = pd.DataFrame(scaler.fit_transform(X),
columns=X.columns)
X_norm.describe()
```

```
#create a model
model_5 =Sequential([
    Dense(units=64, activation='relu', input_shape=[len(X_train.columns)]),
    Dense(units=64, activation ="relu"),
    Dense(units=1)
])
#compile the model
model_5.compile(loss="mae",
optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])
history_5 =model_5.fit(X_train, y_train, epochs=1000)


#create a model
model_6 =Sequential([
    Dense(units=64, activation='relu',input_shape=[len(X_train.columns)]),
    Dense(units=64, activation ="relu"), Dense(units=1)
])
#compile the model
model_6.compile(loss="mae",
optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])
#fit the model
early_stop=keras.callbacks.EarlyStopping(monitor='loss', patience=10)
history_6 =model_6.fit(
X_train, y_train, epochs=1000, callbacks=[early_stop])
```

model loss fails to improve after 10 epochs. This is achieved by specifying the metric to monitor loss and setting patience to 10.

```
#create a model
model_8 =Sequential([
    Dense(units=64, activation='relu',
    input_shape=[len(X_train.columns)]),
    Dense(units=64, activation ="relu"),
    Dense(units=64, activation ="relu"),
    Dense(units=1)
])
#compile the model
model_8.compile(loss="mae", optimizer="Adam",
metrics ="mae")
#fit the model
early_stop=keras.callbacks.EarlyStopping(monitor='loss',
patience=10)
history_8 =model_8.fit(X_train, y_train, epochs=1000, callbacks=[early_stop])
```

- model evaluation, evaluate metrics to all eight models

```
def eval_testing(model):
    return model.evaluate(X_test, y_test)
    
models = [model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8]
for x in models:
    eval_testing(x)
```

- making predictions 

```
#Let's make predictions on our test data
y_preds=model_7.predict(X_test).flatten()
y_preds

#Let's look at the top 10 data points in the test set
df_predictions.sample(10)
```

- saving and loading models 

```
#Saving the model in one line of code
Model7.save('salarypredictor.h5')
#Alternate method is
#model7.save('salarypredictor')

#loading the model
saved_model =tf.keras.models.load_model("/content/salarypredictor.h5")
```