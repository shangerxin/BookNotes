Learn Grafana 10.x Second edition=Eric;Note=Erxin


# Get start with grafana 
- download the latest versions of Grafana, check out https://grafana.com/grafana/download

$ docker run -d --name=grafana -p 3000:3000 grafana/grafana

 run the container with a temporary volume so that Grafana’s internal database will continue to exist, even if you destroy the container:


% docker volume create grafana-storage
% docker run -d --name=grafana -p 3000:3000  \
    -v grafana-storage:/var/lib/grafana \
    grafana/grafana

- all very capable applications. Here’s a short list of the few we covered:

Kibana (https://www.elastic.co/)
Splunk (www.splunk.com)
Datadog (https://www.datadoghq.com/)
Zabbix (https://www.zabbix.com/)

- Fast: The Grafana backend is written in Google’s exciting Go language

- install on linux 

% wget https://dl.grafana.com/oss/release/grafana-%{GRAFANA_VERSION}.x86_64.rpm
% sudo yum install grafana-${GRAFANA_VERSION}.x86_64.rpm
% systemctl daemon-reload
% systemctl start grafana-server
% systemctl status grafana-server
% sudo systemctl enable grafana-server.service


% sudo apt-get install -y adduser libfontconfig1
% wget https://dl.grafana.com/oss/release/grafana_${GRAFANA_VERSION}_amd64.deb
% sudo dpkg -i grafana_${GRAFANA_VERSION}_amd64.deb
% systemctl daemon-reload
% systemctl start grafana-server
% systemctl status grafana-server
% sudo systemctl enable grafana-server.service

- for Windows is straightforward:

Go to https://grafana.com/grafana/download?platform=windows.
Download the latest MSI installer file from the download link.

https://grafana.com/grafana/download?platform=windows

Latest version: 
Release Date:
March 26, 2025
Release Info:
What's New In Grafana 11.6.0


# Touring the grafana interface 
- (add) menu
Clustered on the right side of the search bar are several menu icons. The leftmost is the + (add) menu. Clicking on it allows you to create a new dashboard, import an existing dashboard

- New | New Dashboard creates a new empty dashboard containing instructions on how to add content
- The Snapshots page, also accessed via the Dashboards | Snapshots menu selection, allows you to capture the state of a dashboard in what are called snapshots

- Alerting rules in Grafana are the mechanism for creating alerting pipelines, which begin with your data sources and end with an alert going out to a notification service of your choosing

Connections menu item leads to the Add a new connection page, which contains an extensive library of these data sources.
 
The Plugins page is an inventory page listing all the installed data sources and panel plugins. It also features a link to the plugins catalog on https://grafana.com

Organizations are Grafana’s mechanism for supporting multiple independent Grafana sites from a single server.


# Diving into Grafana's Time Series Visualization
- Add data source 

Home dashboard if you’re not already there.
From the main menu, select Connections | Data Sources.
Click Add Data Source.
Scroll down to Others and select TestData.

- Add visualization. By default, you will have created a time series visualization panel.

- Generating data series in the Query tab

- query 

The Data source menu
Query options
The Query inspector button
The query
The duplicate query button
The query visibility toggle button
The Delete query button
Query order drag and drop
The + Query button
The + Expression button

-  Soft min and Soft max, the goal is to make sure your graph mostly shows data and not an empty space


# Real world grafana 
- Installing Prometheus from Docker

- configure prometheus

 prometheus.yml:

```
global:
    scrape_interval:      15s # By default, scrape targets every 15 seconds.
    # Attach these labels to any time series or alerts when communicating with
    # external systems (federation, remote storage, Alertmanager).
    external_labels:
        monitor: 'codelab-monitor'
# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
    # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
    - job_name: 'prometheus'
    # Override the global default and scrape targets from this job every 5 seconds.
     scrape_interval: 5s
    static_configs:
        - targets: ['localhost:9090']
```

https://prometheus.io/docs/prometheus/latest/configuration/configuration


a job called prometheus that will scrape itself every five seconds. The target server is located at http://localhost:9090.

```
services:
    grafana:
        image: grafana/grafana:latest
    ports:
        - "3000:3000
    prometheus:
        image: prom/prometheus:latest
    ports:
        - "9090:9090"
    volumes:
        -$PWD/prometheus:/etc/prometheus
```


$ docker-compose up –d –pull-missing

- prometheus 

Name: Prometheus
HTTP | Prometheus server URL: http://prometheus:9090
Additional settings | Interval behaviour | Scrape interval: 5s
At the bottom, click on Save & Test.

- Detecting trends with aggregations
As we continue up the stack, let’s now examine some server performance metrics. How about an obvious web server metric? Enter   


# Extracting and visualizing data with influxDB and grafana 
- capture weather data from the National Weather Service (NWS) and store it in an InfluxDB time series database

- Grab the field name as our InfluxDB measurement name. InfluxDB treats measurements much like a traditional RDBMS table

Dockerfile. This is what it looks like:

```
FROM python:3
WORKDIR /usr/src/app
COPY requirements.txt ./
COPY weather.py ./
RUN pip install --no-cache-dir -r requirements.txt
ENTRYPOINT [ "python" ]

```

- configuring the influxdb data source 


Fill out the following form fields:

Name: InfluxDB
Query Language: InfluxQL
HTTP | URL: http://influxdb:8086
Custom HTTP Headers | Header: Authorization
Custom HTTP Headers | Value: Token <API Token>
InfluxDB Details | Database: chapter05

- Understanding the time series data visualization

- Now that we’ve broken down how data points are graphed horizontally in time, let’s look at how they are graphed vertically on the y axis.

- Let’s start by creating a new dashboard panel by clicking the Add | Visualization drop-down menu at the top right of the dashboard. Make sure Time series is the visualization type

- Grafana is somewhat more restrained about the legend. It can live below the graph (or to its right) and can take on a list or a table format;


# Surveying additional grafana visualizations 
- Exploring spatial data with the Geomap visualization



# Dashboard design 
- folder 
- tag 


# Workflow 


# Advance, working with elastic search 
- Elasticsearch container allow open access to the data source, so we won’t need to use an API token to access it:
- create template variables 

Open Dashboard Settings, select Variables, and click Add Variable. Select the Query variable type and use these settings:

General | Name: 311Category
General | Label: 311 Category
Query Options | Data Source: Elasticsearch (the Elasticsearch data source)
Query Options | Query: {"find": "terms", "field": "Category.keyword"}
Query Options | Sort: Alphabetical (asc)
Query Options | Refresh: On dashboard load
Selection Options | Include All Option: on


# Monitorin system 


# Exploring log data with grafana's Loki 
-  words provides the following (https://grafana.com/oss/loki):

Loading system logs into Loki
Visualizing Loki log data with Explore
Capturing simulated logs generated programmatically and real logs output by Docker
Querying synchronized logs and metrics in Explore

- promtail
First, we need to copy a sample Promtail configuration file (promtail-config.yaml) into a local directory before mapping that file into our container.


# Managing grafana 


# Managing permissions 
- Adding users 

# Authenticating Grafana Logins Using LDAP or OAuth 2 Providers
- Setting up an OpenLDAP server
- Configuring Grafana to use LDAP
- Authenticating with OAuth 2
- Authenticating with Okta
Okta is a well-known authentication provider for enterprises