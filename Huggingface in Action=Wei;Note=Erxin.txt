Huggingface in Action=Wei;Note=Erxin

# Introducting huggingface 
- Hugging Face is an AI community that promotes the building, training, and deployment of open source machine learning models. 

epicenter of the AI revolution because

- Model Context Protocol (MCP) to connect AI assistants to external data sources

- This paragraph of text comes from the Internet Movie Database (IMDb) dataset (https://huggingface.co/datasets/stanfordnlp/imdb), a binary sentiment-analysis dataset consisting of 50,000 reviews from the IMDb website labeled positive or negative. 

- The Hugging Face Hub’s Models page (https://huggingface.co/models; see figure 1.2) hosts many pretrained models for a wide variety of machine learning tasks.

- Gradio is an open source Python library that makes it easy to create customizable user interfaces for machine learning models

developed Gradio (https://github.com/gradio-app/gradio), an open source Python library for creating GUIs for machine learning models. 

- The user heads to Hugging Face’s Model Hub, which contains more than 1 million pretrained models. This hub isn’t a dumping ground;

Users download the model weights and configuration files and run them using the Transformers library.

The sentiment classifier returns {"label": "POSITIVE", "score": 0.9998}, and the user’s problem is solved. Mission accomplished.


# Getting started 
- prepare environment 

```
conda create venv 
pip install torch torchvision torchaudio 
pip install transformers

import torch
use_cuda = torch.cuda.is_available()

if use_cuda:
    print('__CUDNN VERSION:', torch.backends.cudnn.version())
    print('__Number CUDA Devices:', torch.cuda.device_count())
    print('__CUDA Device Name:', torch.cuda.get_device_name(0))
    print('__CUDA Device Total Memory [GB]:',
          torch.cuda.get_device_properties(0).total_memory/1e9)
```

with notebook run 
```
!pip install transformers
```

- You can also use the GPUtil package to find details about your GPU

```
pip install GPUtil

import GPUtil

gpus = GPUtil.getGPUs()

for gpu in gpus:
    print("GPU ID:", gpu.id)
    print("GPU Name:", gpu.name)
    print("GPU Utilization:", gpu.load * 100, "%")
    print("GPU Memory Utilization:", gpu.memoryUtil * 100, "%")
    print("GPU Temperature:", gpu.temperature, "C")
    print("GPU Total Memory:", gpu.memoryTotal, "MB")
    
    

from transformers import pipeline
question_classifier = pipeline("text-classification",
                               model="huaen/question_detection",
                               device = 0)
                               
question_classifier = pipeline("text-classification",
                               model="huaen/question_detection",
                               device = "mps:0")       

                               
                               question_classifier = pipeline("text-classification",
                               model="huaen/question_detection",
                               device = "cuda:0")  #1
```

Besides specifying a number for the device parameter to specify that you want to use the GPU for processing

device parameter in the pipeline() function
-1 "cpu"
0   "cuda", "cuda:0"
1   "cuda:1"
n   "cuda:n"
0   "mps:0"    Refers to the MPS backend in PyTorch, which allows machine learning models to run on Apple’s built-in GPU

using the device.type attribute

```
from transformers import pipeline
import torch

if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"

question_classifier = pipeline("text-classification",
                               model="huaen/question_detection",
                               device=device)
print(f"Using device: {device}")
```
- install huggingface hub package 

The Hugging Face Hub (https://huggingface.co; see figure 2.10) is the go-to place for all things related to Hugging Face: pretrained models, demos, datasets, and more.

```
pip install huggingface_hub
```

Hugging Face Hub, you’ll find many pretrained models that you can freely use. Often, when you use a model for the first time, the Transformers library automatically downloads the files associated with the model and stores them locally on your computer.

```
from huggingface_hub import hf_hub_download

hf_hub_download(repo_id="google/pegasus-xsum",
                filename = "config.json")
```

config.json file is downloaded to the following directory:

<home_directory>/.cache/huggingface/hub/
models--google--pegasus-xsum/snapshots/
8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b/

-  you’ve copied the commit hash of the file you want to download, you can set it as the value for the revision parameter in the hf_hub_download() function:

hf_hub_download(
    repo_id="google/pegasus-xsum",
    filename="config.json",
    revision="a0aa5531c00f59a32a167b75130805098b046f9c"
)

- The huggingface_hub package also includes the Hugging Face CLI, a command-line-­interface tool that allows you to authenticate your applications using tokens.

```
$ huggingface-cli
usage: huggingface-cli <command> [<args>]

positional arguments:
  {env,login,whoami,logout,repo,upload,download,
   lfs-enable-largefiles,lfs-multipart-upload,
   scan-cache,delete-cache}
                        huggingface-cli command helpers
    env                 Print information about the environment.
    login               Log in using a token from
                        huggingface.co/settings/tokens
    whoami              Find out which huggingface.co account 
                        you are logged in as.
    logout              Log out
    repo                {create} Commands to interact with 
                        your huggingface.co repos.
    upload              Upload a file or a folder to a repo on the Hub
    download            Download files from the Hub
    lfs-enable-largefiles
                        Configure your repository to enable 
                        upload of files > 5GB.
    scan-cache          Scan cache directory.
    delete-cache        Delete revisions from the cache directory.

options:
  -h, --help            show this help message and exit
```

$ huggingface-cli login

located in the <home_directory> /.cache/huggingface/ directory. To see which account you’ve signed in to, use the whoami option
 
```
from huggingface_hub import login

login()
```

enter your token 


# Using huggingface transformers and pipelines for NLP tasks 
- The transformer architecture, introduced in the paper “Attention is All You Need” by Ashish Vaswani et al. (https://arxiv.org/pdf/1706.03762.pdf), has become the foundation for many state-of-the-art models in NLP.

- key components of the transformer architecture:

Input Embedding, Converts input tokens to dense vectors of fixed size. Adds positional encodings to retain information about the order of the tokens because the model processes input in parallel and lacks inherent sequential information.

Encoder—Comprises multiple identical layers, Multi-Head Attention—Computes the attention scores between each pair of input tokens to capture dependencies. Feed-Forward Neural Network (FFN)—Comprises two linear transformations with a Rectified Linear Unit (ReLU) activation in between.

Decoder—Also consists of multiple identical layers with additional components to handle the target sequence. Masked Multi-Head Attention prevents attending to future tokens in the target sequence, Multi-Head Attention—Allows the decoder to attend to the Encoder’s output.

Positional Encoding, Typically implemented using sine and cosine functions of different frequencies. Adds information about the position of each token in the sequence because self-attention operates without considering token order.

Layer Normalization and Residual Connections, Layer Normalization—Stabilizes and accelerates training by normalizing the input across the features, Residual Connections—Adds the input of each sublayer to its output to help with gradient flow and prevent vanishing/exploding gradients. 

Final Linear and Softmax Layers—In the Decoder, after processing the sequence through multiple layers, a final linear transformation followed by a Softmax layer is used to produce probabilities for the next token.

- Tokenization is the process of converting a text document or sentence to smaller units. Generally, there a few types of tokenization strategies

Word tokenization—Splits text into individual words based on whitespace or punctuation characters. 

Subword tokenization—Breaks text into smaller linguistic units such as prefixes, suffixes, or root words. This strategy is commonly used for languages with complex morphology or tasks like machine translation. 

Character-level tokenization—Segments text into individual characters, including letters, digits, and punctuation marks. 

```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")  #1
input_text = "What is unhappiness?"
tokens = tokenizer.tokenize(input_text, return_tensors="pt")  #2

print(f"{tokens = }")
```

- text is tokenized, the next step is performing token embeddings, which convert tokens to numerical vectors

```
from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")  #1
model = BertModel.from_pretrained("bert-base-uncased")   #1

input_text = '''
After a long day at work, Sarah decided to relax by taking her
dog for a walk in the park. As they strolled along the
tree-lined paths, Sarah's dog, Max, eagerly sniffed around,
chasing after squirrels and birds. Sarah smiled as she watched
Max enjoy himself, feeling grateful for the companionship and
joy that her furry friend brought into her life.'''

tokens = tokenizer(input_text, return_tensors="pt")

with torch.no_grad():
    outputs = model(**tokens)  #2

last_hidden_states = outputs.last_hidden_state  #3
print("Token embeddings:")  #4
for token, embedding in zip(tokens["input_ids"][0],
                            last_hidden_states[0]):
    word = tokenizer.decode(int(token))
    print(f"{word}: {embedding}")
```

In this example, we tokenized the input text using the BERT model and then extracted the token embeddings from the last layer of the model’s outputs. Finally, we print the embedding for each word

- high dimensional embeddings into 2d space 

```
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2, perplexity=5, random_state=42)  #1
embeddings_tsne = tsne.fit_transform(last_hidden_states[0])   #1

plt.figure(figsize=(10, 8))  #2
plt.scatter(embeddings_tsne[:, 0],   #2
            embeddings_tsne[:, 1], marker='o')   #2
for i, word in enumerate(tokenizer.convert_ids_to_tokens(   #2
    tokens["input_ids"][0])):   #2
    plt.annotate(word, xy=(embeddings_tsne[i, 0],   #2
                           embeddings_tsne[i, 1]),   #2
                 fontsize=10)   #2
plt.xlabel('t-SNE Dimension 1') #2
plt.ylabel('t-SNE Dimension 2') #2
plt.title('t-SNE Visualization of Token Embeddings')
plt.show()
```

Positional encoding is typically added to token embeddings before they are input into the transformer model. positional encodings provide information about the positions of words within a sequence. 

```
from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

input_text = '''
After a long day at work, Sarah decided to relax by taking her
dog for a walk in the park. As they strolled along the
tree-lined paths, Sarah's dog, Max, eagerly sniffed around,
chasing after squirrels and birds. Sarah smiled as she watched
Max enjoy himself, feeling grateful for the companionship and
joy that her furry friend brought into her life.'''

tokens = tokenizer(input_text, return_tensors="pt")
embeddings = model.embeddings  #1
positional_embeddings = embeddings.position_embeddings.weight  #2
position_ids = torch.arange(tokens['input_ids'].size(1),
                            dtype=torch.long).unsqueeze(0)  #3
input_positional_embeddings = positional_embeddings[position_ids]  #4

print("Positional embeddings shape:", input_positional_embeddings.shape)
print("Positional embeddings for each token:")

for token_id, pos_embedding in zip(tokens['input_ids'][0],
                                   input_positional_embeddings[0]):
    token = tokenizer.decode([token_id])
    print(f"{token}: {pos_embedding}")
```

- transformer block 

Self-Attention Mechanism—The Transformer block uses self-attention, also known as scaled dot-product attention. This mechanism allows the model to weigh the importance of different words (tokens) in a sequence based on their relationships with each other. 

Feed-Forward Neural Networks—After the self-attention mechanism, the Transformer block applies FFNs to process each word’s representation independently and in parallel.

- Softmax, a mathematical function that converts a vector of numbers to a probability distribution in which the probability of each element is proportional to the exponentiation of that element’s value relative to the sum of all the exponentiated values in the vector.

- pretrained transformers models 

BERT—Introduces bidirectional training for transformers, capturing context from both directions of a word
GPT (Generative Pretrained Transformer)—Focuses on language generation tasks by predicting the next word in a sequence
RoBERTa (Robustly Optimized BERT Approach)—Optimized version of BERT with improved training strategies and larger datasets
DistilBERT—A smaller, faster variant of BERT, suitable for deployment in resource-constrained environments
T5 (Text-To-Text Transfer Transformer)—Trained on a unified framework in which all tasks are treated as text-to-text transformations

- transformers pipelines 

illustrates the role of the pipeline() function.

Pipelines are high-level APIs that use the transformer models provided by the Transformers library.

- DistilBERT is a lighter, smaller, faster version of BERT Victor Sanh et al. (https://arxiv.org/abs/1910.01108).

- reasons for using a model directly or through a pipeline 

    + pipeline 
    
    Simplicity—Pipelines provide a high-level, user-friendly interface for using the model without requiring you to understand the underlying complexities 
    
    Preconfigured settings—Pipelines come with preconfigured settings and default parameters optimized for common use cases.
    
    Rapid prototyping—With pipelines, you can work with models with only a few lines of code
    
    + model directly 
    
    Fine-grained control—You have more control of input preprocessing, tokenization, model inference, and postprocessing steps.
    
- using model directly 

    +  first step, load the tokenizer from the model using the AutoTokenizer.from_pretrained() method:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "distilbert/distilbert-base-uncased-finetuned-sst-2-english")
````    

    +  load the model using the AutoModelForSequenceClassification.from_pretrained() method:
    
```
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert/distilbert-base-uncased-finetuned-sst-2-english")
```    

    + tokenize it using the tokenizer object, The tokenizer object returns a dictionary containing the tokenized representation of the input text, suitable for consumption by a PyTorch model (indicated by the pt value)
```
import torch

text = "I loved the movie, it was fantastic!"

inputs = tokenizer(text, return_tensors = "pt")  #1
print(inputs)
```

    + pass the tokenizer object into model 
    
```
outputs = model(**inputs)  #1
print(outputs)
```

    + performs inference , giving the model the token IDs (stored in input_ids key) and optionally other tensors, such as the attention mask (stored in the attention_­mask key). Then the model performs inference on these inputs, generating predictions 
    
```
SequenceClassifierOutput(loss=None, logits=tensor([[-4.3428,  4.6955]],
grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
```

take note of the values in the logits key: [[-4.3428, 4.6955]]. The value is of shape (1,2), where the first dimension corresponds to the batch size (1, in this case) and the second dimension corresponds to the number of classes (2, in this case: 0 for negative and 1 for positive). Each element in the tensor represents the model’s confidence score for a particular class

    + you can extract the class with the higher confidence score and determine the class of the result
    
```
predicted_label = torch.argmax(outputs.logits)  #1
sentiment = "positive" if predicted_label == 1 else "negative"

print("Predicted sentiment:", sentiment)
```

- Using a transformers pipeline

```
from transformers import pipeline
try:
    dummy_pipeline = pipeline(task="dummy")
except Exception as e:
    print(e)
   
from transformers import pipeline

pipe = pipeline("text-classification",
    model="distilbert/distilbert-base-uncased-finetuned-sst-2-english")   
```

you can view the model’s page on the Hugging Face website and find the task that this model is trained 

The pipeline method will use the default model and revision (version) for that task

```
pipe = pipeline("text-classification")
```

    + because the default model to use might change in the next release of the pipeline() method. It’s better to specify the model parameter 
```
from transformers import pipeline

classifier = pipeline(task = 'text-classification',
    model = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english')
```

    + you’ve already identified the model you want to use, you don’t have to specify the task parameter. Specify only the model
```
classifier = pipeline(
    model = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english')
```

    + provide device to run on gpu
```
classifier = pipeline(
    model = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',
    device = "cuda")
    
review1 = '''From the warm welcome to the exquisite dishes and impeccable
 service, dining at Gourmet Haven is an unforgettable experience that
 leaves you eager to return.'''

review2 = '''Despite high expectations, our experience at Savor Bistro
 fell short; the food was bland, service was slow, and the overall
 atmosphere lacked charm, leaving us disappointed and unlikely to
 revisit.'''
 
print(classifier(review1))
```

- Using transformers for NLP tasks

Text classification
Text generation
Text summarization
Text translation
Zero-shot classification
Question answering

- text classification 
    
```
from transformers import pipeline

question_classifier = pipeline("text-classification",
                               model="huaen/question_detection")
                               
response = question_classifier(
    '''Have you ever pondered the mysteries that lie beneath
    the surface of everyday life?''')
print(response)                               
```

    + Another text classification task you can perform is language detection. Using a model such as papluca/xlm-roberta-base-language-detection
    
```
language_classifier = pipeline("text-classification",
    model="papluca/xlm-roberta-base-language-detection")

response = language_classifier("日本の桜は美しいです。")
print(response)


```

    + a spam classifier, which enables you to identify incoming messages (emails, text messages, comments, and so on) as spam
    
```
spam_classifier = pipeline("text-classification",
                           model="Delphia/twitter-spam-classifier")

response = spam_classifier(
    '''Congratulations! You've been selected as the winner of our
    exclusive prize draw. Claim your reward now by clicking on
    the link below!''')

print(response)
```

- Text generation 
    
use the openai-community/gpt2 model to generate a paragraph of text based on an initial start sentence

```
from transformers import pipeline

generator = pipeline("text-generation",
                     model="openai-community/gpt2")
generator("In this course, we will teach you how to")

generator("In this course, we will teach you how to",
          max_length = 50,
          num_return_sequences = 3)
```

- Summarizing text is another widely recognized 

```
from transformers import pipeline

summarizer = pipeline("summarization",
                      model="facebook/bart-large-cnn")
             
article = """..."""
print(summarizer(article,
                 min_length = 100,
                 max_length = 250,
                 do_sample = False))
```

- Text translation is one of the earliest foundational tasks in NLP. 

text translation models you can use to translate text from one language to another. Let’s start with the google-t5/t5-base model, a variant of the T5 model developed by Google AI

```
from transformers import pipeline

translator = pipeline("translation",
                      model = "google-t5/t5-base")
                      
translator("How are you?")                     
```

- To translate from English to Chinese, you can use the facebook/m2m100_418M model. But before you can use this model, you need to install the sentencepiece package

```
pip install sentencepiece

translator = pipeline('translation_en_to_zh',
                      model = 'facebook/m2m100_418M')

translator('Wikipedia is hosted by the Wikimedia Foundation, 
a non-profit organization that also hosts a range of other 
projects.')
```

- Zero-shot classification, sentiment analysis in which the model was trained on a set of labeled examples, classifying them as positive or negative.


one-shot classification refers to training a model to recognize classes with only one example (or a few examples) per class during training.

The aim is to teach the model to generalize from a small number of examples. This technique is useful when collecting a large number of samples

Natural Language Inference (NLI)

To try zero-shot text classification, let’s use the joeddav/xlm-roberta-large-xnli model (https://mng.bz/MwQo). 

```
from huggingface_hub import notebook_login
notebook_login()
```

you need to install two packages: sentencepiece and protobuf. 

```
!pip install sentencepiece
!pip install protobuf
```

create a pipeline object that uses the joeddav/xlm-roberta-large-xnli model

```
from transformers import pipeline

zero_shot_classifier = pipeline("zero-shot-classification",
                                model='joeddav/xlm-roberta-large-xnli')
                                
candidate_labels = ["technology", "politics", "business", "romance"]
prediction = zero_shot_classifier(text1,
                                  candidate_labels,
                                  multi_label = True)       

import pandas as pd
display(pd.DataFrame(prediction).drop(["sequence"], axis=1)) 

prediction = zero_shot_classifier([text1, text2],
                                  candidate_labels,
                                  multi_label = True)
display(pd.DataFrame(prediction).drop(["sequence"], axis=1))                                 
```


    + zero-shot image classification? For this task, you can use the openai/clip-vit-large-patch14-336 model
```
from transformers import pipeline

classifier = pipeline("zero-shot-image-classification",
                      model = "openai/clip-vit-large-patch14-336")

labels_for_classification =  ["airplane", "car", "train"]
scores = classifier("Emirates_Airbus_A380-861_A6-EER_MUC_2015_04.jpg",
                    candidate_labels = labels_for_classification)
pd.DataFrame(scores)
```

- question answering tasks 

Efficient information retrieval—QA models can quickly retrieve information of interest from a large collection of text.
Natural language understanding—QA models can understand natural language inputs and generate context-relevant answers.

Definition SQuAD is a reading comprehension dataset consisting of questions posed by crowd workers on a set of Wikipedia articles. 

use the deepset/roberta-base-squad2 model to understand how a QA model works.

```
from transformers import pipeline

QA_model = pipeline(task = "question-answering",
                    model = "deepset/roberta-base-squad2")
                    
question = {
          'question': 'What is the meaning of Singapura?',
          'context': text
          }

model_response = QA_model(question)
pd.DataFrame([model_response])                    
```


# Using huggingface for computer vision tasks 
- task type 

Object detection
Image classification
Image segmentation
Video classification
Depth estimation
Image-to-image
Unconditional image generation
Zero-shot image classification