Modern DevOps Practices=Gaurav Agarwal;Note=Erxin


# Reference 
https://learning.oreilly.com/library/view/modern-devops-practices/9781800562387/


# Section 1
- Virtual machines
A virtual machine emulates an operating system using a technology called a Hypervisor. A Hypervisor can run as software on a physical host OS or run as firmware on a bare-metal machine.

- Rkt and Containerd. All of them use the same Linux Kernel cgroups feature. embed OpenVZ into the main Linux Kernel. OpenVZ was an early attempt at implementing features to provide virtual environments

- pstree to display process tree 
- container networking, various types of container networks

none, a fully isolated network 
bridge, default network in most container, provide network address translation NAT
Host: Host networking uses the network namespace of the host machine for all the containers.
Underlay, Underlay exposes the host network interfaces directly to containers.
Overlay, Overlay networks allow communication between containers running on different host machines via a networking tunnel.

orchestrators are flannel, calico, and vxlan.

jenkins cd -deploy-> kubernetes cluster 
                            |
                            pull
                            |
                            V
build -> run -> test -> docker hub 



# Containerization with docker 
- install tools 

git 
vim 
docker 

- docker storage drivers 
overlay2: This is a production-ready and the preferred storage driver for Docker
aufs, prefer driver type of driver 18.04 and below 
devicemapper 
btrfs and zfs 
vfs
devicemanager

- configure a storage driver 
- docker container 

$ docker run -d --name nginx --restart unless-stopped 

- docker has several kinds of logging drivers 

- Docker monitoring with Prometheus

Prometheus query language (PromQL), efficient time-series databases, and modern alerting capabilities.

Host metrics You need to monitor your host metrics  

Host CPU
Host memory 
Host disk space 

docker container metrics 
cpu 
throttled cpu time 
container memory fail counters 
container memory usage 
container swap 
container disk i/o 
container network metrics 

- Docker Compose is very simple. You download the docker-compose binary from its official repository


# Creating and managing container images 
- components Docker uses to orchestrate the following activities:

Docker daemon: This process runs on the servers where we want to run our containers. They deploy and run containers on the docker server.
Docker registries: These store and distribute Docker images.
Docker client: This is the command-line utility that we've been using to issue docker commands to the Docker daemon

- build container image, It is actually a one-line command – docker build -t <image-name>:version <build_context>. 

$ $ docker run -d -p 80:80 <your_dockerhub_user>\/nginx-hello-world

- Building and managing Docker images
We built some Docker images in the previous section, so by now, you should have some idea about how to write Dockerfiles and create Docker images from them.

- Multi-stage builds
Now, let's modify the Dockerfile according to the multi-stage build process and see what we get.

- Managing Docker images
In modern DevOps practices, Docker images are primarily built either in a developer machine or a CI/CD pipeline. The images are stored in a container registry and then deployed to multiple staging environments and production machines.

- Flattening Docker images
Docker inherently uses a layered filesystem, and we have already discussed why it is necessary and how it is beneficial in depth

- Understanding Docker registries
A Docker Registry is a stateless, highly scalable server-side application that stores and lets you distribute Docker images

    + Other public registries
amazon 
google 
azure 
oracle 
coreos quay 



# Container Orchestration with Kubernetes 
- As containers are portable, they can run on any machine that runs Docker just fine. Multiple containers also share server resources to optimize resource consumption. 

portable
load balancer 

mostly run within a server and can see other 

modern cloud paltforms 

- High Availability (HA), Disaster Recovery (DR), and other operational aspects for every application on your tech stack.


- Kubernetes is made of a cluster of nodes. There are two possible roles for nodes in Kubernetes – control plane and worker nodes.

API server 

controller manager 

cloud controller manager 

etcd, log book of the ship 

scheduler 

kubelet 

kube-proxy 

- Installing KinD
Kubernetes in Docker or KinD, in short, allows you to run a multi-node Kubernetes cluster on a single server that runs Docker.

- kubernetes pods 

a pod contains one or more containers, and all containers within a pod are always scheduled in the same host.

- The kubectl run command was the imperative way of creating pods, but there's another way of interacting with Kubernetes – by using declarative manifests.

    + nginx-pod.yaml 
```    
apiVersion: v1

kind: Pod

metadata:

  labels:

    run: nginx

  name: nginx

spec:

  containers:

  - image: nginx

    imagePullPolicy: Always

    name: nginx

    resources:

      limits:

        memory: "200Mi"

        cpu: "200m"

      requests:

        memory: "100Mi"

        cpu: "100m"

  restartPolicy: Always
```

- using the port forwarding, expose the pod 

$ kubectl port-forward nginx 8080:80 

- Sidecar pattern
Sidecars derive their names from motorcycle sidecars. The sidecar, though, does not change the bike's core functionality, and it can work perfectly fine without it.

- Secrets
Secrets are very similar to config maps, with the difference that the secret values are base64-encoded instead of plaintext.

- Google Kubernetes Engine (GKE), for the exercises. 

- spinning up google kubernetes engine 

$ gcloud services enable container.googleapis.com

$ kubectl get replicaset
...

- Kubernetes Deployment strategies 
Recreate The Recreate strategy is the most straightforward deployment strategy. 

Rolling Update: Slowly roll out Pods of the new version while still running the old version

Blue/Green: This is a derived deployment strategy where we keep both versions running simultaneously and switch the traffic

Canary: This is applicable to Blue/Green deployments where we switch a percentage of traffic to the newer version

A/B testing: A/B testing is more of a technique to apply to Blue/Green deployments. 

- ClusterIP Services
ClusterIP Service resources expose Pods within the Kubernetes cluster and they are the default if you don't specify a type

- LoadBalancer services
LoadBalancer Service resources help expose your Pods on a single load-balanced endpoint. 

- Ingress resources act as reverse proxies into Kubernetes. You don't need a load balancer for every application you run within your estate.

- Horizontal Pod autoscaling
HorizontalPodAutoscaler is a Kubernetes resource that helps you to update the replicas within your ReplicaSet resources based on defined factors

- Dynamic provisioning is when Kubernetes provides storage resources for you by interacting with the cloud provider.



# Delivering containers 
- Terraform 
- Amazon 
- Terraform commandline 



# Configuration management with ansible 
- $200 worth of free credits, and you can sign up at https://azure.microsoft.com/en-in/free.
- ansible playbook 
```
---

  - hosts: all

    tasks:

      - name: Ping all servers

        action: ping
```

- Sourcing variable values
While you can manually define variables and provide their values, sometimes we need dynamically generated values

- Jinja2 templates
Ansible allows for templating files using dynamic Jinja2 templates. You can use the Python syntax within the file



# IaC and config management in action 
- boot up a scalable Linux Apache MySQL and PHP (LAMP) stack on Azure with Terraform
- Installing Packer
Installing Packer is very simple. Just download the required Packer binary and move it to your system path.

Building the Apache and MySQL images using Packer

- Infrastructure as Code (IaC) Using Terraform instead.

- And as we get the Database Connected successfully message, we see that the configuration is successful! We've successfully created a scalable LAMP stack by using Packer



# Continue integration 
- Amazon ECS with EC2 and Fargate
Amazon Elastic Container Service (ECS) is a container orchestration platform that AWS offers

- Installing the AWS and ECS CLIs
The AWS CLI is available as a deb package within the public apt repositories. 

- Creating task definitions
ECS tasks are similar to Kubernetes pods. They are the basic building blocks of ECS and are comprised of one or more related containers

- Scaling tasks
We can easily scale tasks using ecs-cli. Use the following command to scale the tasks to two:

$ ecs-cli compose scale 2 --cluster cluster-1 --launch-type EC2

- Load balancing is an essential functionality of multi-instance applications. They help us serve the application on a single endpoint

```
$ aws elbv2 create-load-balancer --name ecs-alb \

--subnets <SUBNET-1> <SUBNET-2> \

--security-groups <SECURITY_GROUP_ID> \

--region us-east-1
```

- Scalable Jenkins on Kubernetes with Kaniko
Jenkins is the most popular CI tool available in the market. It is open source, simple to install, and runs with ease

- install jenkins configure clouds for kubernetes 

- Automating a build with triggers
The best way to allow your CI build to trigger when you make changes to your code is to use a post-commit webhook.

- Build performance best practices
CI is an ongoing process, and therefore, you will have a lot of parallel builds running within your environment at a given time.



# Continous deployment/delivery with spinnaker 
-  pushing your code and your Continuous Integration (CI) pipeline is building, testing, and publishing the builds to your artifact repository

- Spinnaker is an open source, multi-cloud continuous delivery tool developed by Netflix and Google and which was then open sourced to the Cloud Native Computing Foundation (CNCF).

- microservices:

Deck: A simple browser-based UI that takes care of the user interactions.
Gate: The API Gateway that frontends all Spinnaker components and listens to calls from the Spinnaker UI and API Callers.
Orca: This is the orchestration engine that takes care of all ad hoc operations and pipelines.
Clouddriver: This indexes/caches deployed resources and makes calls to cloud providers.
Front50: This is the persistent store and stores the application, pipeline, project, and notification metadata.
Rosco: This helps in baking custom VM images by providing a wrapper over Packer.
Igor: This is used to trigger Jenkins and Travis pipeline jobs from Spinnaker.
Echo: This is Spinnaker's Eventing engine. It allows for all communications between multiple systems, such as sending notifications to Slack or receiving webhooks from systems such as Jenkins.
Fait: This is the authorization service and provides RBAC.
Kayenta: This provides automatic canary analysis for deployments.
Keel: This provides managed delivery.
halyard: This is Spinnaker's configuration tool. We will use this to configure Spinnaker.

- Installing halyard
Before installing halyard, we need to have kubectl installed within the system as halyard will use that to interact with our Kubernetes cluster



# Securing the deployment pipeline 
- constitute secrets are listed here:

Passwords
API tokens, GitHub tokens, and any other application key
Secure Shell (SSH) keys
Transport Layer Security (TLS), Secure Sockets Layer (SSL), and Pretty Good Privacy (PGP) private keys
One-time passwords

- Installing Anchore Grype
Anchore Grype offers an installation script within their GitHub repository that you can download and run

- Binary authorization is a deploy-time security mechanism that ensures that only trusted binary files are deployed within your environments

- You might have heard about the principle of least privilege (PoLP) several times in this book




# Modern DevOps with GitOps 
- GitOps is a method that involves the implementation of DevOps such that Git forms the single source of truth. Instead of maintaining a long list of scripts and tooling to support this

- Gitflow structure with several branches  

feature branches   

develop contain everything and feature merges, hotfixes  

release branches, contain the main release tag and bugfixes commits  

hotfixes, will be merged back to develop and main branch after new tag will be added 

master only contain the main release tag 

- environment repository 

showcases the branching strategy you might want to follow for your environment repository:

- cloud platform OAuth scope.

We have a provider.tf file that contains the provider and backend configuration. 



# CI/CD Pipelines with GitOps 
- Creating an application repository on github 
- Creating a GitHub Actions workflow
To start working with the workflow, create the .github/workflows directory within the repository.

```
name: Build, Test, and Raise Pull Request

on:

  push:

    branches: [ feature/* ]

jobs:

  build:

    runs-on: ubuntu-latest

    steps:

    - uses: actions/checkout@v2

    - name: Build the Docker image

      id: build

      run: docker build . --file Dockerfile --tag ${{ secrets.DOCKER_USER  }}/flask-app-gitops

    - name: Raise a Pull Request

      id: pull-request

      uses: repo-sync/pull-request@v2

      with:

        destination_branch: master

        github_token: ${{ secrets.GH_TOKEN }}
```

- Release gating with pull requests
As we saw in the previous section, the CI pipeline built the code, did a test, ran a container, and verified that everything is OK. 

- Continuous deployment with Flux CD
In the CI section, we created an application repository for building and testing the core application. In the case of CD, we would need to create an environment repository as that is what we will use for doing all deployments. 

- Flux CD is an open source CD tool initially developed by Weaveworks, and it works on the pull-based GitOps deployment model. It is a prevalent tool, and since it is a part of the Cloud Native Computing Foundation (CNCF)

- Managing sensitive configuration and Secrets
Sealed Secrets solves the problem of I can manage all my Kubernetes config in Git, except Secrets

- Installing the Sealed Secrets operator
To install the Sealed Secrets operator, all you need to do is to download the controller manifest from the latest release on https://github.com/bitnami-labs/sealed-secrets/releases. At the time of writing this book, https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.16.0/controller.yaml is the latest controller manifest.

- Installing kubeseal
To install the client-side utility, you can go to https://github.com/bitnami-labs/sealed-secrets/releases and get the kubeseal installation binary link from the page

- Creating Sealed Secrets
To create the Sealed Secret, we would first have to define the Kubernetes Secret resource