Jenkins Document;Note=Erxin


# Reference 
https://jenkins.io/doc/



# Using jenkins agents 
- To generate the SSH key pair, you have to execute a command line tool named ssh-keygen on a machine you have access to. It could be:




# Using jenkins 
- aborting a build 

Pipeline jobs can be stopped by sending an HTTP POST request to URL endpoints of a build.

BUILD ID URL/stop - aborts a Pipeline.

BUILD ID URL/term - forcibly terminates a build (should only be used if stop does not work).

BUILD ID URL/kill - hard kill a pipeline. This is the most destructive way to stop a pipeline and should only be used as a last resort.

- relevant projects need to be configured to Record fingerprints of the jar files

- go to jenkins_server/systemInfo and see the user.timezone system property.

- Jenkins installation sits at https://ci.jenkins.io, visiting https://ci.jenkins.io/api/ will show just the top-level API features available                   

       



# Guide Tour
- download and run 
http://mirrors.jenkins.io/war-stable/latest/jenkins.war

Run java -jar jenkins.war --httpPort=8080.

Browse to http://localhost:8080

- Jenkins Pipeline is typically written into a text file (called a Jenkinsfile) which in turn is checked into a project’s source control repository.

https://jenkins.io/doc/book/pipeline/jenkinsfile/

pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                echo 'Building..'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing..'
            }
        }
        stage('Deploy') {
            steps {
                echo 'Deploying....'
            }
        }
    }
}

- pipeline made of multiple steps 
    + "step" like a single command which performs a single action. When a step succeeds it moves onto the next step. 

    + all the steps in the Pipeline have successfully completed, the Pipeline is considered to have successfully executed

        * linux use sh to execute shell 
pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                sh 'echo "Hello World"'
                sh '''
                    echo "Multiline shell steps works too"
                    ls -lah
                '''
            }
        }
    }
}

        * window use bat to execute shell 
pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                bat 'set'
            }
        }
    }
}

    + timeout and retries and more 
pipeline {
    agent any
    stages {
        stage('Deploy') {
            steps {
                retry(3) {
                    sh './flakey-deploy.sh'
                }

                timeout(time: 3, unit: 'MINUTES') {
                    sh './health-check.sh'
                }
            }
        }
    }
}

The "Deploy" stage retries the flakey-deploy.sh script 3 times, and then waits for up to 3 minutes for the health-check.sh script to execute. If the health check script does not complete in 3 minutes, the Pipeline will be marked as having failed in the "Deploy" stage.

- finishing up pipeline with post section 
Jenkinsfile (Declarative Pipeline)
pipeline {
    agent any
    stages {
        stage('Test') {
            steps {
                sh 'echo "Fail!"; exit 1'
            }
        }
    }
    post {
        always {
            echo 'This will always run'
        }
        success {
            echo 'This will run only if successful'
        }
        failure {
            echo 'This will run only if failed'
        }
        unstable {
            echo 'This will run only if the run was marked as unstable'
        }
        changed {
            echo 'This will run only if the state of the Pipeline has changed'
            echo 'For example, if the Pipeline was previously failing but is now successful'
        }
    }
}

- define execution environment, This approach allows you to use practically any tool which can be packaged in a Docker container.

Jenkinsfile (Declarative Pipeline)
pipeline {
    agent {
        docker { image 'node:7-alpine' }
    }
    stages {
        stage('Test') {
            steps {
                sh 'node --version'
            }
        }
    }
}

Pipeline executes, Jenkins will automatically start the specified container and execute the defined steps within it:

- environment variables 
Jenkinsfile (Declarative Pipeline)
pipeline {
    agent {
        label '!windows'
    }

    environment {
        DISABLE_AUTH = 'true'
        DB_ENGINE    = 'sqlite'
    }

    stages {
        stage('Build') {
            steps {
                echo "Database engine is ${DB_ENGINE}"
                echo "DISABLE_AUTH is ${DISABLE_AUTH}"
                sh 'printenv'
            }
        }
    }
}

- cleaning up and notifications 
pipeline {
    agent any
    stages {
        stage('No-op') {
            steps {
                sh 'ls'
            }
        }
    }
    post {
        always {
            echo 'One way or another, I have finished'
            deleteDir() /* clean up our workspace */
        }
        success {
            echo 'I succeeeded!'
        }
        unstable {
            echo 'I am unstable :/'
        }
        failure {
            echo 'I failed :('
        }
        changed {
            echo 'Things were different before...'
        }
    }
}

post {
    failure {
        mail to: 'team@example.com',
             subject: "Failed Pipeline: ${currentBuild.fullDisplayName}",
             body: "Something is wrong with ${env.BUILD_URL}"
    }
}

also support hipchat and slack message 

- deployment 
pipeline {
    agent any
    options {
        skipStagesAfterUnstable()
    }
    stages {
        stage('Build') {
            steps {
                echo 'Building'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing'
            }
        }
        stage('Deploy') {
            steps {
                echo 'Deploying'
            }
        }
    }
}

stage('Deploy - Staging') {
    steps {
        sh './deploy staging'
        sh './run-smoke-tests'
    }
}
stage('Deploy - Production') {
    steps {
        sh './deploy production'
    }
}

    + ask for human input with input step 
Jenkinsfile (Declarative Pipeline)
pipeline {
    agent any
    stages {
        /* "Build" and "Test" stages omitted */

        stage('Deploy - Staging') {
            steps {
                sh './deploy staging'
                sh './run-smoke-tests'
            }
        }

        stage('Sanity check') {
            steps {
                input "Does the staging environment look ok?"
            }
        }

        stage('Deploy - Production') {
            steps {
                sh './deploy production'
            }
        }
    }
}

- add test stage for jenkinsfile
        stage('Deliver for development') {
            when {
                branch 'development'
            }
            steps {
                sh './jenkins/scripts/deliver-for-development.sh'
                input message: 'Finished using the web site? (Click "Proceed" to continue)'
                sh './jenkins/scripts/kill.sh'
            }
        }
        stage('Deploy for production') {
            when {
                branch 'production'
            }
            steps {
                sh './jenkins/scripts/deploy-for-production.sh'
                input message: 'Finished using the web site? (Click "Proceed" to continue)'
                sh './jenkins/scripts/kill.sh'
            }
        }


# Install jenkins in docker 
- docker image 

use is the jenkinsci/blueocean image (from the Docker Hub repository)

A new jenkinsci/blueocean image is published each time a new release of Blue Ocean is published

Jenkins Docker images you can use (accessible through jenkins/jenkins on Docker Hub)
- install docker 
+ create a bridge network in docker using docker network 
$ docker network create jenkins

- Create the following volumes to share the Docker client TLS certificates needed to connect to the Docker daemon and persist the Jenkins data 

$ docker volume create jenkins-docker-certs
$ docker volume create jenkins-data

- download and run the docker:dind docker image using the following docker container run command 

$ docker container run \
  --name jenkins-docker \ 
  --rm \ 
  --detach \ 
  --privileged \ 
  --network jenkins \ 
  --network-alias docker \ 
  --env DOCKER_TLS_CERTDIR=/certs \ 
  --volume jenkins-docker-certs:/certs/client \ 
  --volume jenkins-data:/var/jenkins_home \ 
  --publish 2376:2376 \ 
  docker:dind 
 
--env Maps the /certs/client directory inside the container to a Docker volume named jenkins-docker-certs as created above.

Maps the /var/jenkins_home directory inside the container to the Docker volume named jenkins-data as created above.

    
- Download the jenkinsci/blueocean image and run it as a container in Docker
docker container run \
  --name jenkins-blueocean \ 
  --rm \ 
  --detach \ 
  --network jenkins \ 
  --env DOCKER_HOST=tcp://docker:2376 \ 
  --env DOCKER_CERT_PATH=/certs/client \
  --env DOCKER_TLS_VERIFY=1 \
  --publish 8080:8080 \ 
  --publish 50000:50000 \ 
  --volume jenkins-data:/var/jenkins_home \ 
  --volume jenkins-docker-certs:/certs/client:ro \ 
  jenkinsci/blueocean 
  
- on window 
$ docker network create jenkins
$ docker volume create jenkins-docker-certs
$ docker volume create jenkins-data
  
download docker:dind docker image using docker container run command 

$ docker container run --name jenkins-docker --rm --detach ^
  --privileged --network jenkins --network-alias docker ^
  --env DOCKER_TLS_CERTDIR=/certs ^
  --volume jenkins-docker-certs:/certs/client ^
  --volume jenkins-data:/var/jenkins_home ^
  docker:dind

download blue ocean image jenkinsci/blueocean 
docker container run --name jenkins-blueocean --rm --detach ^
  --network jenkins --env DOCKER_HOST=tcp://docker:2376 ^
  --env DOCKER_CERT_PATH=/certs/client --env DOCKER_TLS_VERIFY=1 ^
  --volume jenkins-data:/var/jenkins_home ^
  --volume jenkins-docker-certs:/certs/client:ro ^
  --publish 8080:8080 --publish 50000:50000 jenkinsci/blueocean
  
accessing jenkins/blue ocean docker container 
$ docker container exec -it jenkins-blueocean bash 

accessing the jenkins console log through docker logs 
$ docker container logs <docker-container-name>
$ docker container logs jenkins-blueocean




# Install jenkins from war 
- WAR file
The Web application ARchive (WAR) file version of Jenkins can be installed on any operating system or platform that supports Java.

Download the latest stable Jenkins WAR file to an appropriate directory on your machine.

Open up a terminal/command prompt window to the download directory.

Run the command java -jar jenkins.war.

Browse to http://localhost:8080 and wait until the Unlock Jenkins page appears.

Continue on with the Post-installation setup wizard below.




# Pipeline
Jenkins pipeline is a suite of plugins which supports implementing and integrating continuous delivery pipelines into jenkins

- A continuous delivery (CD) pipeline is an automated expression of your process for getting software from version control right through to your users and customers. 

The definition of a Jenkins Pipeline is written into a text file (called a Jenkinsfile) which in turn can be committed to a project’s source control repository. pipeline as code, a part of the application to be version and reviewed like any other code 

- can use the Declarative Directive Generator to help you get started with configuring the directives and sections in your Declarative Pipeline.

- create a jenkinsfile and committing it to source control provides a number of immediate benefits 

automatically creates a pipeline build process for all branches and pull requests 
code review/itegration on the pipeline 
audit trail fro the pipeline 
single source of truth for the pipeline which can be viewed and edited by mutliple members of the project 

- jenkinsfile pipeline can be written in two syntax 
    + declarative 
    provides richer syntactical features over Scripted Pipeline syntax
    
    + scripted 
    
- why pipeline 
code 
durable 
pausable 
versatile 
extensible 

- concepts 

    pipeline, a user-defined model of a CD, defines your entire build process, typically includes stages for building an application, testing it and then delivering it 

    node is a machine to executing a pipeline 

    stage, a block defines a conceptually distinct subset of tasks performed through the entire pipeline 

    step, a single task fundamentally, a step tells jenkins what to do at a particular point in time. It can execute a shell use the sh step, bat for windows 
    
    
    
    
    
## Pipeline syntax 
- declarative pipeline fundamentals, jenkinsfile example 
 
``` 
pipeline {
    agent any 
    stages {
        stage('Build') { 
            steps {
                // 
            }
        }
        stage('Test') { 
            steps {
                // 
            }
        }
        stage('Deploy') { 
            steps {
                // 
            }
        }
    }
}

```
    follow the same rules as Groovy’s syntax with the following exceptions
    top level pipeline must be pipeline {}
    no semicolons 
    block must only consist of sections, directives, steps or assignment statement 

    agent is Declarative Pipeline-specific syntax that instructs Jenkins to allocate an executor (on a node) and workspace for the entire Pipeline.

    + scripted pipeline fundamentals
    
    Schedules the steps contained within the block to run by adding an item to the Jenkins queue. As soon as an executor is free on a node, the steps will run

    Creates a workspace (a directory specific to that particular Pipeline) where work can be done on files checked out from source control

``` 
node {  
    stage('Build') { 
        // 
    }
    stage('Test') { 
        // 
    }
    stage('Deploy') { 
        // 
    }
}
```

blocks in a Scripted Pipeline provides clearer visualization of each `stage’s subset of tasks/steps in the Jenkins UI.

- pipeline example 
pipeline { 
    agent any 
    options {
        skipStagesAfterUnstable()
    }
    stages {
        stage('Build') { 
            steps { 
                sh 'make' 
            }
        }
        stage('Test'){
            steps {
                sh 'make check'
                junit 'reports/**/*.xml' 
            }
        }
        stage('Deploy') {
            steps {
                sh 'make publish'
            }
        }
    }
}

agent is Declarative Pipeline-specific syntax that instructs Jenkins to allocate an executor (on a node) and workspace for the entire Pipeline.

https://plugins.jenkins.io/workflow-durable-task-step

https://plugins.jenkins.io/pipeline-stage-view

- Sections

agent, specifies where the entire pipeline or specific stage will execute in the jenkins environment 

    + top level agents, outer most level of the pipeline, options are invoked after entering the agent, when using timeout 

node("myAgent") {
    timeout(unit: 'SECONDS', time: 5) {
        stage("One"){
            sleep 10
            echo 'hello'
        }
    }
}

    + stage agents 
    
agents declared within a stage, the options are invoked before entering the agent and before checking any when conditions.

    + agent paremeters, agent section supports a few different types of parameters.
    
     any, any avaliable 
        
        none, no global agent will be allocated, each stage section will need to contgain its own agent section 
        
        label, execute the on specific label nodes 
        
        node, behave similar to label but accept additional options like customWorkspace
        
        docker, accept docker based pipelines, on a node matching the optionally defined label parameter. alwaysPull option will force a docker pull 
        
```
agent {
    docker {
        image 'maven:3.8.1-adoptopenjdk-11'
        label 'my-defined-label'
        args  '-v /tmp:/tmp'
        registryUrl 'https://myregistry.com/'
        registryCredentialsId 'myPredefinedCredentialsInJenkins'
    }
}
```       
       dockerfile, a container built from a Dockerfile contained in the source repository. In order to use this option, the Jenkinsfile must be loaded from either a Multibranch Pipeline or a Pipeline from SCM. 
       
```
agent {
    // Equivalent to "docker build -f Dockerfile.build --build-arg version=1.0.2 ./build/
    dockerfile {
        filename 'Dockerfile.build'
        dir 'build'
        label 'my-defined-label'
        additionalBuildArgs  '--build-arg version=1.0.2'
        args '-v /tmp:/tmp'
        registryUrl 'https://myregistry.com/'
        registryCredentialsId 'myPredefinedCredentialsInJenkins'
    }
}
```       
        kubernetes,  inside a pod deployed on a Kubernetes cluster. In order to use this option, the Jenkinsfile must be loaded from either a Multibranch Pipeline or a Pipeline from SCM
        
```
agent {
    kubernetes {
        defaultContainer 'kaniko'
        yaml '''
kind: Pod
spec:
  containers:
  - name: kaniko
    image: gcr.io/kaniko-project/executor:debug
    imagePullPolicy: Always
    command:
    - sleep
    args:
    - 99d
    volumeMounts:
      - name: aws-secret
        mountPath: /root/.aws/
      - name: docker-registry-config
        mountPath: /kaniko/.docker
  volumes:
    - name: aws-secret
      secret:
        secretName: aws-secret
    - name: docker-registry-config
      configMap:
        name: docker-registry-config
'''
   }
```     
         create a secret aws-secret for Kaniko to be able to authenticate with ECR. This secret should contain the contents of ~/.aws/credentials. 

        * common options 
        
        label 
        
        customWorkspace, a path which pipeline/stage this agent is applied to within this custom workspace, rather than the default 
        
        reuseNode, false by default. If true, run the container on the node specified at the top-level of the Pipeline
        
        args, a string, pass to docker run 


- post, The post section defines one or more additional steps that are run upon the completion of a Pipeline’s or stage’s run (depending on the location of the post section within the Pipeline). post can support any of the following 

post-condition blocks: always, changed, fixed, regression, aborted, failure, success, unstable, unsuccessful, and cleanup.

always 

changed, Only run the steps in post if the current Pipeline’s or stage’s run has a different completion status from its previous run.

fixed, Only run the steps in post if the current Pipeline’s or stage’s run is successful and the previous run failed or was unstable.

regression, Only run the steps in post if the current Pipeline’s or stage’s run’s status is failure, unstable, or aborted and the previous run was successful.

failure, Only run the steps in post if the current Pipeline’s or stage’s run has a "failed" status, typically denoted by red in the web UI.

unstable Only run the steps in post if the current Pipeline’s or stage’s run has an "unstable" status, usually caused by test failures, code violations, etc. This is typically denoted by yellow in the web UI.

unsuccessful, Only run the steps in post if the current Pipeline’s or stage’s run has not a "success" status

cleanup, run the steps in the post condition after every post condition has been evaluated 


pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                echo 'Hello World'
            }
        }
    }
    post { 
        always { 
            echo 'I will always say Hello again!'
        }
    }
}

- stages, a sequence of one or more stage directives, the stages section is where the bulk of the "work" described by a Pipeline, the stages section will typically follow the directives such as agent, options, etc.


pipeline {
    agent any
    stages { 
        stage('Example') {
            steps {
                echo 'Hello World'
            }
        }
    }
}

- steps, The steps section defines a series of one or more steps to be executed

pipeline {
    agent any
    stages {
        stage('Example') {
            steps { 
                echo 'Hello World'
            }
        }
    }
}

- Directives 

- The environment directive specifies a sequence of key-value pairs which will be defined as environment variables for all steps

a special helper method credentials() which can be used to access pre-defined Credentials by their identifier in the Jenkins environment

    + support credentials types 
    
    Secret Text 
    
    Secret File 
    
    Username and password 
    
    SSH with private key 

pipeline {
    agent any
    environment { 
        CC = 'clang'
    }
    stages {
        stage('Example') {
            environment { 
                AN_ACCESS_KEY = credentials('my-predefined-secret-text') 
            }
            steps {
                sh 'printenv'
            }
        }
    }
}

An environment directive used in the top-level pipeline block will apply to all steps within the Pipeline.

An environment directive defined within a stage will only apply the given environment variables to steps
    
pipeline {
    agent any
    stages {
        stage('Example Username/Password') {
            environment {
                SERVICE_CREDS = credentials('my-predefined-username-password')
            }
            steps {
                sh 'echo "Service user is $SERVICE_CREDS_USR"'
                sh 'echo "Service password is $SERVICE_CREDS_PSW"'
                sh 'curl -u $SERVICE_CREDS https://myservice.example.com'
            }
        }
        stage('Example SSH Username with private key') {
            environment {
                SSH_CREDS = credentials('my-predefined-ssh-creds')
            }
            steps {
                sh 'echo "SSH private key is located at $SSH_CREDS"'
                sh 'echo "SSH user is $SSH_CREDS_USR"'
                sh 'echo "SSH passphrase is $SSH_CREDS_PSW"'
            }
        }
    }
}

- options, The options directive allows configuring Pipeline-specific options from within the Pipeline itself.

buildDiscarder, persist artifacts and console output for specific number of runs 
options { buildDiscarder(logRotator(numToKeepStr: '1')) }

checkoutToSubdirectory Perform the automatic source control checkout in a subdirectory of the workspace. 

disableConcurrentBuilds Disallow concurrent executions of the Pipeline. 

disableResume Do not allow the pipeline to resume if the controller restarts.

newContainerPerStage, used with docker or dockerfile 

overrideIndexTriggers Allows overriding default treatment of branch indexing triggers. 

preserveStashes Preserve stashes from completed builds, for use with stage restarting.

quietPeriod Set the quiet period, in seconds, for the Pipeline, overriding the global default. 

retry On failure, retry the entire Pipeline the specified number of times
options { retry(3) }

skipDefaultCheckout Skip checking out code from source control by default in the agent directive.
options { skipDefaultCheckout() }

skipStagesAfterUnstable Skip stages once the build status has gone to UNSTABLE.

timeout Set a timeout period for the Pipeline run, after which Jenkins should abort the Pipeline. For example: options { timeout(time: 1, unit: 'HOURS') }

timestamps Prepend all console output generated by the Pipeline run with the time at which the line was emitted. For example: options { timestamps() }

parallelsAlwaysFailFast Set failfast true for all subsequent parallel stages in the pipeline.

    + stage options,  similar to the options directive at the root of the Pipeline. However, the stage-level options can only contain steps like retry, timeout, or timestamps.  

- parameters directive provides a list of parameters that a user should provide when triggering the Pipeline. parameters are made available to Pipeline steps via the params object 

string A parameter of a string type, for example: 
parameters { string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') }

text A text parameter, which can contain multiple lines, for example: 
parameters { text(name: 'DEPLOY_TEXT', defaultValue: 'One\nTwo\nThree\n', description: '') }

booleanParam A boolean parameter, for example: 
parameters { booleanParam(name: 'DEBUG_BUILD', defaultValue: true, description: '') }

choice A choice parameter, for example: 
parameters { choice(name: 'CHOICES', choices: ['one', 'two', 'three'], description: '') }

password  A password parameter, for example: 
parameters { password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'A secret password') }
 
 
pipeline {
    agent any
    parameters {
        string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?')

        text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person')

        booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value')

        choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something')

        password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password')
    }
    stages {
        stage('Example') {
            steps {
                echo "Hello ${params.PERSON}"

                echo "Biography: ${params.BIOGRAPHY}"

                echo "Toggle: ${params.TOGGLE}"

                echo "Choice: ${params.CHOICE}"

                echo "Password: ${params.PASSWORD}"
            }
        }
    }
}

- triggers, webhooks-based integration will likely already be present. The triggers currently available are cron, pollSCM and upstream.

    + acron
a cron-style string to define a regular interval, triggers { cron('H */4 * * 1-5') }

    + pollSCM 
Accepts a cron-style string to define a regular interval at which Jenkins should check for new source changes.
triggers { pollSCM('H */4 * * 1-5') }
    
    + upstream 
Accepts a comma-separated string of jobs and a threshold. 
triggers { upstream(upstreamProjects: 'job1,job2', threshold: hudson.model.Result.SUCCESS) }

- cron time string is five values 

[ * (MINUTE TIME FORMAT) ] [ * (HOUR TIME FORMAT) ] [ * (DAY OF THE MONTH TIME FORMAT) ] [ * (REPRESENT MONTH TIME FORMAT) ] [ * (DAY OF WEEK TIME FORMAT) ] [ PATH OF THE SCRIPT OR JOBS or APPLICATION ]

    + examples 
    
//run automatically every next 7 minutes 
*/7 * * * * /root/time.sh

//every 4 hour 
* */4 * * * /root/time.sh

//weekly 
@weekly /root/time.sh

* specifies all valid values

M-N specifies a range of values

M-N/X or */X steps by intervals of X through the specified range or whole valid range

A,B,…​,Z enumerates multiple values

- The stage directive goes in the stages section and should contain a steps section, an optional agent section
// Declarative //
pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                echo 'Hello World'
            }
        }
    }
}

- tool A section defining tools to auto-install and put on the PATH. This is ignored if agent none is specified.

pipeline {
    agent any
    tools {
        maven 'apache-maven-3.0.1' 
    }
    stages {
        stage('Example') {
            steps {
                sh 'mvn --version'
            }
        }
    }
}

tool name must be pre-configured in Jenkins under Manage Jenkins → Global Tool Configuration.

    + supported 
    
maven
jdk
gradle

- input, The input directive on a stage allows you to prompt for input, using the input step. will be available in the environment for the rest of the stage

pipeline {
    agent any
    stages {
        stage('Example') {
            input {
                message "Should we continue?"
                ok "Yes, we should."
                submitter "alice,bob"
                parameters {
                    string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?')
                }
            }
            steps {
                echo "Hello, ${PERSON}, nice to meet you."
            }
        }
    }
}

- The when directive allows the Pipeline to determine whether the stage should be executed depending on the given condition. 

nesting conditions: not, allOf, or anyOf. Nesting conditions may be nested to any arbitrary depth.

    + built-in condition 
    
the branch being built matches the branch pattern, when { branch 'master' }. Note that this only works on a multibranch Pipeline.
when { branch pattern: "release-\\d+", comparator: "REGEXP"}

build is building a tag. Example: when { buildingTag() }

changelog Execute the stage if the build’s SCM changelog contains a given regular expression pattern
when { changelog '.*^\\[DEPENDENCY\\] .+$' }

changeset Execute the stage if the build’s SCM changeset contains one or more files matching the given pattern. 
when { changeset pattern: ".TEST\\.java", comparator: "REGEXP" } or when { changeset pattern: "*/*TEST.java", caseSensitive: true }

changeRequest, executes the stage if the current build is for a "change request" (a.k.a. Pull Request on GitHub and Bitbucket 
when { changeRequest() }. Possible attributes are id, target, branch, fork, url, title, author, authorDisplayName, and authorEmail
when { changeRequest target: 'master' }.
when { changeRequest authorEmail: "[\\w_-.]+@example.com", comparator: 'REGEXP' }

environment, when { environment name: 'DEPLOY_TO', value: 'production' }

equals, the expected value is equal to the actual value, for example: when { equals expected: 2, actual: currentBuild.number }

expression, when the specified Groovy expression evaluates to true, for example
when { expression { return params.DEBUG_BUILD } }

tag Execute the stage if the TAG_NAME variable matches the given pattern.  when { tag "release-*" }. If an empty pattern is provided the stage will execute if the TAG_NAME variable exists (same as buildingTag()).
when { tag pattern: "release-\\d+", comparator: "REGEXP"}

not,  Execute the stage when the nested condition is false. 
when { not { branch 'master' } }

allOf Execute the stage when all of the nested conditions are true

anyOf Execute the stage when at least one of the nested conditions is true.

triggeredBy Execute the stage when the current build has been triggered by the param given
when { triggeredBy 'SCMTrigger' }
when { triggeredBy 'TimerTrigger' }
when { triggeredBy 'BuildUpstreamCause' }
when { triggeredBy cause: "UserIdCause", detail: "vlinde" }

beforeAgent evaluating when before entering agent in a stage by specifying the beforeAgent option within the when block. If beforeAgent is set to true 

beforeInput the when condition for a stage will not be evaluated before the input, if one is defined. However, this can be changed by specifying the beforeInput option

beforeOptions the when condition for a stage will be evaluated after entering the options for that stage, if any are defined

    + when examples 
pipeline {
    agent any
    stages {
        stage('Example Build') {
            steps {
                echo 'Hello World'
            }
        }
        stage('Example Deploy') {
            when {
                allOf {
                    branch 'production'
                    environment name: 'DEPLOY_TO', value: 'production'
                }
            }
            steps {
                echo 'Deploying'
            }
        }
    }
}

pipeline {
    agent any
    stages {
        stage('Example Build') {
            steps {
                echo 'Hello World'
            }
        }
        stage('Example Deploy') {
            when {
                branch 'production'
                anyOf {
                    environment name: 'DEPLOY_TO', value: 'production'
                    environment name: 'DEPLOY_TO', value: 'staging'
                }
            }
            steps {
                echo 'Deploying'
            }
        }
    }
}

pipeline {
    agent any
    stages {
        stage('Example Build') {
            steps {
                echo 'Hello World'
            }
        }
        stage('Example Deploy') {
            when {
                expression { BRANCH_NAME ==~ /(production|staging)/ }
                anyOf {
                    environment name: 'DEPLOY_TO', value: 'production'
                    environment name: 'DEPLOY_TO', value: 'staging'
                }
            }
            steps {
                echo 'Deploying'
            }
        }
    }
}

- sequential stages, a stages section containing a list of nested stages to be run in sequential order. a stage directive within a parallel or matrix block can use all other functionality of a stage, including agent, tools, when, etc.

pipeline {
    agent none
    stages {
        stage('Non-Sequential Stage') {
            agent {
                label 'for-non-sequential'
            }
            steps {
                echo "On Non-Sequential Stage"
            }
        }
        stage('Sequential') {
            agent {
                label 'for-sequential'
            }
            environment {
                FOR_SEQUENTIAL = "some-value"
            }
            stages {
                stage('In Sequential 1') {
                    steps {
                        echo "In Sequential 1"
                    }
                }
                stage('In Sequential 2') {
                    steps {
                        echo "In Sequential 2"
                    }
                }
                stage('Parallel In Sequential') {
                    parallel {
                        stage('In Parallel 1') {
                            steps {
                                echo "In Parallel 1"
                            }
                        }
                        stage('In Parallel 2') {
                            steps {
                                echo "In Parallel 2"
                            }
                        }
                    }
                }
            }
        }
    }
}

- parallel, a parallel section containing a list of nested stages to be run in parallel. Note that a stage must have one and only one of steps, stages, parallel, or matrix. However, a stage directive within a parallel or matrix block can use all other functionality of a stage, including agent, tools, when, etc. 

you can force your parallel stages to all be aborted when any one of them fails, by adding failFast true to the stage containing the parallel. or an option to the pipeline definition: parallelsAlwaysFailFast() in the pipeline. 

```
pipeline {
    agent any
    stages {
        stage('Non-Parallel Stage') {
            steps {
                echo 'This stage will be executed first.'
            }
        }
        stage('Parallel Stage') {
            when {
                branch 'master'
            }
            failFast true
            parallel {
                stage('Branch A') {
                    agent {
                        label "for-branch-a"
                    }
                    steps {
                        echo "On Branch A"
                    }
                }
                stage('Branch B') {
                    agent {
                        label "for-branch-b"
                    }
                    steps {
                        echo "On Branch B"
                    }
                }
                stage('Branch C') {
                    agent {
                        label "for-branch-c"
                    }
                    stages {
                        stage('Nested 1') {
                            steps {
                                echo "In stage Nested 1 within Branch C"
                            }
                        }
                        stage('Nested 2') {
                            steps {
                                echo "In stage Nested 2 within Branch C"
                            }
                        }
                    }
                }
            }
        }
    }
}
```

```
pipeline {
    agent any
    options {
        parallelsAlwaysFailFast()
    }
    stages {
        stage('Non-Parallel Stage') {
            steps {
                echo 'This stage will be executed first.'
            }
        }
        stage('Parallel Stage') {
            when {
                branch 'master'
            }
            parallel {
                stage('Branch A') {
                    agent {
                        label "for-branch-a"
                    }
                    steps {
                        echo "On Branch A"
                    }
                }
                stage('Branch B') {
                    agent {
                        label "for-branch-b"
                    }
                    steps {
                        echo "On Branch B"
                    }
                }
                stage('Branch C') {
                    agent {
                        label "for-branch-c"
                    }
                    stages {
                        stage('Nested 1') {
                            steps {
                                echo "In stage Nested 1 within Branch C"
                            }
                        }
                        stage('Nested 2') {
                            steps {
                                echo "In stage Nested 2 within Branch C"
                            }
                        }
                    }
                }
            }
        }
    }
}

```

- Matrix stages in Declarative Pipeline may have a matrix section defining a multi-dimensional matrix of name-value combinations to be run in parallel.

"cells" in a matrix. Each cell in a matrix can include one or more stages to be run sequentially using the configuration for that cell. must have only one of steps, stages, parallel, or matrix. parallel and matrix are not allow to nest. a stage in a parallel or matrix can use all other functionality. 

failFast, parallelsAlwaysFailFast()

The matrix section must include an axes section and a stages section. he axes section defines the values for each axis in the matrix. The stages section defines a list of stages to run sequentially. n excludes section to remove invalid cells from the matrix 

    + axes, Each axis consists of a name and a list of values
    
matrix {
    axes {
        axis {
            name 'PLATFORM'
            values 'linux', 'mac', 'windows'
        }
        axis {
            name 'BROWSER'
            values 'chrome', 'edge', 'firefox', 'safari'
        }
    }
    stages {
        stage('build-and-test') {
            // ...
        }
    }
    //optional 
    excludes {
        exclude {
            axis {
                name 'PLATFORM'
                notValues 'mac'
            }
            axis {
                name 'ARCHITECTURE'
                values '32-bit', '64-bit'
            }
        }
    }
}

exclude axis directives can use notValues instead of values. These will exclude cells that do not match one of the values passed to notValues.

    + Matrix cell-level directives, optional. The axis and exclude directives define the static set of cells that make up the matrix. 

agent

environment

input

options

post

tools

when

    + matrix example 
pipeline {
    parameters {
        choice(name: 'PLATFORM_FILTER', choices: ['all', 'linux', 'windows', 'mac'], description: 'Run on specific platform')
    }
    agent none
    stages {
        stage('BuildAndTest') {
            matrix {
                agent {
                    label "${PLATFORM}-agent"
                }
                when { anyOf {
                    expression { params.PLATFORM_FILTER == 'all' }
                    expression { params.PLATFORM_FILTER == env.PLATFORM }
                } }
                axes {
                    axis {
                        name 'PLATFORM'
                        values 'linux', 'windows', 'mac'
                    }
                    axis {
                        name 'BROWSER'
                        values 'firefox', 'chrome', 'safari', 'edge'
                    }
                }
                excludes {
                    exclude {
                        axis {
                            name 'PLATFORM'
                            values 'linux'
                        }
                        axis {
                            name 'BROWSER'
                            values 'safari'
                        }
                    }
                    exclude {
                        axis {
                            name 'PLATFORM'
                            notValues 'windows'
                        }
                        axis {
                            name 'BROWSER'
                            values 'edge'
                        }
                    }
                }
                stages {
                    stage('Build') {
                        steps {
                            echo "Do Build for ${PLATFORM} - ${BROWSER}"
                        }
                    }
                    stage('Test') {
                        steps {
                            echo "Do Test for ${PLATFORM} - ${BROWSER}"
                        }
                    }
                }
            }
        }
    }
}

- Steps, Declarative Pipelines may use all the available steps documented in the Pipeline Steps reference

- The script step takes a block of Scripted Pipeline and executes that in the Declarative Pipeline

pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                echo 'Hello World'

                script {
                    def browsers = ['chrome', 'firefox']
                    for (int i = 0; i < browsers.size(); ++i) {
                        echo "Testing the ${browsers[i]} browser"
                    }
                }
            }
        }
    }
}

- flow control 

node {
    stage('Example') {
        if (env.BRANCH_NAME == 'master') {
            echo 'I only execute on the master branch'
        } else {
            echo 'I execute elsewhere'
        }
    }
}

node {
    stage('Example') {
        try {
            sh 'exit 1'
        }
        catch (exc) {
            echo 'Something failed, I should sound the klaxons!'
            throw
        }
    }
}

- Scripted Pipeline must serialize data back to the controller. Due to this design requirement, some Groovy idioms such as collection.each { item → /* perform operation */ } are not fully supported.

a general-purpose DSL [2] built with Groovy. Most functionality provided by the Groovy language is made available


## Getting started with pipeline 
- define a pipeline 
    + through blue ocean, after setting up a pipeline in blue ocean, it will helps you create pipeline's jenkinsfile into source control 
    
    + through the classic UI 
    Jenkins > New Item > Pipeline > enter your pipeline code into the script text area 
    
    + In SCM, Choose pipeline script from SCM option, Jenkins can then check out your Jenkinsfile from source control as part of your Pipeline project’s build process and then proceed to execute your Pipeline.
    
    create pipeline from classic UI till the step 5. Definition field, choose the Pipeline script from SCM option
    
    In the script path field, specify the location of your Jenkinsfile. The default value of this field assumes that your jenkinsfile is named Jenkinsfile and located at the root of the repository 

    if your IDE is not correctly syntax highlighting your Jenkinsfile, try inserting the line #!/usr/bin/env groovy at the top of the Jenkinsfile
    
- The built-in documentation can be found globally at ${YOUR_JENKINS_URL}/pipeline-syntax
- Select the desired step in the Sample Step dropdown menu
    + ${YOUR_JENKINS_URL}/pipeline-syntax.
    + desired step in the Sample Step dropdown menu
    + Generate Pipeline Script to create a snippet of Pipeline 
    
- Global variable reference 
env, environment variables,  reference at ${YOUR_JENKINS_URL}/pipeline-syntax/globals
params, all aprameters read-only Map, for example: params.MY_PARAM_NAME
currentBuild,  such as currentBuild.result, currentBuild.displayName
scm, represents the configuration in a multibranch build 
complete global variable list, ${YOUR_JENKINS_URL}/pipeline-syntax/globals for a complete

- declarative directive generator
    + directly to ${YOUR_JENKINS_URL}/directive-generator.
    + Click Generate Directive to create the directive’s configuration to copy into your Pipeline

- a Jenkins Pipeline is written into a text file (called a Jenkinsfile) which in turn can be committed to a project’s source control repository    

Automatically creates a Pipeline build process for all branches and pull requests.

Code review/iteration on the Pipeline (along with the remaining source code).

Audit trail for the Pipeline.

Single source of truth [3] for the Pipeline, which can be viewed and edited by multiple members of the project.

- the Blue Ocean UI helps you set up your Pipeline project, and automatically creates and writes your Pipeline (i.e. the Jenkinsfile) for you through the graphical Pipeline editor.

- The built-in "Snippet Generator" utility is helpful for creating bits of code for individual steps





## Using a jenkinsfile 
- benefits 

Code review/iteration on the Pipeline

Audit trail for the Pipeline

Single source of truth [2] for the Pipeline, which can be viewed and edited by multiple members of the project

- example jenkinsfile 

Jenkinsfile (Declarative Pipeline)
pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                echo 'Building..'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing..'
            }
        }
        stage('Deploy') {
            steps {
                echo 'Deploying....'
            }
        }
    }
}

-  create a new Jenkinsfile in the root directory of the project.

Jenkinsfile (Scripted Pipeline)
node {
    checkout scm 
    /* .. snip .. */
}

The checkout step will checkout code from source control;

- build, *nix can use bash, window can use batch  

pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                sh 'make' 
                archiveArtifacts artifacts: '**/target/*.jar', fingerprint: true 
            }
        }
    }
}

- test 

Jenkinsfile (Declarative Pipeline)
pipeline {
    agent any

    stages {
        stage('Test') {
            steps {
                /* `make check` returns non-zero on test failures,
                * using `true` to allow the Pipeline to continue nonetheless
                */
                sh 'make check || true' 
                junit '**/target/*.xml' 
            }
        }
    }
}

sh step always sees a zero exit code, giving the junit step the opportunity to capture and process the test reports

- deploy, Deployment can imply a variety of steps, depending on the project or organization requirements

pipeline {
    agent any

    stages {
        stage('Deploy') {
            when {
              expression {
                currentBuild.result == null || currentBuild.result == 'SUCCESS' 
              }
            }
            steps {
                sh 'make publish'
            }
        }
    }
}

- using environment variable 

Pipeline is documented at ${YOUR_JENKINS_URL}/pipeline-syntax/globals#env 

- setting environment variables dynamically, run time and can be used by shell scripts (sh), Windows batch scripts (bat) and PowerShell scripts (powershell). Each script can either returnStatus or returnStdout. 

pipeline {
    agent any 
    environment {
        // Using returnStdout
        CC = """${sh(
                returnStdout: true,
                script: 'echo "clang"'
            )}""" 
        // Using returnStatus
        EXIT_STATUS = """${sh(
                returnStatus: true,
                script: 'exit 1'
            )}"""
    }
    stages {
        stage('Example') {
            environment {
                DEBUG_FLAGS = '-g'
            }
            steps {
                sh 'printenv'
            }
        }
    }
}

- handling credentials 

Jenkins' declarative Pipeline syntax has the credentials() helper method (used within the environment directive) which supports secret text, username and password, as well as secret file credentials. 

respective credential IDs jenkins-aws-secret-key-id and jenkins-aws-secret-access-key.

Jenkinsfile (Declarative Pipeline)
pipeline {
    agent {
        // Define agent details here
    }
    environment {
        AWS_ACCESS_KEY_ID     = credentials('jenkins-aws-secret-key-id')
        AWS_SECRET_ACCESS_KEY = credentials('jenkins-aws-secret-access-key')
    }
    stages {
        stage('Example stage 1') {
            steps {
                // 
            }
        }
        stage('Example stage 2') {
            steps {
                // 
            }
        }
    }
}

Any sensitive information in credential IDs themselves (such as usernames) are also returned as “****” in the Pipeline run’s outpu

stage’s steps using the syntax $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY.

environment {
    BITBUCKET_COMMON_CREDS = credentials('jenkins-bitbucket-common-creds')
}

BITBUCKET_COMMON_CREDS - contains a username and a password separated by a colon in the format username:password.
BITBUCKET_COMMON_CREDS_USR - an additional variable containing the username component only.
BITBUCKET_COMMON_CREDS_PSW - an additional variable containing the password component only.

- jenkins pipeline 

pipeline {
    agent {
        // Define agent details here
    }
    stages {
        stage('Example stage 1') {
            environment {
                BITBUCKET_COMMON_CREDS = credentials('jenkins-bitbucket-common-creds')
            }
            steps {
                // 
            }
        }
        stage('Example stage 2') {
            steps {
                // 
            }
        }
    }
}

this stage’s steps and can be referenced using the syntax:
$BITBUCKET_COMMON_CREDS
$BITBUCKET_COMMON_CREDS_USR
$BITBUCKET_COMMON_CREDS_PSW

- secret files, uploaded to Jenkins. Secret files are used for credentials that are:

pipeline {
    agent {
        // Define agent details here
    }
    environment {
        // The MY_KUBECONFIG environment variable will be assigned
        // the value of a temporary file.  For example:
        //   /home/user/.jenkins/workspace/cred_test@tmp/secretFiles/546a5cf3-9b56-4165-a0fd-19e2afe6b31f/kubeconfig.txt
        MY_KUBECONFIG = credentials('my-kubeconfig')
    }
    stages {
        stage('Example stage 1') {
            steps {
                sh("kubectl --kubeconfig $MY_KUBECONFIG get pods")
            }
        }
    }
}

- other credential types 

Bindings, click Add and choose from the dropdown

SSH User Private Key - to handle SSH public/private key pair credentials

Certificate - to handle PKCS#12 certificates, from which you can specify

Keystore Variable

SSH User Private Key example

withCredentials(bindings: [sshUserPrivateKey(credentialsId: 'jenkins-ssh-key-for-abc', \
                                             keyFileVariable: 'SSH_KEY_FOR_ABC', \
                                             passphraseVariable: '', \
                                             usernameVariable: '')]) {
  // some block
}

    + jenkins file 
    
pipeline {
    agent {
        // define agent details
    }
    stages {
        stage('Example stage 1') {
            steps {
                withCredentials(bindings: [sshUserPrivateKey(credentialsId: 'jenkins-ssh-key-for-abc', \
                                                             keyFileVariable: 'SSH_KEY_FOR_ABC')]) {
                  // 
                }
                withCredentials(bindings: [certificate(credentialsId: 'jenkins-certificate-for-xyz', \
                                                       keystoreVariable: 'CERTIFICATE_FOR_XYZ', \
                                                       passwordVariable: 'XYZ-CERTIFICATE-PASSWORD')]) {
                  // 
                }
            }
        }
        stage('Example stage 2') {
            steps {
                // 
            }
        }
    }
}

node {
  withCredentials([string(credentialsId: 'mytoken', variable: 'TOKEN')]) {
    sh /* WRONG! */ """
      set +x
      curl -H 'Token: $TOKEN' https://some.api/
    """
    sh /* CORRECT */ '''
      set +x
      curl -H 'Token: $TOKEN' https://some.api/
    '''
  }
}

- string interpolation 

    + Groovy string interpolation should never be used with credentials.

    + interpolation with environment variable 
    
    sh("curl -u ${EXAMPLE_CREDS_USR}:${EXAMPLE_CREDS_PSW} https://example.com/")

- handling parameters, Declarative Pipeline supports parameters out-of-the-box, allowing the Pipeline to accept user-specified parameters at runtime

pipeline {
    agent any
    parameters {
        string(name: 'Greeting', defaultValue: 'Hello', description: 'How should I greet the world?')
    }
    stages {
        stage('Example') {
            steps {
                echo "${params.Greeting} World!"
            }
        }
    }
}

- handling failure 

a number of different "post conditions" such as: always, unstable, success, failure, and changed

pipeline {
    agent none
    stages {
        stage('Build') {
            agent any
            steps {
                checkout scm
                sh 'make'
                stash includes: '**/target/*.jar', name: 'app' 
            }
        }
        stage('Test on Linux') {
            agent { 
                label 'linux'
            }
            steps {
                unstash 'app' 
                sh 'make check'
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
        stage('Test on Windows') {
            agent {
                label 'windows'
            }
            steps {
                unstash 'app'
                bat 'make check' 
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
    }
}

- using multiple agents 

The stash step allows capturing files matching an inclusion pattern (**/target/*.jar) 
    
pipeline {
    agent none
    stages {
        stage('Build') {
            agent any
            steps {
                checkout scm
                sh 'make'
                stash includes: '**/target/*.jar', name: 'app' 
            }
        }
        stage('Test on Linux') {
            agent { 
                label 'linux'
            }
            steps {
                unstash 'app' 
                sh 'make check'
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
        stage('Test on Windows') {
            agent {
                label 'windows'
            }
            steps {
                unstash 'app'
                bat 'make check' 
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
    }
}

Once pipelines are completed its execution, stashed files are deleted from the Jenkins controller.

unstash will retrieve the named "stash" from the Jenkins controller into the Pipeline’s current workspace.

bat allow execute windows batch command 
    
- optional step arguments 

uses the syntax [key1: value1, key2: value2]. Making statements like the following functionally equivalent:

```
git url: 'git://example.com/amazing-project.git', branch: 'master'
git([url: 'git://example.com/amazing-project.git', branch: 'master'])
```

- advanced scripted pipeline, parallel execution 

stage('Build') {
    /* .. snip .. */
}

stage('Test') {
    parallel linux: {
        node('linux') {
            checkout scm
            try {
                unstash 'app'
                sh 'make check'
            }
            finally {
                junit '**/target/*.xml'
            }
        }
    },
    windows: {
        node('windows') {
            /* .. snip .. */
        }
    }
}


## Extending with shared libraries 
- Pipeline has support for creating "Shared Libraries" which can be defined in external source control repositorie

- the SCM is using an SCM plugin which has been specifically updated to support a new API for checking out an arbitrary named version (Modern SCM option). 

- legacy SCM need to display version 

include ${library.yourLibName.version} somewhere in the configuration of the SCM
 
- directory structure 

The vars directory hosts script files that are exposed as a variable in Pipelines.
 

```
(root)
+- src                     # Groovy source files
|   +- org
|       +- foo
|           +- Bar.groovy  # for org.foo.Bar class
+- vars
|   +- foo.groovy          # for global 'foo' variable
|   +- foo.txt             # help for 'foo' variable
+- resources               # resource files (external libraries only)
|   +- org
|       +- foo
|           +- bar.json    # static helper data for org.foo.Bar
```

The basename of each .groovy file should be a Groovy (~ Java) identifier, conventionally camelCased. The matching .txt, if present, can contain documentation

Groovy source files in these directories get the same “CPS transformation” as in Scripted Pipeline.

- Global shared libraries 

Manage Jenkins » Configure System » Global Pipeline Libraries as many libraries as necessar

- Control permissions 

Overall/RunScripts permission to configure these libraries (normally this will be granted to Jenkins administrators)

- Folder-based libraries are not considered "trusted:" they run in the Groovy sandbox just like typical Pipelines.

- the GitHub Branch Source plugin provides a "GitHub Organization Folder" is automatic checkout with anonymous user and shared 

- using libraries 

    + mark Load implicitly allows Pipelines to immediately use classes or global variables defined by any such libraries.
    
    
    + other libaries to be referenced with @Library annotation, specifying the library’s name
    
```
@Library('my-shared-library') _
/* Using a version specifier, such as branch, tag, etc */
@Library('my-shared-library@1.0') _
/* Accessing multiple libraries with one statement */
@Library(['my-shared-library', 'otherlib@abc1234']) _
```
    + When referring to class libraries (with src/ directories), conventionally the annotation goes on an import statement
    
```
@Library('somelib')
import com.mycorp.pipeline.somelib.UsefulClass
```

    + It is not recommended to import a global variable/function, since this will force the compiler to interpret fields and methods as static

- loading libraries dynamically 

Pipeline: Shared Groovy Libraries plugin, there is a new option for loading (non-implicit) libraries in a script: a library step that loads a library dynamically, at any time

    + global variables/functions (from the vars/ directory), the syntax is quite simple:

```
library 'my-shared-library'
```

    + classes from the src/ directory 

    use library classes dynamically (without type checking), accessing them by fully-qualified name from the return value of the library step. static methods can be invoked using a Java-like syntax
    
    
```
library('my-shared-library').com.mycorp.pipeline.Utils.someStaticMethod()
```

- librariy versions 
    + not defined 

@Library('my-shared-library') _. If a "Default version" is not defined, the Pipeline must specify a version, for example @Library('my-shared-library@master') _.

    + using the library step you may also specify a version:

library 'my-shared-library@branch-name'

    + from parameter 
    

properties([parameters([string(name: 'LIB_VERSION', defaultValue: 'master')])])
library "my-shared-library@${params.LIB_VERSION}"

- retrival method an arbitrary named version (Modern SCM option). As of this writing, the latest versions of the Git and Subversion plugins

- dynamic retrival 

```
library identifier: 'custom-lib@master', retriever: modernSCM(
  [$class: 'GitSCMSource',
   remote: 'git@git.mycorp.com:my-jenkins-utils.git',
   credentialsId: 'my-private-key'])

```

- writing library, valid Groovy code is okay for use. Different data structures, utility methods


```
// src/org/foo/Point.groovy
package org.foo

// point in 3D space
class Point {
  float x,y,z
}
```

- accerssing steps, Library classes cannot directly call steps such as sh or git
    + passfrom parameters 

```
// src/org/foo/Zot.groovy
package org.foo

def checkOutFrom(repo) {
  git url: "git@github.com:jenkinsci/${repo}"
}

return this

```

pipeline reference the function 
```
//Which can then be called from a Scripted Pipeline:

def z = new org.foo.Zot()
z.checkOutFrom(repo)
```

    + a set of steps can be passed explicitly using this to a library class
 
```
package org.foo
class Utilities implements Serializable {
  def steps
  Utilities(steps) {this.steps = steps}
  def mvn(args) {
    steps.sh "${steps.tool 'Maven'}/bin/mvn -o ${args}"
  }
}
``` 

pipeline 
```
@Library('utils') import org.foo.Utilities
def utils = new Utilities(this)
node {
  utils.mvn 'clean package'
}
```

    + access global variables, numerous variables from the Scripted Pipeline into a library,
 
```
package org.foo
class Utilities {
  static def mvn(script, args) {
    script.sh "${script.tool 'Maven'}/bin/mvn -s ${script.env.HOME}/jenkins.xml -o ${args}"
  }
}
``` 

pipeline 
```
@Library('utils') import static org.foo.Utilities.*
node {
  mvn this, 'clean package'
}
```

- defining global variables from library 

```
vars/log.groovy
def info(message) {
    echo "INFO: ${message}"
}

def warning(message) {
    echo "WARNING: ${message}"
}

///use a field in your global for some state, annotate it as such:

@groovy.transform.Field
def yourField = [:]

def yourFunction....
```

pipeline 
```
//Jenkinsfile
@Library('utils') _

log.info 'Starting'
log.warning 'Nothing to do!'


//Declarative Pipeline does not allow method calls on objects outside "script" blocks. 
@Library('utils') _

pipeline {
    agent none
    stages {
        stage ('Example') {
            steps {
                // log.info 'Starting'   this will failed to call log.info 
                script { 
                    log.info 'Starting'
                    log.warning 'Nothing to do!'
                }
            }
        }
    }
}
```

- define custom steps 

    + variables defined in Shared Libraries must be named with all lowercase or "camelCased" in order to be loaded properly by Pipeline

    to define sayHello, the file vars/sayHello.groovy should be created and should implement a call method.
```
//the global variable to be invoked in a manner similar to a step

// vars/sayHello.groovy
def call(String name = 'human') {
    // Any valid steps can be called from this code, just like in other
    // Scripted Pipeline
    echo "Hello, ${name}."
}
```
    
    + reference and invoke this variable:
```
sayHello 'Joe'
sayHello() /* invoke with default arguments */
```

    + If called with a block, the call method will receive a Closure

```
// vars/windows.groovy
def call(Closure body) {
    node('windows') {
        body()
    }
}
```

Pipeline can then use this variable like any built-in step which accepts a block:
```
windows {
    bat "cmd /?"
}
```

- defining a more structured DSL, all Jenkins plugins are built and tested in the same way, so we might write a step named buildPlugin


```
// vars/buildPlugin.groovy
def call(Map config) {
    node {
        git url: "https://github.com/jenkinsci/${config.name}-plugin.git"
        sh 'mvn install'
        mail to: '...', subject: "${config.name} plugin build", body: '...'
    }
}
```

pipeline 
```
Jenkinsfile (Scripted Pipeline)
buildPlugin name: 'git'
```
- using third-party libraries 

    + use third-party Java libraries, typically found in Maven Central, from trusted library code using the @Grab annotation
    
```
@Grab('org.apache.commons:commons-math3:3.4.1')
import org.apache.commons.math3.primes.Primes
void parallelize(int count) {
  if (!Primes.isPrime(count)) {
    error "${count} was not prime"
  }
  // …
}
```    

Third-party libraries are cached by default in ~/.groovy/grapes/ on the Jenkins controller.

- External libraries may load adjunct files from a resources/ directory using the libraryResource step.

```
def request = libraryResource 'com/mycorp/pipeline/somelib/request.json'
```

The file is loaded as a string, suitable for passing to certain APIs or saving to a workspace using writeFile.

- pretesting library changes 

click the Replay link to try editing one or more of its source files, and see if the resulting build behaves as expected. 

Replay is not currently supported for trusted libraries. modify resource is also not supported in replay 

- defining declarative pipelines,  

execute a different Declarative Pipeline depending on whether the build number is odd or even

```
// vars/evenOrOdd.groovy
def call(int buildNumber) {
  if (buildNumber % 2 == 0) {
    pipeline {
      agent any
      stages {
        stage('Even Stage') {
          steps {
            echo "The build number is even"
          }
        }
      }
    }
  } else {
    pipeline {
      agent any
      stages {
        stage('Odd Stage') {
          steps {
            echo "The build number is odd"
          }
        }
      }
    }
  }
}

```

pipeline 
```
// Jenkinsfile
@Library('my-shared-library') _

evenOrOdd(currentBuild.getNumber())
```



## Pipeline development tools 
- Jenkins can validate, or "lint", a Declarative Pipeline from the command line before actually running it. 
- recommended ssh to run the linter  

```
Linting via the CLI with SSH
# ssh (Jenkins CLI)
# JENKINS_PORT=[sshd port on controller]
# JENKINS_HOST=[Jenkins controller hostname]
ssh -p $JENKINS_PORT $JENKINS_HOST declarative-linter < Jenkinsfile
```

- agent declaration 
```
the agent declaration.

Jenkinsfile
pipeline {
  agent
  stages {
    stage ('Initialize') {
      steps {
        echo 'Placeholder.'
      }
    }
  }
}
```

```
# pass a Jenkinsfile that does not contain an "agent" section
ssh -p 8675 localhost declarative-linter < ./Jenkinsfile
```

- VisualStudio Code Jenkins Pipeline Linter Connector

- The Pipeline Unit Testing Framework allows you to unit test Pipelines and Shared Libraries before running them in full.


## Unit test pipeline 
- reference project 
https://github.com/jenkinsci/JenkinsPipelineUnit

- using groovy script to run, mock Jenkinsfile 

```
class TestExampleJob extends BasePipelineTest {

    @Override
    @Before
    void setUp() throws Exception {
        baseScriptRoot = 'jenkinsJobs'
        scriptRoots += 'src/main/groovy'
        scriptExtension = 'pipeline'
        super.setUp()
    }

}

import org.junit.Rule
import org.junit.rules.ExpectedException
class TestCase extends BasePipelineTest {
    @Rule
    public ExpectedException thrown = ExpectedException.none();

    @Test
    void verify_exception() throws Exception {
        thrown.expect(Exception)
        thrown.expectMessage(containsString("error message"))
        runScript("Jenkinsfile")
    }
}

class TestCase extends BasePipelineTest {
  @Test
  void check_build_status() throws Exception {
      helper.registerAllowedMethod("sh", [String.class], {cmd->
          // cmd.contains is helpful to filter sh call which should fail the pipeline
          if (cmd.contains("make")) {
              binding.getVariable('currentBuild').result = 'FAILURE'
          }
      })
      runScript("Jenkinsfile")
      assertJobStatusFailure()
  }
}

@Test
void debianBuildSuccess() {
    helper.addShMock('uname', 'Debian', 0)
    helper.addShMock('./build.sh --release', '', 0)
    helper.addShMock('./test.sh', '', 0)
    // Have the sh mock execute the closure when the corresponding script is run:
    helper.addShMock('./processTestResults.sh --platform debian') { script ->
        // Do something "dynamically" first...
        return [stdout: "Executing ${script}: SUCCESS", exitValue: 0]
    }

    runScript("Jenkinsfile")

    assertJobStatusSuccess()
}

@Test
void debianBuildUnstable() {
    helper.addShMock('uname', 'Debian', 0)
    helper.addShMock('./build.sh --release', '', 0)
    helper.addShMock('./test.sh', '', 1)

    runScript("Jenkinsfile")

    assertJobStatusUnstable()
}
```

Jenkinsfile 
```
// Jenkinsfile
node {
    stage('Mock build') {
        def systemType = sh(returnStdout: true, script: 'uname')
        if (systemType == 'Debian') {
            sh './build.sh --release'
            int status = sh(returnStatus: true, script: './test.sh')
            if (status > 0) {
                currentBuild.result = 'UNSTABLE'
            } else {
                def result = sh(returnStdout: true, script: './processTestResults.sh --platform debian')
                if (!result.endsWith('SUCCESS')) {
                    currentBuild.result = 'FAILURE'
                    error 'Build failed!'
                }
            }
        }
    }
}
```





## Scaling pipelines 
- enable performance-optimized modes, users need to explicitly set a Speed/Durability Setting for Pipelines. 

- how do i set speed/durability settings?

Globally, you can choose a global default durability setting under "Manage Jenkins" > "Configure System", labelled "Pipeline Speed/Durability Settings".

per pipeline job, labelled "Custom Pipeline Speed/Durability Level" - this overrides the global setting.

per branch for multibranch project, configure a custom Branch Property Strategy (under the SCM) and add a property for Custom Pipeline Speed/Durability Level

- Durability Settings?

Performance-optimized mode ("PERFORMANCE_OPTIMIZED") - Greatly reduces disk I/O. 
 
Maximum durability ("MAX_SURVIVABILITY") - behaves just like Pipeline did before, slowest option. 

Less durable, a bit faster ("SURVIVABLE_NONATOMIC") - Writes data with every step but avoids atomic writes.

- other scaling suggestions 




## Pipeline CPS method mismatches 
- Jenkins Pipeline uses a library called Groovy CPS to run Pipeline scripts. 
- This uses a continuation-passing style (CPS) transform to turn your code into a version that can save its current state to disk (a file called program.dat  inside your build directory)

continue running even after jenkins restarted 

- not CPS-transformed:

    1. Compiled Java bytecode, including

    2. Constructor bodies in your Pipeline script

    3. Any method in your Pipeline script marked with the @NonCPS annotation

    4. A few Pipeline steps which take no block and act instantaneously, such as echo or properties

- common problems and solutions 
    + use of pipeline steps from @NonCPS is NOT required 
```   

//should removed NonCPS
@NonCPS
def compileOnPlatforms() {
  ['linux', 'windows'].each { arch ->
    node(arch) {
      sh 'make'
    }
  }
}
compileOnPlatforms()
```
    + Constructors 
    
```
class Test {
  def x
  public Test() {
    setX()
  }
  private void setX() {
    this.x = 1;
  }
}
def x = new Test().x
echo "${x}"
```

constructor should not call CPS transformed method setX 

    + calling non-CPS transformed methods with CPS-transformed arguments, closure passed to Iterable.toSorted is CPS-transformed, but Iterable.toSorted itself is not CPS-transformed internally this will not work as intended
    
```
def sortByLength(List<String> list) {
  list.toSorted { a, b -> Integer.valueOf(a.length()).compareTo(b.length()) }
}
def sorted = sortByLength(['333', '1', '4444', '22'])
echo(sorted.toString())
```    

    + overrides of non-CPS transformed methods 
    
```
class Test {
  @Override
  public String toString() {
    return "Test"
  }
}
def builder = new StringBuilder()
builder.append(new Test())
echo(builder.toString())
```
    
    + closures inside GString, NOT work 

    In Groovy, it is possible to use a closure in a GString so that the closure is evaluated every time the GString is used as a String.
    
```
def x = 1
def s = "x = ${-> x}"
x = 2
echo(s)
```



## Pipeline best practices 
- steps (such as sh) to accomplish multiple parts of the build.
- Running shell scripts in Jenkins Pipeline
Using a shell script within Jenkins Pipeline can help simplify builds by combining multiple steps into a single stage.

- avoid complex groovy code 

JsonSlurper, function can be used to read from a file on disk, parse the data from that file into a JSON object. Instead of using JsonSlurper, use a shell step and return the standard out.

HttpRequest: Frequently this command is used to grab data from an external source and store it in a variable. Use shell like curl or wget to perform http request 

- reudcing repetition of similar pipeline steps. echo or sh steps, combine them into a single step or script.

- If you must use Jenkins APIs in your build, the recommended approach is to create a minimal plugin in Java that implements a safe wrapper around the Jenkins API you want to access using Pipeline’s Step API.

if using whitelisted may become a security risk.

- clean up old jenkins build 

- using shared libraries
    + do not override built-in pipeline steps 
    
    + avoiding large global variable declaration files 
    
    + avoiding very large shared libraries 
    
- dealing with concurrency in pipelines 

Avoiding NotSerializableException

ensure persisted variables are serializable 

do not assign non-serializable objects to variables 

using @NonCPS to disable the CPS tranformation from a specific method whose body would not execute concurrently. 



# Managing Jenkins 
- Configuration as code 

Manage Jenkins > Configuration  View Configuration to view the YAML file.

https://plugins.jenkins.io/configuration-as-code/

- yaml reference card 

https://yaml.org/refcard.html

- get the maximum benefit of JCasC, the YAML files should be stored in SCM. This gives you a history that you can use to trace changes that are made and allow you to easily roll back to an earlier version

- Configuring a plugin with JCasc 

- Yaml file location, is located in $JENKINS_HOME/jenkins.yaml. The location and name of the file being used is displayed on the Configuration as Code UI page

Populate the CASC_JENKINS_CONFIG environment variable to point to a comma-separated list that defines where configuration files are located.

Use the casc.jenkins.config Java property to control the file name and location.

config files such as /var/jenkins_home/casc_configs.

A full path to a single file such as /var/jenkins_home/casc_configs/jenkins.yaml.

A URL pointing to a file served on the web such as https://acme.org/jenkins.yaml.

- the value of the CASC_JENKINS_CONFIG 

- managing tools 

Built-in tool providers
Ant
Ant build step
Git
JDK
Maven

- managing plugins 

Jenkins controller, such as: JENKINS_HOME/plugins/PLUGIN_NAME.jpi.disabled.

$  Jenkins CLI using the enable-plugin or disable-plugin commands.

- about jenkins 

Manage Jenkins > About Jenkins page will shows the current release of Jenkins on the system 

third party libaries 
static resource 
installed plugins 

- system information 

- jenkins cli 

The command line interface can be accessed over SSH or with the Jenkins CLI client, a .jar file distributed with Jenkins

    + using the CLI over ssh 
    
    SSH service is disabled by default. Administrators may choose to set a specific port or ask Jenkins to pick a random port in the Configure Global Security page
    
```
% curl -Lv https://JENKINS_URL/login 2>&1 | grep -i 'x-ssh-endpoint'
< X-SSH-Endpoint: localhost:53801
%
```   
    + authentication, used for authentication with the Jenkins controller must have the Overall/Read permission in order to access the CLI
    
    + common commands 
    
    get help 
    
    ```
    execute the CLI help command:

    % ssh -l kohsuke -p 53801 localhost help
    ```
    
    the port 53801 is a random port, it will different to each installation 
    
        * build 
        
        % ssh -l kohsuke -p 53801 localhost help build
        
        * console 
        
        % ssh -l kohsuke -p 53801 localhost help console
        

        * who-am-i command is helpful for listing the current user’s credentials and permissions available to the user

        % ssh -l kohsuke -p 53801 localhost 
        
    + using the CLI client 
    
    
    + throubleshooting logs 
    
    Go into Manage Jenkins > System Log > Add new log recorder.
    
    Type org.jenkinsci.main.modules.sshd.PublicKeyAuthenticatorImpl (or type PublicKeyAuth and then select the full name)

    Set the level to ALL.

    Repeat the previous three steps for hudson.model.User
 
- system information

system properties 

environment variables 

list of plugins 

memory usage

- jenkins features controlled with system properties

System properties are defined by passing -Dproperty=value to the java command line to start Jenkins. Need to pass before the -jar argument 

    + properties in jenkins core 
    
    debug.YUI 
    
    executable-war 
    
    historyWidget.descriptionLimit
    
    hudson.bundled.plugins 
    ...
    
    + the properties include control runtime behavior and security etc. 
    
- change system time zone 
    + linux 
  
java system property:

java -Dorg.apache.commons.jelly.tags.fmt.timeZone=TZ ...  

running 

$ systemctl edit jenkins 

adding the following:

[Service]
Environment="JAVA_OPTS=-Dorg.apache.commons.jelly.tags.fmt.timeZone=America/New_York"
     

    + On windows, edit %INSTALL_PATH%/jenkins/jenkins.xml. Put -Dargs before -jar:

<arguments>-Duser.timezone="Europe/Minsk" -jar "%BASE%\jenkins.war"</arguments>

    + from script console 
    
    + Post-initialization script to make it permanent.

System.setProperty('org.apache.commons.jelly.tags.fmt.timeZone', 'America/New_York')

- script console 

    + security 

Jenkins features a Groovy script console which allows one to run arbitrary Groovy scripts within the Jenkins controller runtime or in the runtime on agents.

Offers no administrative controls to stop a User (or Admin) once they are able to execute the Script Console from affecting all parts of the Jenkins infrastructure. 

Can configure any Jenkins setting. It can disable security, reconfigure security, even open a backdoor on the host operating system completely outside of the Jenkins process

    + mutiple contexts, can run either on the controller or any configured agent 
    
    + running on the controller 
    
    "Manage Jenkins" > "Script Console".  Or by visiting the sub-URL /script on your Jenkins instance.
     
    + running on agents  

    "Manage Jenkins" > "Manage Nodes".  Select any node to view the status page.  In the menu on the left, a menu item is available to open a "Script Console"
    
    + Run scripts from controller Script Console on agents
    
script executes code on agent from master script console 
```
import hudson.util.RemotingDiagnostics
import jenkins.model.Jenkins

String agentName = 'your agent name'
//groovy script you want executed on an agent
groovy_script = '''
println System.getenv("PATH")
println "uname -a".execute().text
'''.trim()

String result
Jenkins.instance.slaves.find { agent ->
    agent.name == agentName
}.with { agent ->
    result = RemotingDiagnostics.executeGroovy(groovy_script, agent.channel)
}
println result
```   

reading and writing files    
```
new File('/tmp/file.txt').withWriter('UTF-8') { writer ->
    try {
        writer << 'hello world\n'
    } finally {
        writer.close()
    }
}
```    

read file from controller 
```
new File('/tmp/file.txt').text
```

    + remote access, A Jenkins Admin can execute groovy scripts remotely by sending an HTTP POST request to /script/ url or /scriptText/.
    
```
curl -d "script=<your_script_here>" https://jenkins/script
# or to get output as a plain text result (no HTML)
curl -d "script=<your_script_here>" https://jenkins/scriptText
```    
    
    + groovy scripts example 
    
https://github.com/cloudbees/jenkins-scripts
https://github.com/samrocketman/jenkins-script-console-scripts
https://github.com/jenkinsci/jenkins-scripts
    

- groovy hook scripts 

    + hook HOOK, the following locations are searched:

WEB-INF/HOOK.groovy in jenkins.war

WEB-INF/HOOK.groovy.d/*.groovy in the lexical order in jenkins.war

$JENKINS_HOME/HOOK.groovy

$JENKINS_HOME/HOOK.groovy.d/*.groovy in the lexical order

HOOK.groovy.d is suitable to avoid conflicts — multiple entities can insert stuff into the hook without worrying about overwriting each other’s code.

    + events use this mechanism:

init: Post-initialization script

boot-failure: Boot failure hook

    + init hook 
    
    $JENKINS_HOME/init.groovy, or any .groovy file in the directory $JENKINS_HOME/init.groovy.d/, to run some additional things right after Jenkins starts up. 

- managing nodes 

Jenkins controller, The Jenkins controller is the Jenkins service itself and is where Jenkins is installed

Jenkins Nodes, Nodes are the "machines" on which build agents run. 

Agents, Agents manage the task execution on behalf of the Jenkins controller by using executors. An agent can use any operating system that supports Java. Tools required for builds and tests are installed on the node where the agent runs; they can be installed directly or in a container

Executors, it is a slot for execution of tasks; effectively, it is a thread in the agent. The number of executors on a node defines the number of concurrent tasks that can be executed on that node at one time. In other words, this determines the number of concurrent Pipeline stages that can execute on that node at one time.

    + create agent nodes 

    + Launch inbound agent via Windows Scheduler
    
If you are having trouble getting the inbound agent installed as a Windows service (i.e., you followed the instructions on installing the agent as a service here but it didn’t work), an alternative method of starting the service automatically when Windows starts is to use the Windows Scheduler. 

    + Installing a Jenkins agent on Windows
    
    use websocket to connect windows will avoid open port 

    windows service wrapper to run any executable as windows service

    https://github.com/winsw/winsw to download the WinSW-x64.exe  and rename to jenkins-agent.exe 
    
    jenkins-agent.exe 
    jenkins-agent.xml 
    ```
    <service>
        <id>jenkins-agent</id>
        <name>jenkins agent for local-win10</name>
        <description>this service run the agent process to jenkins-client</description>
        <arguments>-jar c:\jenkins\agent.jar -jnlpUrl http://localhost/computer/vm%2Dclient/jenkins-agent.jnlp -secret 6a915bdfdbfd1574d0122917dc7a4747ff641ef20eeaf359955382eef134b525 -workDir "C:\Jenkins"</arguments>
        <log mode="roll"/>
        <onfailure action="restart"/>
    </service>
    ```
    
    example service configuration file 
    
    goto the controller, goto agent page from the Jenkins controller. copy the commandline 
    
    download the agent.jar file. 
    
    open command as administrator 
    $ jenkins-agent.exe --help 
    
    $ jenkins-agent.exe install 
    will automatic load the jenkins-agent.xml to start the java commandline required from the controller 
    
    check the services installed on windows 
    
    we can also add service account to let the service run under the account 
    
    

- inprocess script approval 

plugins execute user-provided scripts in a Groovy Sandbox that limits the internal APIs that are accessible. This protection is provided by the Script Security plugin.

Scripts using the Groovy Sandbox are all subject to the same restrictions, therefore a Pipeline authored by an Administrator is subject to the restrictions as one authorized by a non-administrative user.

When the Groovy Sandbox is disabled, or a method outside of the built-in list is invoked, the Script Security plugin will check the Administrator-managed list of approved scripts

- manage users 

- themes for user interface 
dark 
light 
...

- user content 

 "User Content", where administrators can place files inside $JENKINS_HOME/userContent, and these files are served from http://yourhost/jenkins/userContent. This can be thought of as a mini HTTP server to serve images, stylesheets, and other static resources that you can use from various description fields inside Jenkins.

- spawning processes from build 

    + normal process 

Jenkins and the child process are connected by three pipes (stdin, stdout, and stderr.) This allows Jenkins to capture the output from the child process. The child process may write a lot of data to the pipe and quit immediately after that, so Jenkins waits for end-of-file (EOF)

the operating system closes all the file descriptors it owned. So, even if the process did not close stdout and stderr, Jenkins gets end of file (EOF).

    + problem, child process fork and a daemon process inherit the three pipes. 
    
        * unix 
        
        this to make the daemon behave. For example:

        daemonize -E BUILD_ID=dontKillMe /path/to/your/command
        In a Jenkins Pipeline, use JENKINS_NODE_COOKIE instead of BUILD_ID.
        
        or -Dhudson.util.ProcessTree.disable=true - see long running agent process for details.

        * On Windows, use the 'at' command to launch a process in the background. 

```      
<scriptdef name="get-next-minute" language="beanshell">
  <attribute name="property" />

  date = new java.text.SimpleDateFormat("HH:mm")
    .format(new Date(System.currentTimeMillis() + 60000));
  project.setProperty(attributes.get("property"), date);
</scriptdef>

<get-next-minute property="next-minute" />
<exec executable="at">
  <arg value="${next-minute}" />
  <arg value="/interactive" />
  <arg value="${jboss.home}\bin\run.bat" />
</exec>
```

use a wrapper script and launch your program through 

another workaround is schedule a permanent task and force running it from the Ant script. For example, run the command:







## Email extension 
- reference 
https://plugins.jenkins.io/email-ext/

- pipeline step 


emailext body: 'Test Message',
    subject: 'Test Subject',
    to: 'test@example.com'


     
     
# Pipeline example 
- reference 
https://jenkins.io/doc/pipeline/examples/
- ansi color build wrapper 
- rchive build output 
- artifactory generic upload download 
- artifactory maven build 
- configure provider plugin 
- external workspace manager 
- get build cause 
- gitcommit 
- gitcommit_changeset 
- ircnotify commandline 
- jobs in parallel 
- load from file 
- maven and jdk specific version 
- parallel from grep 
- parallel from list 
- parallel multiple nodes 
- push git repo 
- slacknotify 
- timestamper wrapper 
- trigger job on all nodes 
- unstash different dir




# Blue Ocean 
Blue Ocean is a new project that rethinks the user experience of Jenkins. Designed from the ground up for Jenkins Pipeline and compatible with Freestyle jobs

- sophisticated visualization of CD pipelines 
- pipeline editor 
- personalization UI 
- pinpoint precision 
- native integration for branch and pull requests 

- Jenkins is installed on most platforms, the Blue Ocean plugin and all its other dependent plugins (which form the Blue Ocean "suite of plugins") are not installed by default. blueocean is part of docker jenkins image https://hub.docker.com/

- Alternatively, you can access Blue Ocean directly by appending /blue to the end of your Jenkins server’s URL - e.g. https://jenkins-server-url/blue


# Using Credentials 
- credentials stored in jenkins 
Secret text - a token such as an API token (e.g. a GitHub personal access token),

Username and password - which could be handled as separate components or as a colon separated string in the format username:password (read more about this in Handling credentials),

Secret file - which is essentially secret content in a file,

SSH Username with private key - an SSH public/private key pair,

Certificate - a PKCS#12 certificate file and optional password, or

Docker Host Certificate Authentication credentials.

- adding new global credentials 

System, click the Global credentials (unrestricted) link to access this default domain.




# Setup jenkins proxy 
- Manage Jenkins > Manage Plugins > Advanced tab > Proxy 

- Make a copy of the jenkins.xml file as a backup. Edit jenkins.xml 

<executable>%BASE%\jre\bin\java</executable>

<arguments> -Dhttp.proxyHost=http://ProxyServerName -Dhttp.proxyPort=3128 -Xrs -Xmx256m -Djava.library.path="C:\natives;%Path% " -Dhudson.lifecycle=hudson.lifecycle.WindowsServiceLifecycle -jar "%BASE%\jenkins.war"</arguments>


# SCM setup 
- make sure setup proxy for the system environment to let git access the web 


# Jenkins: 403 No valid crumb was included in the request
- solution 1 this post helped me to do away with the crumb problem but still securing Jenkins from CSRF attack.

Solution for no-valid crumb included in the request issue

- solution 2 Manage Jenkins > "Configure Global Security" that "Enable proxy compatibility" This helped with my issue


# Jenkins add github SCM get error pack file didn't match any object 
- clean jenkins git cache folder and then rerun the job 

C:\ProgramData\Jenkins\.jenkins\caches




# Pipeline syntax reference 


# Pipeline steps reference 
- git relative plugins 

git automerge 

git bisect 

git changelog, https://www.jenkins.io/doc/pipeline/steps/git-changelog/ 

git forensics plugin 

git plugin 

git push plugin 

github coverage reporter 

github integration plugin 

github issues plugin 

github plugin 

github pull request builder 

github pull request coverage status 

github status wrapper plugin 

- Office 365 Connector
office365ConnectorSend: Send job status notifications to Office 365 (e.g. Microsoft Teams or Outlook)

- more... 


# LTS upgrade guides 


# Scuring jenkins 
- access control 
 Jenkins does not allow anonymous access, and a single admin user exists
 
- scuring jenkins 

basic 

ldap 

matrix based security,

    + apache frontend for security 
```
$ SUDO ./configure --enable-proxy \
--enable-ldap \
--enable-vhost \
--enable-ssl \
--enable-suexec \
--enable-rewrite \
--enable-proxy-ajp \
--enable-authnz-ldap \
--enable-mods-shared=all \
--with-ssl \
--with-ldap \
--with-ldap-include=/usr/include/ \
--prefix=/opt/apache/httpd-2.2.6
```

apache basic 

apache authentication against x509/SSL 

Running Jenkins with AJP

- managing security 

    + TCP port 
    random 
    fixed 
    
    + access control 
    security realm, informs the jenkins environment how and where to pull user information 
    
    authorization 
    
    + ldap 
    
    + unix user/group database 
    
- controller isolation 

Build script authors (pom.xml, Makefile, etc.)

To prevent builds from running on the built-in node directly, navigate to Manage Jenkins » Manage Nodes and Clouds. 

The Jenkins controller and agents can be thought of as a distributed process which executes across multiple discrete processes and machines. 

- csrf protection 
Jenkins protects from cross-site request forgery (CSRF) by default

Manage Jenkins » Configure Global Security » CSRF Protection, administrators can configure CSRF Protection.


    + The user name that the crumb was generated for

The web session ID that the crumb was generated in

The IP address of the user that the crumb was generated for

A salt unique to this Jenkins instance

- rendering user content 

serve files that can be viewed or downloaded from Jenkins. Some built-in examples of that are the workspace browser, archived artifacts, file parameters to builds, or the /userContent/ directory. Plugins like Javadoc, HTML Publisher, or Maven Integration 

resource root url, relaxing Content-Security-Policy, administrators can configure Jenkins to serve files from potentially less trusted sources from a different domain

Resource URLs do not require authentication

expiration, Resource URLs expire after 30 minutes by default. Expired resource URLs will redirect users to their equivalent Jenkins URLs

- access control for builds 

This is implemented by Authorize Project Plugin, which allows flexible configuration of global and per-project build authorization

- handling environment variables 

the behavior of build scripts: PATH (or Path on Windows) is generally used to find the programs that are launched during a build

Less well known variables, such as LD_PRELOAD (on Linux) and DYLD_LIBRARY_PATH (on macOS) may have a similar effect.

Windows Batch allows scripts to be written in a manner that prevents this problem (using EnableDelayedExpansion and the !variable! syntax)

Safe batch environment filter, generic build step environment filters, pipeline keep environment step 

```
node {
    withEnv(['foo=bar', 'baz=qux', 'PATH=/usr/bin:/bin:/foo']) {
        sh 'env'
        keepEnv(['foo', 'WORKSPACE']) {
            sh 'env'
        }
    }
}
```
- markup formatters, Manage Jenkins » Configure Global Security » Markup Formatter 

Allow users to use rich formatting for these descriptions

Protect other users from Cross-Site Scripting (XSS) attacks

Every user with an account and Overall/Read permission can edit their own user profile. This includes a description that is rendered using the configured markup formatter.

Job/Configure and Build/Update to fully trusted users: Anyone with an account will be able to edit their own description and any other user accessing their profile may become victim of an XSS attack.

- exposed services and ports 

TCP agent listener port 

    + random 
    + fixed 
    
ssh server can be configured to open an ssh port on jenkins controller 





# System administration 
- authenticating scripted clients 

make scripted clients (such as wget) invoke operations that require authorization (such as scheduling a build), use HTTP BASIC authentication to specify the user name and the API token.

```
$ curl -X POST -L --user your-user-name:apiToken \
    https://jenkins.example.com/job/your_job/build
    
    
$ wget --auth-no-challenge \
    --user=user --password=apiToken \
    http://jenkins.example.com/job/your_job/build
```

    + groovy script with cdancy/jenkins-reset, The cdancy/jenkins-rest client greatly simplifies REST API access.
    
```
@Grab(group='com.cdancy', module='jenkins-rest', version='0.0.18')

import com.cdancy.jenkins.rest.JenkinsClient

JenkinsClient client = JenkinsClient.builder()
    .endPoint("http://127.0.0.1:8080") // Optional. Defaults to http://127.0.0.1:8080
    .credentials("user:apiToken") // Optional.
    .build()

println(client.api().systemApi().systemInfo())
```

- reverse proxy configuration, A 'reverse proxy' allows an alternate HTTP or HTTPS provider to communicate with web browsers on behalf of Jenkins. 

- reverse proxy iss, where you have existing web sites on your server, you may find it useful to run Jenkins (or the servlet container that Jenkins runs in) behind IIS, so that you can bind Jenkins to the part of a bigger website

- ...

- managing systemd services

service configuration of the Jenkins service as configured by the package installers

$ systemctl cat jenkins

    + overriding service configurations, running systemd(1), the systemd(1) service unit is delivered to:

Debian
/lib/systemd/system/jenkins.service

Red Hat
/usr/lib/systemd/system/jenkins.service

openSUSE
/usr/lib/systemd/system/jenkins.service

Edit the drop-in unit with,  override.conf file is stored at /etc/systemd/system/jenkins.service.d/override.conf and can be used to customize the service. 

$ systemctl edit jenkins 

$ systemctl daemon-reload 
for the changes to take effect

```
[Unit]
Description=My Company Jenkins Controller

[Service]
# Add JVM configuration options
Environment="JAVA_OPTS=-Djava.awt.headless=true -XX:+UseStringDeduplication"

# Arbitrary additional arguments to pass to Jenkins.
# Full option list: java -jar jenkins.war --help
Environment="JENKINS_OPTS=--prefix=/jenkins --javaHome=/opt/jdk-11"

# Configuration as code directory
Environment="CASC_JENKINS_CONFIG=/var/lib/jenkins/configuration-as-code/"
```

    + systemd service has been defined, it can be started with:

$ systemctl start jenkins

    + read jenkins log 

$ journalctl -u jenkins

    + service status 
    
$ systemctl status jenkins

- reference systemd 

https://www.freedesktop.org/wiki/Software/systemd/

systemd is a suite of basic building blocks for a Linux system. It provides a system and service manager that runs as PID 1 and starts the rest of the system