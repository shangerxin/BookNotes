大数据与人工智能导论（第二版）
姚海鹏 王露瑶 刘韵洁 买天乐
164个想法

◆ 第1章 绪论

>> 大容量（Volume）：人类社会活动产生的数据量已经超越300 EB

>> 多种类（Variety）：多样化的数据可被归类为结构化数据、半结构化数据和非结构化数据

>> 速度快（Velocity）：面对如此大的数据体量

>> 真实性（Veracity）：可靠的数据来源能够保障数据的真实性

>> 非结构性（Nonstructural）：在获得数据之前无法提前预知其结构，目前绝大多数数据是非结构化数据

>> 时效性（Timeliness）：大数据的处理速度非常重要

>> 安全性（Security）：由于大数据高度依赖数据存储和共享，必须寻求更好的方法来消除各种隐患与漏洞

◆ 1.2 人工智能

>> 机电控制：机电受控系统的模型参数具有复杂不确定性

◆ 2.3 数据的获取

>> 数据采集和处理的基本步骤如图2-2所示：① 将需要抓取数据网站的URL（Uniform Resource Locator）信息写入URL队列；② 爬虫从URL队列中获取需要抓取数据网站的Site URL信息；③ 爬虫从Internet抓取对应网页内容，并抽取其特定属性的内容值；④ 爬虫将从网页中抽取的数据写入数据库；⑤ DP（Data Process）读取Spider Data，并进行处理；⑥ DP将处理后的数据写入数据库。

>> 大数据采集平台（1）Apache FlumeFlume是Apache旗下的一款开源、高可靠、高扩展、容易管理、支持客户扩展的数据采集系统。

>> （2）FluentdFluentd是另一个开源的数据收集框架

>> （3）LogstashLogstash是著名的开源数据栈ELK（ElasticSearch，Logstash，Kibana）中的“L”。Logstash用JRuby开

>> （4）Splunk ForwarderSplunk是一个分布式的机器数据平台，主要有三个角色：①Search Head负责数据的搜索和处理，提供搜索时的信息抽取；②Indexer负责数据的存储和索引；③Forwarder负责数据的收集、清洗、变形，并发送给Indexer。

◆ 2.5 数据的预处理技术

>> 数据预处理主要包括以下方法。（1）数据清理：通过填写缺失的值、光滑噪声数据、识别或删除离群点并解决不一致性来“清理”数据

>> （2）数据集成：将多个数据源中的数据结合起来并统一存储，建立数据仓库的过程实际上就是数据集成。（3）数据变换：通过平滑、聚集、规范化、最小最大规范化等方法，把原始数据转换成适合数据挖掘的形式。（4）数据归约：维归约（删除不相关的属性（维））、数据压缩（PCA、LDA、SVD、小波变换）、数值归约（回归和对数线性模型、线形回归、直方图）。

>> （1）处理缺失值的方法① 忽略元组：当挖掘任务涉及分类，并且缺少类标号时可用此方法

>> ② 人工填写缺失值：缺失量大时不可行。③ 使用一个全局常量填充缺失值：将缺失的属性值用同一个常量替换，简单但不可靠。

>> ④ 使用属性的均值填充缺失值：正常数据分布可以使用均值

>> ⑤ 使用与给定元组属同一类的所有样本的属性均值

>> ⑥ 使用最有可能的值填充缺失值：可以用回归，使用贝叶斯形式化的基于推理的工具或决策树归纳确定

>> 将数据转换或统一成适合挖掘的形式，涉及如下内容。（1）光滑：去掉数据的噪声，包括分箱、回归和聚类。（2）聚集：对数据进行汇总或聚集。这一步通常用来为多粒度数据分析构造数据立方体。（3）数据泛化：使用概念分层，用高层概念替换底层或“原始”数据。（4）规范化：又称为归一化，特征缩放（Feature Scaling）

>> 数据归约策略包括维归约、数值归约、数据压缩。（1）维归约：减少所考虑的随机变量或属性的个数

>> （2）数值归约：用替代的、较小的数据形式替换原数据。

>> （3）数据压缩：通过变换以得到原数据的归约或“压缩”表示

◆ 2.6 模型的构建与评估

>> 如格搜索（Grid Search）、随机搜索（Random Search）以及启发式搜索（Smart Search）等。这些搜索算法是从超参数空间中寻找一个最优的值，后面会进行详细介绍。

>> 分类（Classification）、回归（Regression）、排序（Ranking）、聚类（Clustering）、热门主题模型（Topic ModeLing）、推荐（Recommendation）等，

>> （2）回归评价指标与分类不同的是，回归是对连续的实数值进行预测，即输出值是连续的实数值，而分类中是离散值

>> RMSE回归模型中最常用的评价模型是平方根误差（RMSE，Root Mean Square Error）

>> Quantiles of Errors为了改进RMSE的缺点，提高评价指标的鲁棒性，使用误差的分位数来代替，如中位数来代替平均数

>> “Almost Correct”Predictions有时可以使用相对误差不超过设定的值来计算平均误差，如当[插图]超过100%

◆ 2.7 数据的可视化

>> DataWranglerDataWrangler是斯坦福大学可视化组（SUVG，Stanford University Visual Group）设计的基于Web的服务，用来清理重复数据。

>> Open Refine图2-3是Open Refine的示例，用户在计算机上运行这个应用程序后可以通过浏览器访问它，该工具主要用于帮助用户整理数据

>> D3.jsD3是指数据驱动文档（Data-Driven Documents）。D3.js是一个JavaScript库，它可以通过数据来操作文档。D3可以通过使用HTML、SVG和CSS把数据鲜活形象地展现出来

>> GephiGephi是一款开源免费跨平台基于JVM的复杂网络分析软件，其主要用于各种网络和复杂系统，动态和分层图的交互可视化与探测开源工具；可用作探索性数据分析、链接分析、社交网络分析、生物网络分析等。

>> EChartsECharts是一个纯Javascript的图表库，可以流畅地运行在PC和移动设备上，兼容当前绝大部分浏览器（如IE8/9/10/11、Chrome、Firefox、Safari等）

>> BonsaijsBonsaijs是一个轻量级的免费开源的javascript图形库，可以方便地创建图形和动画

◆ 第3章 大数据框架

>> MapReduce和HDFS3.1.1节介绍了Hadoop的由来，接下来深入介绍MapReduce和HDFS在实际生产过程中的意义。有了批量的数据

◆ 3.2 Hadoop大数据处理框架

>> NameNode保存元信息的种类有：① 文件名、目录名及它们之间的层级关系；② 文件目录的所有者及其权限；③ 每个文件块的名及文件由哪些块组成。

>> DataNodeDataNode是HDFS中的Worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时会根据NameNode的指示进行创建、删除和复制等操作

>> Secondary NameNode在早期的Hadoop版本（0.21.0版本之前）中，存在一个辅助NameNode，称为Secondary NameNode

>> YARN框架和运行机制3.2.2节详细介绍了MapReduce组件的运行机制，从MapReduce的设计原理可以看出，JobTracker上承载了作业和任务调度的工作

◆ 3.4 Spark简介

>> RDDRDD是一个容错、并行的数据结构，可以理解为让用户显式地将大的数据分布式存储到集群服务器的内存或硬盘中

>> 算子算子就是对数据进行操作的函数。Spark 算子大致可以分为以下4类。第一类是创建（Creation）算子，用于将内存中的集合或外部文件创建为RDD对象，便于后续操作的处理。

>> Spark SQLSpark SQL提供在大数据上的SQL查询功能，在整个生态系统的角色定位为分析工具

◆ 3.6 Flink简介

>> Flink概述Flink起源于Stratosphere项目。2014年4月，Stratosphere的代码被复制并捐赠给了Apache软件基金会。而当初Stratosphere系统的核心开发人员成为这个孵化项目的初始成员

>> Flink主页在其顶部展示了该项目的理念：“Apache Flink是为分布式、高性能、随时可用以及准确的流处理应用程序打造的开源流处理框架”。Apache Flink是一个分布式处理的引擎和框架

◆ 第4章 机器学习算法

>> 留出法需要合理选择训练集的大小。交叉验证法是一种比较常用的方法，交叉验证法的步骤如下（以10份交叉验证为例）：首先将数据随机均分为10份，然后每次取出9份来训练，再将剩余的1份用来作为测试，并重复10次，保证每份数据均作为测试集经过测试，最后对模型的测试集（共10个）性能取平均值

>> 对包含N个样本的数据集D进行有放回的采样，每次从中取一个样本，每次取完样本后放回样本，这意味着这个样本在下一次采样的时候还有可能被取到，共进行N次采样（注意，这里的N与数据集的大小一样），一个样本在这N次采样后还未被取到的概率是[插图]，在高等数学中有一个关于自然常数e的极限公式[插图]，把公式中的x用-N来代替，则有[插图]N的取值与概率的关系如表4-1所示。

>> 常见的机器学习算法有线性回归、决策树、神经网络、支持向量机、贝叶斯等

>> 高维空间的几何概念通常带有前缀“超（Hyper-）”

>> 决策树的几何解释可以将图4-3的二维坐标升维变成三维坐标，如果找到合适的映射关系，几何模型就可以变成图4-5，在新的三维空间中很容易找到一个决策面将这两个不同的类划分开。这种进行升维、在高维空间中对原问题进行线性分类的方法叫作核方法

>> 机器学习中最关键的问题是如何对x和Y之间的依赖关系进行建模。从统计学上看，假设样本空间中的所有样本具有某种概率分布，得到的数据集可以看作对样本空间中进行采样后得到的观测值

>> 已知的x对Y进行概率估计的条件概率P（Y|x）被称为后验概率

>> 把条件概率P（x|Y）称为似然函数（或者叫作类条件概率，Class-Conditional Probability），之所以叫作似然函数而不是似然概率，是因为这一概率可看作Y到一个概率值的映射，依据贝叶斯公式[插图]，可以在似然函数和后验概率之间建立联系

>> 从邮件中提取出一系列的特征词记为x1,x2,…,xn，记Y1为垃圾邮件，Y2为正常邮件，计算概率[插图]通过比较式（4-1）和式（4-2）的大小，可以对邮件进行判决。至此，概率模型介绍完毕

◆ 4.2 决策树理论

>> 信息熵为（信息熵的概念来源于信息论）[插图]

>> 现有特征C（特征即属性），其有v个可能的取值为{c1,c2,…,cv}，根据特征C可将样本集D划分为v个子样本集，分别记为{D1，D2,…,Dv}，定义信息增益为[插图]

>> 特征进行划分的情况下的信息熵为[插图]可求得对于属性CPU的信息增益为[插图]

>> 决策树剪枝的基本策略有“预剪枝”和“后剪枝”。避免过拟合需要用到测试集，预剪枝是在决策树的生成过程中进行剪枝处理

>> 后剪枝是在一棵树生成以后再从叶子节点开始逐步考察每个属性节点，判断是否存在过拟合的方法同预剪枝一样

◆ 4.3 朴素贝叶斯理论

>> 验概率分布和条件概率分布，先验概率分布为[插图]条件概率分布（又叫似然函数）为[插图]条件概率分布P（X=x|Y=ci）是十分复杂的，特别是在特征较多的情况下。

>> 特征相互独立的假设很难成立，有时会牺牲一定的分类准确率。描述先验概率、后验概率和似然函数关系的贝叶斯公式
