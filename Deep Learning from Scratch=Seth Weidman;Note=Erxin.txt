Deep Learning from Scratch=Seth Weidman;Note=Erxin

# Preface 
- what is neural network 

A neural network is a mathematical function that takes in inputs and produces outputs.

A neural network is a computational graph through which multidimensional arrays flow.

A neural network is made up of layers, each of which can be thought of as having a number of “neurons.”

A neural network is a universal function approximator that can in theory represent the solution to any supervised learning problem.


# Foundations
- numpy 

a = np.array([[1,2,3],
              [4,5,6]])
              
def operation(x1: ndarray, x2: ndarray) -> ndarray:
    pass 
    
- transposing 

np.transpose(ndarray, (1, 0))

- derivatives 

math, This limit can be approximated numerically by setting a very small value for Δ, such as 0.001, so we can compute the derivative

dfdu(a)=limΔ→0f(a+Δ)−f(a−Δ)2×Δ

dfdu(a)=f(a+0.001)−f(a−0.001)0.002

- diagrams 

using callable to restrict the type of function 
```
from typing import Callable

def deriv(func: Callable[[ndarray], ndarray],
          input_: ndarray,
          delta: float = 0.001) -> ndarray:
    '''
    Evaluates the derivative of a function "func" at every element in the
    "input_" array.
    '''
    return (func(input_ + delta) - func(input_ - delta)) / (2 * delta)
```

slope = (f(a+0.001) - f(a-0.001))/0.002

- nested function 

def chain_length_2(chain: Chain,
                   a: ndarray) -> ndarray:
    '''
    Evaluates two functions in a row, in a "Chain".
    '''
    assert len(chain) == 2, \
    "Length of input 'chain' should be 2"

    f1 = chain[0]
    f2 = chain[1]

    return f2(f1(x))


- chain rule, lets us compute derivatives of composite functions.

    + math 
df2(x)/du = df2(f1(x))/du * df1(x)/du

- code 

def sigmoid(x: ndarray) -> ndarray:
    '''
    Apply the sigmoid function to each element in the input ndarray.
    '''
    return 1 / (1 + np.exp(-x))


def chain_deriv_2(chain: Chain,
                  input_range: ndarray) -> ndarray:
    '''
    Uses the chain rule to compute the derivative of two nested functions:
    (f2(f1(x))' = f2'(f1(x)) * f1'(x)
    '''

    assert len(chain) == 2, \
    "This function requires 'Chain' objects of length 2"

    assert input_range.ndim == 1, \
    "Function requires a 1 dimensional ndarray as input_range"

    f1 = chain[0]
    f2 = chain[1]

    # df1/dx
    f1_of_x = f1(input_range)

    # df1/du
    df1dx = deriv(f1, input_range)

    # df2/du(f1(x))
    df2du = deriv(f2, f1(input_range))

    # Multiplying these quantities together at each point
    return df1dx * df2du

- a slightly longer example, f1f2f3

```
def chain_deriv_3(chain: Chain,
                  input_range: ndarray) -> ndarray:
    '''
    Uses the chain rule to compute the derivative of three nested functions:
    (f3(f2(f1)))' = f3'(f2(f1(x))) * f2'(f1(x)) * f1'(x)
    '''

    assert len(chain) == 3, \
    "This function requires 'Chain' objects to have length 3"

    f1 = chain[0]
    f2 = chain[1]
    f3 = chain[2]

    # f1(x)
    f1_of_x = f1(input_range)

    # f2(f1(x))
    f2_of_x = f2(f1_of_x)

    # df3du
    df3du = deriv(f3, f2_of_x)

    # df2du
    df2du = deriv(f2, f1_of_x)

    # df1dx
    df1dx = deriv(f1, input_range)

    # Multiplying these quantities together at each point
    return df1dx * df2du * df3du
```

PLOT_RANGE = np.range(-3, 3, 0.01)
plot_chain([leaky_relu, sigmoid, square], PLOT_RANGE)
plot_chain_deriv([leaky_relu, sigmoid, square], PLOT_RANGE)

- math 

    + matrix multiple 
    
```
def matmul_forward(X: ndarray,
                   W: ndarray) -> ndarray:
    '''
    Computes the forward pass of a matrix multiplication.
    '''

    assert X.shape[1] == W.shape[0], \
    '''
    For matrix multiplication, the number of columns in the first array should
    match the number of rows in the second; instead the number of columns in the
    first array is {0} and the number of rows in the second array is {1}.
    '''.format(X.shape[1], W.shape[0])

    # matrix multiplication
    N = np.dot(X, W)

    return N
```

    + matrix forward extra 
    
s=f(X,W)=σ(ν(X,W))=σ(x1×w1+x2×w2+x3×w3)
```
def matrix_forward_extra(X: ndarray,
                         W: ndarray,
                         sigma: Array_Function) -> ndarray:
    '''
    Computes the forward pass of a function involving matrix multiplication,
    one extra function.
    '''
    assert X.shape[1] == W.shape[0]

    # matrix multiplication
    N = np.dot(X, W)

    # feeding the output of the matrix multiplication through sigma
    S = sigma(N)

    return S
```

    + matrix function backward 
    
∂f∂X=∂σ∂u(ν(X,W))×∂ν∂X(X,W)=∂σ∂u(x1×w1+x2×w2+x3×w3)×WT

```
def matrix_function_backward_1(X: ndarray,
                               W: ndarray,
                               sigma: Array_Function) -> ndarray:
    '''
    Computes the derivative of our matrix function with respect to
    the first element.
    '''
    assert X.shape[1] == W.shape[0]

    # matrix multiplication
    N = np.dot(X, W)

    # feeding the output of the matrix multiplication through sigma
    S = sigma(N)

    # backward calculation
    dSdN = deriv(sigma, N)

    # dNdX
    dNdX = np.transpose(W, (1, 0))

    # multiply them together; since dNdX is 1x1 here, order doesn't matter
    return np.dot(dSdN, dNdX)
```

- computational graph with two 2d matrix inputs 

L=Λ(σ(X×W)) =σ(XW11)+σ(XW12)+σ(XW21)+σ(XW22)+σ(XW31)+σ(XW32)


```
def matrix_function_forward_sum(X: ndarray,
                                W: ndarray,
                                sigma: Array_Function) -> float:
    '''
    Computing the result of the forward pass of this function with
    input ndarrays X and W and function sigma.
    '''
    assert X.shape[1] == W.shape[0]

    # matrix multiplication
    N = np.dot(X, W)

    # feeding the output of the matrix multiplication through sigma
    S = sigma(N)

    # sum all the elements
    L = np.sum(S)

    return L
```
- compute this directly 

∂Λ∂u(N)=∂Λ∂u(S)×∂σ∂u(N)

```
def matrix_function_backward_sum_1(X: ndarray,
                                   W: ndarray,
                                   sigma: Array_Function) -> ndarray:
    '''
    Compute derivative of matrix function with a sum with respect to the
    first matrix input.
    '''
    assert X.shape[1] == W.shape[0]

    # matrix multiplication
    N = np.dot(X, W)

    # feeding the output of the matrix multiplication through sigma
    S = sigma(N)

    # sum all the elements
    L = np.sum(S)

    # note: I'll refer to the derivatives by their quantities here,
    # unlike the math, where we referred to their function names

    # dLdS - just 1s
    dLdS = np.ones_like(S)

    # dSdN
    dSdN = deriv(sigma, N)

    # dLdN
    dLdN = dLdS * dSdN

    # dNdX
    dNdX = np.transpose(W, (1, 0))

    # dLdX
    dLdX = np.dot(dSdN, dNdX)

    return dLdX
```

-  gradient of L with respect to W would be XT. However, because of the order in which the XT expression factors out of the derivative for L,

∂Λ/∂u(W)=XT×(∂Λ/∂u)(S)×(∂σ/∂u)(N)


# Fundamentals 
- linear regression 

yi=β0+β1×x1+...+βn×xk+ϵ

pi=xi×W=w1×xi1+w2×xi2+...+wk×xik

pbatch=Xbatch×W

MSE(pbatch,ybatch) = ((y1−p1)^2+(y2−p2)^2+(y3−p3)^2)3

L=Λ((ν(X,W),Y)

pbatch_with_bias=x_i dot W + b

```
def forward_linear_regression(X_batch: ndarray,
                              y_batch: ndarray,
                              weights: Dict[str, ndarray])
                              -> Tuple[float, Dict[str, ndarray]]:
    '''
    Forward pass for the step-by-step linear regression.
    '''
    # assert batch sizes of X and y are equal
    assert X_batch.shape[0] == y_batch.shape[0]

    # assert that matrix multiplication can work
    assert X_batch.shape[1] == weights['W'].shape[0]

    # assert that B is simply a 1x1 ndarray
    assert weights['B'].shape[0] == weights['B'].shape[1] == 1

    # compute the operations on the forward pass
    N = np.dot(X_batch, weights['W'])

    P = N + weights['B']

    loss = np.mean(np.power(y_batch - P, 2))

    # save the information computed on the forward pass
    forward_info: Dict[str, ndarray] = {}
    forward_info['X'] = X_batch
    forward_info['N'] = N
    forward_info['P'] = P
    forward_info['y'] = y_batch

    return loss, forward_info
```

- calculating the gradients

(∂Λ/∂P)(P,Y)×(∂α/∂N)(N,B)×(∂ν/∂W)(X,W)

Λ(P,Y)=(Y−P)^2 

(∂Λ/∂P)(P,Y)=−1×(2×(Y−P))

```
dLdP = -2 * (Y - P)
```

```
def loss_gradients(forward_info: Dict[str, ndarray],
                   weights: Dict[str, ndarray]) -> Dict[str, ndarray]:
    '''
    Compute dLdW and dLdB for the step-by-step linear regression model.
    '''
    batch_size = forward_info['X'].shape[0]

    dLdP = -2 * (forward_info['y'] - forward_info['P'])

    dPdN = np.ones_like(forward_info['N'])

    dPdB = np.ones_like(weights['B'])

    dLdN = dLdP * dPdN

    dNdW = np.transpose(forward_info['X'], (1, 0))

    # need to use matrix multiplication here,
    # with dNdW on the left (see note at the end of last chapter)
    dLdW = np.dot(dNdW, dLdN)

    # need to sum along dimension representing the batch size
    # (see note near the end of this chapter)
    dLdB = (dLdP * dPdB).sum(axis=0)

    loss_gradients: Dict[str, ndarray] = {}
    loss_gradients['W'] = dLdW
    loss_gradients['B'] = dLdB

    return loss_gradients
```

for key in weights.keys():
    weights[key] -= learning_rate * loss_grads[key]

- use these gradients to train the model 
    + select a batch of data 
    + run the forward pass of the model 
    + run the backward pass of the model using the info computed on the forward pass 
    + use the gradients computed on the backward pass to update the weights 

```
forward_info, loss = forward_loss(X_batch, y_batch, weights)

loss_grads = loss_gradients(forward_info, weights)

for key in weights.keys():  # 'weights' and 'loss_grads' have the same keys
    weights[key] -= learning_rate * loss_grads[key]
    
train_info = train(X_train, y_train,
                   learning_rate = 0.001,
                   batch_size=23,
                   return_weights=True,
                   seed=80718)
```

- assessing our model 

```
def predict(X: ndarray,
            weights: Dict[str, ndarray]):
    '''
    Generate predictions from the step-by-step linear regression model.
    '''
    N = np.dot(X, weights['W'])

    return N + weights['B']
    
preds = predict(X_test, weights)  # weights = train_info[0]
```


```
def mae(preds: ndarray, actuals: ndarray):
    '''
    Compute mean absolute error.
    '''
    return np.mean(np.abs(preds - actuals))
    
def rmse(preds: ndarray, actuals: ndarray):
    '''
    Compute root mean squared error.
    '''
    return np.sqrt(np.mean(np.power(preds - actuals, 2)))
```

- sigmoid function 

    + First, we want the function we use here to be monotonic so that it “preserves” information about the numbers that were fed in. Let’s say that, given the date that was fed in, two of our linear regressions produced values of –3 and 3, respectively
    
    + nonlinear; this nonlinearity will enable our neural network to model the inherently nonlinear relationship between the features and the target
    
    + sigmoid function has the nice property that its derivative can be expressed in terms of the function itself:

(∂σ/∂u(x))=σ(x)×(1−σ(x))



# Deep learning from scratch 
- recently, in the popular Keras library, they are also often called Dense layers, a more concise term that gets across the same idea.

- training a model one batch at a time

Feed input through the model function (the “forward pass”) to get a prediction.

Calculate the number representing the loss.

Calculate the gradient of the loss with respect to the parameters, using the chain rule

Update the parameters using these gradients

- Optimizer class will give us the flexibility to swap in one update rule for another

- Trainer 



# Extensions
- MSE
- The Softmax Cross Entropy Loss Function

The cross entropy loss function, for each index i in these vectors

CE(p_i, y_i) = -y_i * log(p_i) - ( 1 - y_i) * log(1 - p_i)

- Intuition for Momentum

the momentum parameter, which will determine the degree of this decay; the higher it is, the more the weight update at each time step will be based on the parameter’s accumulated momentum

- without dropout, the technique we will now learn, deep learning models are very challenging to train effectively without overfitting

- Dropout simply involves randomly choosing some proportion p of the neurons in a layer and setting them equal to 0 during each forward pass of training



# Convolution neural networks 
- advanced CNN variants, such as ResNets, DenseNets, and Octave Convolutions on your own.

- The input will have shape:

    Batch size

    Input channels

    Image height

    Image width

- The output will have shape:

    Batch size

    Output channels

    Image height

    Image width

-  The convolutional filters themselves will have shape:

    Input channels

    Output channels

    Filter height

    Filter width

- unit of padding around the edges to keep the output the same size as the input.

- convolution functions to work with batches of inputs—2D inputs whose first dimension represents the batch size of the input and whose second dimension represents the length of the 1D sequence



# Recurrent neural networks 
- RNN will thus be a three-dimensional ndarray of shape [batch_size, sequence_length, num_features]—a batch of sequences

- RNNLayer class that passes a sequence of data forward one sequence element at a time

- first RNNNode in each RNNLayer, with the network ultimately outputting a prediction at that time step of dimension output_size. 

- a sigmoid operation, and then multiplied by the prior hidden state. This allows the network to “learn to forget” what was in the hidden state, given the particular input that was passed in.



# PyTorch 
- example 
```
class HousePricesModel(PyTorchModel):

    def __init__(self,
                 hidden_size: int = 13):
        super().__init__()
        self.fc1 = nn.Linear(13, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)

    def forward(self, x: Tensor) -> Tensor:

        assert_dim(x, 2)

        assert x.shape[1] == 13

        x = self.fc1(x)
        x = torch.sigmoid(x)
        return self.fc2(x)
        
model = MNIST_ConvNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

trainer = PyTorchTrainer(model, optimizer, criterion)

trainer.fit(X_train, y_train,
            X_test, y_test,
            epochs=5,
            eval_every=1)
```

- optimization 
```
import torch.optim as optim

optimizer = optim.SGD(pytorch_boston_model.parameters(), lr=0.001)
```

- converlution network 

```
class ConvLayer(PyTorchLayer):
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 filter_size: int,
                 activation: nn.Module = None,
                 flatten: bool = False,
                 dropout: float = 1.0) -> None:
        super().__init__()

        # the main operation of the layer
        self.conv = nn.Conv2d(in_channels, out_channels, filter_size,
                              padding=filter_size // 2)

        # the same "activation" and "flatten" operations from before
        self.activation = activation
        self.flatten = flatten
        if dropout < 1.0:
            self.dropout = nn.Dropout(1 - dropout)

    def forward(self, x: Tensor) -> Tensor:

        # always apply the convolution operation
        x = self.conv(x)

        # optionally apply the convolution operation
        if self.activation:
            x = self.activation(x)
        if self.flatten:
            x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])
        if hasattr(self, "dropout"):
            x = self.dropout(x)

        return x
```

- bayesian inference 

```
def B(alpha: float, beta: float) -> float:
    """A normalizing constant so that the total probability is 1"""
    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta)

def beta_pdf(x: float, alpha: float, beta: float) -> float:
    if x <= 0 or x >= 1:          # no weight outside of [0, 1]
        return 0
    return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta)
```


# Gradient descent 
- estimating the gradient 

from typing import Callable

def difference_quotient(f: Callable[[float], float],
                        x: float,
                        h: float) -> float:
    return (f(x + h) - f(x)) / h

- choosing the right step size 

    Using a fixed step size

    Gradually shrinking the step size over time

    At each step, choosing the step size that minimizes the value of the objective function


# Getting data 
- stdin, stdout 
- reading files 
- delimited files 

Python’s csv module (or the pandas library, or some other library that’s designed to read comma-separated or tab-delimited files

- scraping the web 

beautiful soup library 

- keeping tabs on congress 

- using APIs 

JSON and xml 

- using an unauthenticated API 

import requests, json

github_user = "joelgrus"
endpoint = f"https://api.github.com/users/{github_user}/repos"

repos = json.loads(requests.get(endpoint).text)



# Working with Data 
- exploring your data 

from typing import List, Dict
from collections import Counter
import math

import matplotlib.pyplot as plt

def bucketize(point: float, bucket_size: float) -> float:
    """Floor the point to the next lower multiple of bucket_size"""
    return bucket_size * math.floor(point / bucket_size)

def make_histogram(points: List[float], bucket_size: float) -> Dict[float, int]:
    """Buckets the points and counts how many in each bucket"""
    return Counter(bucketize(point, bucket_size) for point in points)

def plot_histogram(points: List[float], bucket_size: float, title: str = ""):
    histogram = make_histogram(points, bucket_size)
    plt.bar(histogram.keys(), histogram.values(), width=bucket_size)
    plt.title(title)

- two dimensions 

plt.scatter(xs, ys1, marker='.', color='black', label='ys1')
plt.scatter(xs, ys2, marker='.', color='gray',  label='ys2')
plt.xlabel('xs')
plt.ylabel('ys')
plt.legend(loc=9)
plt.title("Very Different Joint Distributions")
plt.show()

- using NamedTuples 

import datetime

stock_price = {'closing_price': 102.06,
               'date': datetime.date(2014, 8, 29),
               'symbol': 'AAPL'}

- dataclasses 

from dataclasses import dataclass

@dataclass
class StockPrice2:
    symbol: str
    date: datetime.date
    closing_price: float

    def is_high_tech(self) -> bool:
        """It's a class, so we can add methods too"""
        return self.symbol in ['MSFT', 'GOOG', 'FB', 'AMZN', 'AAPL']

price2 = StockPrice2('MSFT', datetime.date(2018, 12, 14), 106.03)

assert price2.symbol == 'MSFT'
assert price2.closing_price == 106.03
assert price2.is_high_tech()

- cleaning and munging 

- manipulating data 

- recaling 

- an aside tqdm 

One way of doing this is with the tqdm library, which generates custom progress bars

- dimensionality reduction 

the “actual” (or useful) dimensions of the data might not correspond to the dimensions we have



# Machine learning 
- modeling 

- A common danger in machine learning is overfitting—producing a model that performs well on the data you train it on but generalizes poorly to any new data. 

The horizontal line shows the best fit degree 0 (i.e., constant) polynomial. It severely underfits the training data. The best fit degree 9 (i.e., 10-parameter) polynomial goes through every training data point exactly, but it very severely overfits; if we were to pick a few more data points

- f1 score 

def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:
    p = precision(tp, fp, fn, tn)
    r = recall(tp, fp, fn, tn)

    return 2 * p * r / (p + r)

- the bias variance tradeoff 



# K nearest neighbours 
- example 

from typing import List
from collections import Counter

def raw_majority_vote(labels: List[str]) -> str:
    votes = Counter(labels)
    winner, _ = votes.most_common(1)[0]
    return winner

assert raw_majority_vote(['a', 'b', 'c', 'b']) == 'b'

- The Iris dataset is a staple of machine learning. It contains a bunch of measurements for 150 flowers

You can download it from https://archive.ics.uci.edu/ml/datasets/iris
```
import requests

data = requests.get(
  "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
)

with open('iris.dat', 'w') as f:
    f.write(data.text)
```

- The Curse of Dimensionality

In low-dimensional datasets, the closest points tend to be much closer than average. But two points are close only if they’re close in every dimension



# Naive bayes 
- Bayes’s theorem tells us that the probability that the message is spam conditional on containing the word bitcoin is:




























































