Docker on Windows=Elton;Note=Erxin

# What's this book covered 
- Environment 
Docker for windows 17.06+ 
Windows 10 or window server 2016+

- Container doesn't add any overhead to the memory and compute requirements of app. You can run a brand new .NET Core app in one container and a 10-year old ASP.NET 2.0 WebForms app in another container on the same server.

- windows licensing, VMs running Windows. Windows is licensed at the host level, not the container level. If you have 100 Windows containers running on one server, you only need a license for the server. 

- On Windows Server, you can also run containers in Hyper-V mode to get increased isolation.

- the docker service and docker command line 

- Docker Universal Control Plane (UCP) 

- configure the Docker API to be remotely accessible and configure your Docker CLI to connect to a remote service. 

- You describe all these steps in a simple script called a Dockerfile

- This cluster is called a swarm, and you don't need to install anything extra to run 

- In production, you should ideally use Windows Server 2016 Core, the installation with no UI. Docker EE running as a Windows service.


Docker playground at https://dockr.ly/play-with-docker. 

$ docker container run dockeronwindows/ch01-whale

- docker as a service 
ps> Install-Module -Name DockerMsftProvider -Repository PSGallery -Force
ps> Install-Package -Name docker -ProviderName DockerMsftProvider
ps> Restart-Computer -Force

- The image has a script that secures the Docker service with TLS certificates. You can read more details on Stefan's blog at https://stefanscherer.github.io.


Docker service so that communication with the client is secured using TLS.

$ipAddress = '<vm-ip-address>'mkdir -p C:\certs\clientdocker container run --rm ` --env SERVER_NAME=$(hostname) ` --env IP_ADDRESSES=127.0.0.1,$vm-ip-address ` --volume 'C:\ProgramData\docker:C:\ProgramData\docker' ` --volume 'C:\certs\client:C:\Users\ContainerAdministrator\.docker' ` stefanscherer/dockertls-windowsRestart-Service docker

On the client machine, you can set environment variables to point the Docker client to use a remote Docker service. 
$ipAddress = '<vm-ip-address>'
$env:DOCKER_HOST='tcp://$($ipAddress):2376'
$env:DOCKER_TLS_VERIFY='1'
$env:DOCKER_CERT_PATH='C:\certs\client'

- docker in an azure vm 
- GitHub repository at https://github.com/sixeyed/docker-on-windows. 

 version 17.06 of Docker

- doing one thing with a task container. The image is based on Microsoft Nano Server and is set up to run a simple PowerShell script 

ps> docker container ls --all 

with Docker installed can run that script without having to install the engine.

- connecting t an interactive container 
> docker container run --interactive --tty dockeronwindows/ch02-powershell-env `    powershell

The --interactive option runs an interactive container, and the --tty flag attaches a dummy terminal connection to the container.

- keeping a process running in a background container 

It's a container that behaves like a Windows Service. In Docker terminology, it's called a detached container, and it's the Docker service that keeps it running in the background.

- build a docker image 
    + dockerfile, source code for an image 
FROM microsoft/nanoserver
COPY scripts/print-env-details.ps1 c:\\print-env.ps1
CMD ["powershell.exe", "c:\\print-env.ps1"]

The default registry is a free public service called Docker Hub. Microsoft has made the Nano Server image available on Docker Hub

    + build docker image from a dockerfile 
    ps > docker image build --tag dockeronwindows/ch02-powershell-env .


    Each instruction in the Dockerfile is executed as a separate step that produces a new image layer, and the final image will be the combined stack of all the layers.

- packaging your own applications 
    + docker file 
FROM microsoft/dotnet:1.1-sdk-nanoserver

WORKDIR /src
COPY src/ .

RUN dotnet restore; dotnet build
CMD ["dotnet", "run"]

    + The Dockerfile uses Microsoft's .NET Core image from Docker Hub as the base image. 
    + WORKDIR specifies the current working directory. 
    + RUN executes a command inside an intermediate container and saves the state of the container after the command completes, creating a new image layer
    
    + A simple build script that compiles the application and builds the Docker image looks like this:
ps> dotnet restore src; dotnet publish src
ps> docker image build --file Dockerfile.slim --tag dockeronwindows/ch02-dotnet-helloworld:slim .

- compiling with multi-stage builds, multiple FROM instructions in your Dockerfile, where each FROM instruction starts a new stage in the build.  final stage is used for the completed image.

//# build stage
FROM microsoft/dotnet:1.1-sdk-nanoserver AS builder
WORKDIR /src
COPY src/ .
RUN dotnet restore; dotnet publish

//# final image stage
FROM microsoft/dotnet:1.1-runtime-nanoserver
WORKDIR /dotnetapp
COPY --from=builder /src/bin/Debug/netcoreapp1.1/publish .
CMD ["dotnet", "HelloWorld.NetCore.dll"]

    + The second stage uses the runtime .NET Core image, which doesn't have the SDK installed. In that stage I copy the published output from the previous stage, specifying --from=builder

- using the main dockerfile instructions 
- Dockerfile for a simple static website - it uses Internet Information Services (IIS) and serves an HTML 

# escape=`
FROM microsoft/iis
SHELL ["powershell"]

ARG ENV_NAME=DEV

EXPOSE 80

COPY template.html C:\template.html

RUN (Get-Content -Raw -Path C:\template.html) ` 
    -replace '{hostname}', [Environment]::MachineName ` 
    -replace '{environment}', [Environment]::GetEnvironmentVariable('ENV_NAME') ` | Set-Content -Path C:\inetpub\wwwroot\index.html


    + ARG specifies a build argument to use in the image with a default value
    + EXPOSE will make a port available in the image
    + run the static web server 
    ps> docker container run --detach --publish 80 dockeronwindows/ch02-static-website

That's a virtual IP address assigned by Docker, which I can use on the host to communicate with the container. I can browse to that IP address and see the response from IIS running inside the container

- understanding temprary containers and image state. This is an important concept - when you build a Docker image, the instructions execute inside temporary containers. The containers are removed, but the state they write is persisted in the final image 

- working with data in docker images and containers 

- data in layers and the virtual C drive 
    + Each container has its own writable layer on top of all the read-only layers

//# escape=`
FROM dockeronwindows/ch02-fs-1
RUN echo 'from image 2' > c:\data\file2.txt

    + Many options in Docker commands have short and long forms. 

- sharing data between containers with volumes 
    + In the Windows base images, there is only a C drive available, so volumes need to be created on the C drive.

    + create two volumes 
    //# escape=`FROM microsoft/nanoserver
    VOLUME C:\app\config
    VOLUME C:\app\logs
    ENTRYPOINT powershell

    + Because volumes are separate from the container, they can be shared with other containers even if the source container isn't running

    ps> docker container run --name source dockeronwindows/ch02-volumes "echo 'start' > c:\app\logs\log-1.txt"
    
    create a new file in the shared volumes with a task container 

    + read a file from a shared volumes 
    ps> docker container run -it --volumes-from source dockeronwindows/ch02-volumes

    ps> docker container run -it --volumes-from source:ro dockeronwindows/ch02-volumes

- sharing data between container and host with volumes 

    + use container inspect commnad to extract volume information
    ps> docker container inspect --format '{{ json .Mounts }}' source | ConvertFrom-Json

    Instead, you can mount a volume from a specific location on the host when you create a container.

- mounting volumes from host directories 
    + linux 
    docker allowed mount folder with files. it will be auto merged 
    docker allowed mount a single file 
    
    + window 
    can only mount whole directories 

- using volumes for configuration and state 
- packaging a traditonal asp.net web app as a docker image 
    + write a docker file for nerddinner 
    + before move to docker 
    install windows on a clean server 
    run all windows updates 
    install iis 
    install .net 
    set up asp.net 
    copy the web app into c drive 
    create an application pool in iis 
    create the website in iis using the application pool 
    delete default website 

    + create docker file 
    FROM microsoft/aspnet:windowsservercore-10.0.14393.1198
    SHELL ["powershell", "-Command", "$ErrorActionPreference = 'Stop';"]

    WORKDIR C:\nerd-dinner

    RUN Remove-Website -Name 'Default Web Site'; `    New-Website -Name 'nerd-dinner' -Port 80 -PhysicalPath 'c:\nerd-dinner' -ApplicationPool '.NET v4.5'

    RUN & c:\windows\system32\inetsrv\appcmd.exe unlock config /section:system.webServer/handlers

    COPY --from=builder C:\out\NerdDinner\_PublishedWebsites\NerdDinner C:\nerd-dinner

    But in the containerized app, there will be only one website running - another website would be in another container, so we already have isolation, and each container can use the default application pool 


# Developing dockerized .net and .net core applications 

- host existing .net app in docker, need to use the Windows Server Core base image.
- configuring iis for docker-friendly logging 
    +  first need to configure IIS so it writes all log entries from any site to a single file

    + cmdlet in the Dockerfile for the dockeronwindows/ch03-iis-log-watcher image

    + use multiple commands at startup 
CMD Start-Service W3SVC; `     Invoke-WebRequest http://localhost -UseBasicParsing | Out-Null; `     netsh http flush logbuffer | Out-Null; `     Get-Content -path 'c:\iislog\W3SVC\u_extend1.log' -Tail 1 -Wait   

        * start iise W3SVC
        * make a http get request to the localhost 
        * lfush the http log buffer 
        * read the content of the log file 
docker container run -d -P --name log-watcher dockeronwindows/ch03-iis-log-watcher

- promoting environment variables 
//docker file 
ENV A01_KEY A01 value

ps> docker container run -d -P --name iis-env dockeronwindows/ch03-iis-environment-variables

- health check application in docker 

The HEALTHCHECK instruction in the Dockerfile is very simple. You can configure the interval between checks and the number of checks

HEALTHCHECK --interval=5s `
 CMD powershell -command `
    try { `
     $response = iwr http://localhost/diagnostics -UseBasicParsing; `
     if ($response.StatusCode -eq 200) { return 0} `
     else {return 1}; `
    } catch { return 1 }
    
return 0 means ok, 1 means error 

    + use the health check 
ps> docker container run -d -P --name healthcheck dockeronwindows/ch03-iis-healthcheck

- separating dependencies
    + SQL Server LocalDB on the same host where the app is running. LocalDB is an MSI-based installation

    + creating docker image for sql server database 
    install sql server 
    configure sql server 
    run dll scripts to create the database schema 
    run dml scripts to populate static data 

# escape=`
FROM sixeyed/msbuild:netfx-4.5.2-ssdt AS builder
WORKDIR C:\src\NerdDinner.Database
COPY src\NerdDinner.Database .
RUN msbuild NerdDinner.Database.sqlproj `    /p:SQLDBExtensionsRefPath="C:\Microsoft.Data.Tools.Msbuild.10.0.61026\lib\net40" `    /p:SqlServerRedistPath="C:\Microsoft.Data.Tools.Msbuild.10.0.61026\lib\net40"

you can ensure your data files are stored in a RAID array on your server.

packaging the Dacpac file into the image so I have everything I need to create or upgrade the database when the container starts.

you can run the container with a volume mounted from the host, mapping the expected SQL Server data directory from a host directory so your files live outside of the container

- running database in containers 
ps> docker container run -d -p 1433:1433 ` --name nerd-dinner-db ` -v C:\databases\nd:C:\data ` dockeronwindows/ch03-nerd-dinner-db

connect the database outside of the host with SQL SERVER Management Studio 

- kibana, data analyze tool 

- runing a message queue in docker 
The Env class in the web project is a helper class that fetches values for known configuration items

- starting a multi-container solution 
docker container run -d -p 4222 ` --name message-queue ` nats:nanoserver; 

docker container run -d -p 1433 ` --name nerd-dinner-db ` -v C:\databases\nd:C:\data ` dockeronwindows/ch03-nerd-dinner-db;

docker container run -d -p 80 ` --name nerd-dinner-homepage ` dockeronwindows/ch03-nerd-dinner-homepage;

docker container run -d ` --name nerd-dinner-save-handler ` dockeronwindows/ch05-nerd-dinner-save-handler;

docker container run -d -p 80 ` --name nerd-dinner-web ` --env-file api-keys.env ` dockeronwindows/ch05-nerd-dinner-web;

- using elasticsearch with docker,  Elasticsearch is a full search document data store which works well as a reporting database, along with the companion product Kibana which provides a user friendly web front end.

There is a NuGet package for Elasticsearch called NEST, which is an API client for reading and writing data
- adding analytics withh kibana 

 docker container run -d -p 9200 ` --name elasticsearch ` sixeyed/elasticsearch:nanoserverdocker container run -d -p 5601 ` --name kibana ` sixeyed/kibana:nanoserver;
 
 docker container run -d ` --name nerd-dinner-index-handler ` dockeronwindows/ch05-nerd-dinner-index-handler;
- using hybrid .net framework and .net core solutions in docker 

nats, Elasticsearch, and Kibana

NATS.io is a simple, secure and high performance open source messaging system for cloud native applications, IoT messaging, and microservices architectures.

https://nats.io/


# Organizing Distributed Solutions with Docker Compose
- compose file is a yaml file 

web -> database 
|
V 
message queue -> message handler 

Compose files are conventionally called docker-compose.yml

- capturing service definitions 
Compose files are conventionally called docker-compose.ym

docker network create and docker volume create.

- definitiong infrastructure service 
elasticsearch:  
    image: sixeyed/elasticsearch:nanoserver  
    environment:     
        - ES_JAVA_OPTS=-Xms512m -Xmx512m  
    volumes:    
        - es-data:C:\data  
    networks:   
        - nd-net

message-queue, the name of the service 
image, full name of the image to start containers from 
networks, list of the networks to connect containers 

- managing app with docker compose 

Docker CLI. The docker-compose command uses some of the same command names and arguments for the functionality it supports—which is a subset of the functionality of the full Docker CLI

- running compose file 
ps > docker-compose up -d

up command is used to start application 

- you can't have the same host port mapped to multiple container ports. On a Docker swarm where you have multiple hosts, you can scale services with published ports

Docker Compose compares the state of the running application with the configuration in the Compose file 

- other compose commands 
up, down. stop, start 

- You need to have access to the Docker Compose file in order to run any compose commands. This is one of the biggest drawbacks of using Docker Compose on a single host to run your applications.

the laternative is deploy the compose file as a stack to a docker swarm 

- monitor container 
$ docker-compose top

volumes:  
    es-data:    
        external:       
            name: nerd-dinner-elasticsearch-data    
    db-data:    
        external:       
            name: nerd-dinner-database-data

$ docker volume create --name nerd-dinner-elasticsearch-data

- orchestrating distributed solutions with docker swarm,  join several Docker hosts together using the swarm mode.

 introduced in Docker 1.12, and it provides production-grade service orchestration. All communication in a swarm is secured with TLS

- creating a swarm and manage code 

Swarms can be practically any size. You can run a single-node swarm on your laptop to test the functionality, and you can scale up to thousands of nodes

$ docker swarm init --listen-addr 192.168.2.232 --advertise-addr 192.168.2.232

$ docker swarm join --token SWMTKN-1-1rmgginooh3f0t8zxhuauds7vxcqpf5g0244xtd7fnz9fn43p3-az1n29jvzq4bdodd05zpu55vu 192.168.2.232:2377This node joined a swarm as a worker.

To leave a swarm, you need to run the docker swarm leave command on the node itself

container network, containers using this network can communicate with each other, whichever node they're running on
$ docker network ls --filter name=nd-swarm

$ docker service create ` --detach=true ` --network nd-swarm --endpoint-mode dnsrr ` --name message-queue  nats:nanoserver

$ docker service create ` --network nd-swarm --endpoint-mode dnsrr ` --env-file db-credentials.env ` --env-file api-keys.env ` --env HOMEPAGE_URL=http://nerd-dinner-homepage ` --env MESSAGE_QUEUE_URL=nats://message-queue:4222 ` --publish mode=host,target=80,published=80 ` --name nerd-dinner-web ` dockeronwindows/ch05-nerd-dinner-web

















