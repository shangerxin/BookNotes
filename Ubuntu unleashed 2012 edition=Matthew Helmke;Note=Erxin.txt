﻿Ubuntu unleashed 2012 edition=Matthew Helmke;Note=Erxin

# introduction
- author email
opensource@samspublishing.com
- this book is for
become intermediate or advanced users, this book is focus on the first listening stage
- Agile Software Development, Alistair Cockburn, Three levels of listening
	+ following, learner looks for on very detailed process 
	+ detaching, feels comfortable with one method and begins to learn other ways
	+ fluent, experience with or understanding of many methods, doesn't think of any of them in particular while doing a task
- book contains
	+ part I, installation and configuration
	+ part II, desktop ubuntu, use ubuntu on desktop systems
	+ part III, system admin, sophisticated details of setting up a system for specific tasks and maintaining that system
	+ part IV, ubuntu as a server, info to start your own file,web and other servers for home or office
	+ part V, programming linux, provides a great info to how you can extend ubuntu
	+ part VI, appendices, more depth of the previous topics
- format specification
	+ input 
	+ output
	+ note
	+ tip
	+ caution
	
# installing ubuntu
-researching hardware specification
- ubuntu have 12 official variants
ubuntu, server, cloud, netboot, core, kubuntu desktop, mobile, kubuntu mobile, xubuntu desktop, edubuntu mythbuntu, ubuntu studio, lubuntu
- burn cd to media
	+ window
	infraRecorder, http://infrarecorder.org; 
	iso recorder http://isorecorder.alexfeinman.com/isorecorder.htm
	+ mac
	apple's disk utility
	+ ubuntu, right click the iso and select write to disc
- use usb thumb drive, program enable to use image to create a bootable use drive
	+ window
	iso recorder
	+ ubuntu, use the startup disk creator program available for the system
	recommended to use a bootable use to install on apple mac hardware
- the boot loader, ubuntu automatic install GRUB2, grand unified boot loader, to the master boot record
	+ dual boot is not recommended
	install window first and then ubuntu
	+ use Wubi
- backup entire operating system and current installation using clonezilla
http://clonezilla.org
- many users will benefit from three simple partitions
	+ system
	+ user data
	+ swap partition
- shutdown or restart the computer 
$ sudo shutdown -h now
$ sudo shutdown -r now
	
# post-installation configuration
- responds with detailed messages dmesg command, commonly used with grep command
	+ generate a file with output
	dmesg > dmesg.txt
	+ browse the text content
	$ less dmesg.txt
- google search for linux, www.google.com/linux
- copy file
$ cp src dst
- don't name backup file with .bak suffix it may overwrite by some other automatic back program
- the sudo command, is used to change the system, be careful
$ sudo command options
	+ open text file in vi
	$ sudo vi /etc/x11/xorg.conf
	+ execute command with root prompt by 
	$ sudo -i
	system will run all the command with root prompt
- find programs and files, press dash to find these programs
	+ house icon bring you to the main dash screen
	+ the second shows installed app and list other available
	+ third is the documents
	+ fourth shows media
- software update
	+ update manager which is command apt-get
	+ use the GUI, click dash and type in update manager
	+ on desktop version click applications icon| accessories menu and select terminal
	$ sudo apt-get update
	tell the package manager to check and look for updates
	$ apt-get dist-upgrade
	to upgrade the software will dependencies
	with only the upgrade options will only update the the specify module let system admin to gain more control
	+ ubuntu contain a aptitude program similar as apt-get
- linux remote connect with Secure Shell(SSH) to connect terminal
- configuring software repositories, ubuntu use software repositories to get information about available software that can be install, available in update manager
	+ debian, has access to tens of thousands of different packages
	+ a set of volunteers called Masters of the Universe(MOTUs)
	+ adjust which repositories are enabled using the software sources GUI tool
	+ open source versus proprietary, argument about proprietary drivers or software in ubuntu, the program code that is used for drivers or software cannot be viewed and modified by wider community but only the original developers or company that owns it
	+ don't recommend select the option that automatically installs security updates to prevent broken sth. without knowledge
- system settings, search dash for system settings.
	+ install additional drivers, detecting and configuring graphics cards
	+ if any problems are detected there will be a info which called BulletProofX to give you access to tools that may help you fix your problem
    
	+ detecting and configuring a printer
	recommended to search printer support linux before by it
    
	+ open printing database from linux foundation
	www.openprinting.org/printers
    
	+ configuring power management in ubuntu
	suspend, computer write it's current state to memory and goes into low power mode
	hibernate, write state to disk and powers off
    
	+ setting the date and time
	linux time system base on number of seconds elapsed since January 1, 1970
    
	+ change the time and date 
		* with GUI, click the unlock button
		* using the date command
		$ date   //display date
		$ date 090410332010 //set date to Sat Sep 4 10:33:00 MST 2010
		* using the hwclock command to display or set your linux system time
		$ sudo hwclock --show
		$ sudo hwclock --set --date "09/04/10 10:33:00"
		* set the system time from your PC's hardware clock
		$ sudo hwclock --hctosys
		* set the hardware clock with system time
		$ sudo hwclock --systohc
        
	+ configuring wireless networks, use network manager
		* WEP encryption is easily broken
		* WPA is better
		* use vpnc software (install with ubuntu repositories) to connect cisco VPN
		
        
# Working with unity		
- foundations and the X server, ubuntu use the X window system or called X11R7 and X11 (as found on Mac OS X), website
x.org 
	+ ubuntu family, standard version use gnome window manager
	+ kubuntu uses the KDE window manager
	+ xbuntu sues xfce 
	+ add window manager to existing system by using *-desktop metapackages
	kubuntu -desktop package
	automatically isntalls all the software needed for kubuntu
	+ basic x concepts, provides a system of managing displays on local and remote desktops
		* fastjnetowrk is must for many X clients for remote
		* demonstrate the capability of X use Edubuntu's use of LTSP, linux terminal server project
- unsing X, /usr directory and its subdirectories contain majority of the Xorg software
	+ /usr/bin, location of X server and various X clients
	+ /usr/include, path to the files necessary for developing X 
	+ /usr/lib, directory contains required software libraries
	+ /usr/lib/X11, contains fonts default client resources
	+ /usr/lib/modules, path to drivers and the X server modules	
- elements of the xorg.conf file
	+ BulletProofX, used to give restore X system in graphic way, whatever the bad things happen
	+ modern Xorg don't create xorg.conf by default, instead various files ending in *.conf reside in /usr/share/X11/xorg.conf.d directory, user can create the file and continue making custom configuration in /etc/xorg.conf
	+ potential contents of xorg.conf, components
		* ServerLayout, define the display, define one or more screen layout
		Section “ServerLayout”
			Identifier “single head configuration”
			Screen 0 “Screen0” 0 0
			InputDevice “Mouse0” “CorePointer”
			InputDevice “Keyboard0” “CoreKeyboard”
			InputDevice “DevInputMice” “AlwaysCore”
		EndSection
		* Files, define the location of colors, fonts or port number of the font server
		Section “Files”
			RgbPath “/usr/lib/X11/rgb”
			FontPath “unix/:7100”   
			FontPath “/usr/lib/X11/fonts/100dpi”
		EndSection
		* Module, tells the X server what graphics display support code modules to load
		Section “Module”
			Load “dbe”
			Load “extmod”
			Load “fbdevhw”
			Load “glx”
			Load “record”
			Load “freetype”
			Load “type1”
			Load “dri”
		EndSection
		* InputDevice, such as keyboard and mouse
		Section “InputDevice”
			Identifier “Keyboard0”
			Driver “kbd”
			www.it-ebooks.info
			41 Foundations and the X Server
			3
			Option “XkbModel” “pc105”
			Option “XkbLayout” “us”
		EndSection
		Section “InputDevice”
			Identifier “Mouse0”
			Driver “mouse”
			Option “Protocol” “IMPS/2”
			Option “Device” “/dev/input/mice”
			Option “ZAxisMapping” “4 5”
			Option “Emulate3Buttons” “yes”
		EndSection
		* Monitor, X automatic detect monitor and insert definition and configure from the monitordb. This database contains more than 600 monitors and is located in the /usr/share/hwdatadirectory.
		* Device, defines one or more graphics cards and specifies what optional features
		Section “Device”
			Identifier “Intel Corporation Mobile 945GM/GMS,\943/940GML Express
			Integrated Graphics Controller”
			Driver “intel”
			BusID “PCI:0:2:0”
		EndSection
		* Screen, defines one or more resolutions
- starting x, variety of ways
	+ ubuntu, using LightDM
	+ using a display manager, X display manager is presented after you boot linux, controlled by a runlevel, a system state entry in /etc/event.d the following runlevels as handled by ubuntu
	0 halt (do not set initdefault to this)
	1 multiuser text mode
	2 x graphical multiuser mode
	6 reboot(do not set initdefault to this)
	the level 3 to 5 will be treated the same as level 2
		* example
		id:1:initdefault: this will force your system start up in text mode
	+ changing window managers  
		* it is easy to switch window manager
		* first, ensure relative desktop environment installed, install *-desktop package
		kubuntu-desktop
		* next, need to log out of ubuntu, select the session named for the desktop you want to use
- using unity, a primer, at the forefront of GUI development, not only for unix
www.markshuttleworth.com/archives/717
	+ the standard ubuntu default used the unity desktop
	+ the launcher, seems like the window 7, 8's toolbar, could pin or unpin program
		* workspace
		* the dash, support visual search if you don't know the program name
	+ panel, is the top menu bar
	+ customizing unity at the system setting in the dash
	+ power shortcuts, use to define the system shortcut
		* use special key to open dash, (window key), esc to close
		* use middle mouse button to click icon to open multiple instance 
		* holding alt+tab bring up a menu of icons show open programs
		* switching from alt+tabl to alt+` show miniature images of open program windows
		* click print screen key to screenshot
		* use special+t to open the trash can
		* click ctrl+alt+arrow-key to move up, down, right, and left from workspace to workspace
- references
	+ www.gnome.org/ the launch point for more information about GNOME
	+ ww.xorg, curator of the x window system
	+ www.xfree86.org, home of the xfree86 project
	+ https://wiki.ubuntu.com/X, the place to get started when learning about X and ubuntu
	+ https://wiki.ubuntu.com/X/Config, greate information about configuring X on ubuntu
	+ www.sfreedesktop.org/wiki/software/lightdm, a new window manager
	+ http://castrojo.tumblr.com/post/4795149014/the-power user-guide-to-unity
  
  
# On the internet
- get start with firefox, suggest add-ons, 
adblock plus and the 
stumbleUpon plug-ins
xmarks, allow you to synchronize your bookmarks
- check out google chrome and chromium
www.google.com/chrome?platform=linux
chromium, is the metal from which chrome is made
http://code.google.com/chromium
chromium-browser from unbuntu repositories
without branding automatic updates and is used as the test and development foundation for chrome release
- choose a email client, such as elm and pine
	+ evolution,standard email client in ubuntu
	+ mozilla thunderbird, lightweight email client, plug-in
	+ other mail client
	Clasws, offer spell check
	Muttis, older text-based email program
	Alpine, is another text-based program
	kmail
- rss reader
	+ firefox implement rss feeds as calls live bookmarks, only see the headline
	+ liferea, read rss in browser
- messaging and video conferencing with empathy
	+ empathy is the default messaging program in ubuntu
	https//help.ubuntu.com/community/empathy
	+ google talk
- internet relay chart, documented in RFC 2812 and RFC 2813 the internet relay chart is used for text conferencing like email and news, use public RFC server and personal client
	+ XChat
	+ Pidgin
	+ Quassel
	+ Empathy
	+ ubuntu use irc.freenode.net for the server nad primary support channel is #ubuntu
	+ www.irc.org
	+ IRC etiquette rules
	do not use colored text, all-capitalized text, blinking text, bells beeps by sending ^G to terminal
	show respect for others
	ignore people who act inappropriately
	https//help.ubunut.com/community/internetRelayChat
	+ oppular server is IRCd, which you can obtain from 
	ftp://ftp.irc.org/irc/server
	www.irchelp.org/irchelp/ircd
- usenet news groups, free news server, news.gmane.org, makes the red hat and ubuntu mail lists avaliable
gmane.linux, beta list
redhat.rhl.beta
	+ new program in ubuntu
	Pan, ubuntu repositories is a graphical newsreader
- search RFCs at ftp://metalab.unc.edu/pub/docs/rfc/; look at the file rfc-index.text
useNet is alas well past its prime 
- ubuntu one cloud storage, 2gb space to use
- references
www.novell.com, evolution software
www.mozilla.com, home page for mozilla firefox
www.spreadfirefox.com, firefox advocacy home page for converting those internet explorer types
https://one.ubuntu.com, website for ubuntu one
en.wikipedia.org/wiki/usenet, history of usenet


# Productivity applications
- office app
	+ LibreOffice
		* writer, like ms word
		* calc, like ms excel
		* impress, presentation program like powerpoint
		* math, formula editor
		* base, database application
		* draw, graphic applications
		* dia, technical drawing editor from GNOME office suit, like ms visio
		* planner, project management app for project planning
	+ the gap between vba and third party office like program
	VBA is not automatic converted
	+ history of LibreOffice
	OpenOffice.org office suite is based on a commercial suite called StarOffice. Sun purchased the company and release most of the code as GNU Public License which is OpenOffice.
	Orcale bought Sun and OpenOffice community fork the and temporary name as LibreOffice
	IBM is using this Apach-licensed version of OpenOffice.org called Lotus Symphony 
- Other office suites for ubuntu
	+ GNOME Office 
	+ KOffice, default KDE 
	+ the state of Massachusetts not long ago elected to standardize on two file formats for use in government
		* Adobe Acrobat PDF format
		* OASIS OpenDocument format,(Microsoft don't support this format)
- Working with GNOME Office
	+ GTK widget set
	+ GUI widget set
	GTK+, QT and Motif
	+ Componetns of GNOME
		* Abiword
		* Gnumeric
		* GIMP, create image for common use
		* Evolution, mail client
	+ website
	www.gnome.org/gnome-office
- working with KOffice
- other useful productivity software
		+ working with pdf, adobe reader
		+ LibreOffcie contain a program to edit PDF files, have extension www.libreoffice.org/features/extensions/ and install using the instructions on that site
		+ pdfeidt search from the ubuntu software center
- working with XML and DocBook, docbook is a document define in xml format
	+ Gedit, quick editor
	+ Publican, for complicated documentation in DocBook format
	+ XML Copy Editor
- working with LaTex, created for academia, it is a WYGIWYW(WYSIWYG acronym for what you see is what you get) document markup language created for the TeX typesetting system, multiple editors are available for use with LateX
	+ Texmaker, avaliable in all platform
	www.xmlmath.net/texmaker
	+ LyX, all platform
	+ Kile, for KDE
	http://kile.sourceforge.net
- Productivity applications written for microsoft windows, install and run some microsoft app with an application named Wine in Linux
	+ Wine is default added with ubuntu as it frequent updates
	www.winehq.org
	http://appdb.winhq.org/appbrowse for supported migrate app 
	+ CodeWeavers, use productivity application of microsoft
	www.coeweavers.com
- references
www.libreoffice.org, the home page for the libreoffice
www.documentfoundation.org, the home page for the document foundation
www.openoffice.org
http://incubator.apache.org/openofficeorg, home page for the apache software foundation's incubator project for OpenOffice.org
www.oracle.com/us/products/applications/open-office/index
www.gnome.org/gnome-office
www.koffice.org/
http://pdfedit.cz/en/index.html
www.codeweavers.com


# Multimedia applications
- sound and music, linux is lacking good support for multimedia
- unix however have good multimedia support, unix have limitation of hardware to support but linux have hundreds of sound cards
- sound Cards, ubuntu supports a wide variety of sound hardware and software, two models of sound card
	+ ALSA, the advanced linux sound architecture, entirely open source
	+ OSS, the Open Sound System, which offers free and commercial drivers
ubuntu uses ALSA
	+ ALSA support a long list of sound card
	www.alsa-project.org
	+ APIs major sound servers already in use, ESD, OSS, GStreamer and aRts, PulseAudio is young but powerful, www.pulseaudio.org
- adjust the volumne by the desktop's right corner
- sound format
	+ RAW, .raw, know as headerless format contain an amorphous variety of specific setting
	+ MP3, .mp3, but commercially licensed, not install by ubuntu by default
	+ WAV, .wav, popular uncompressed windows audio-visual sound format
	+ Ogg-Vorbis, .ogg, ubuntu's preferred audio encoding format, open-source encoding
	+ FLAC, .flac, lossless format popular with audiophiles
	+ MPEG, MPEG2 and MPEG3(MP3) decoders are not install by default
	+ AU, from NeXT aand Sun
	+ AIFF from apple and SGI
	+ IFF originally from commodore's Amiga
	+ RA for real audio 
	+ VOC from creative labs
- check file format by www.file-info.com
- listening to music	 
	+ rhythmbox, plays CDs
	+ Banshee, music application can handle ripping and playing back music
	+ Sound Juicer, ubuntu install by default
	+ buy music from the ubuntu music store
- graphics manipulation
	+ Shotwell Photo Manager in application in ubuntu
	+ GNU Image manipulation program, GIMP a free GPL-licensed image editor, lack two features for artist
		* color separation for commercial press printers(CMYK, for the color cyan, magenta, yellow and key[or black]), GIMP use RGB
		* use of Pantone colors(a pentented color specification) to ensure accurate color matching
		* these may fix by the plug-ins
		* GIMP is like adobe photoshop
- using scanners in ubuntu, image scanners with GIMP, also use XSane by itself
- working with graphics formats, ubuntu support many image format conversion
bmp, gif, jpg, pcx, png, svg, tif  
$ convert image_name.type image.type
	+ convert utility from ImageMagick, is the netpbm family of utilities, ubuntu install the netpbm tools by default
	+ netpbm tools
		* ppm, portable pixmap file format
		* pgm, portable graymap file format
		* pnm, portable anymap file format
		* pbm, portable bitmap file format
		* nautilus-image-converter package, easiest way to resize or rotate image file
- Capturing Screen
	+ GNOME desktop use built-in screen shot mechanism, gnome-panel-screenshot
	+ Screen key, alt+print(takes a screenshot of only the window that has focus on a desktop)
	+ search for tools for screenshot
- digital cameras
	+ webcams(small, low resolution)
	+ surveillance cameras connect directly to a network via wired or wireless connection
	+ photo manager, Shotwell Photo Manager
- creating CDs and DVDs 
	+ with Brasero
	+ with command-line use mkisofs command
	$ mkisofs -r -v -J -l -o /tmp/*.iso /source_drectory
	+ write the iso to cd with cdrecord
	$ cdrecord -eject -v speed=12 dev=0,0,0 /tmp/our_sepcial_cd.iso
	+ learn more 
	www.cdmediaworld.com/hardware/cdrom/cd_oversize.shtml
	+ creating dvd from command -line
		* formats, Differences in the + and – formats have mostly to do with how the data is modulated onto the DVD itself, with the + format having an edge in buffer underrun recovery. 
		dvd+r, dvd-r, dvd+rw, dvd-rw
		* need to have dvd+rw-tools package installed as the cdtools package
		dvd+rw-format to format the dvd
		growisofs, write data to disk
		* some dvd may preformated read the package of the dvd
		* pip data into dvd
		$ sudo your_application | growisofs -Z /dev/scd0=/dev/fd/0
		$ sudo growisofs -Z /dev/scd0=image to burn existing image
- viewing video, supported tv card
	+ supported list
	www.sexploits.org/v41/
	+ Video4Linux supports with a device driver
	+ Matrox, Marvel, Matrox Rainbow Runner
	+ RivaTV cards
	+ detect PCI device by lspci to check the installed chipset
	+ calculate the module and kernel dpendencies with 
	$ sudo depmod -a
	$ sudo modprobe module name to load the module
- video format
avi, flv, mepg, mov, ogv/ogg, qt, webm google's royalty-free container for audio and video designed for html5
	+ ubuntu does not support any of the propriety video codecs due to licensing restrictions
	+ support add ubuntu-restricted-extras package from ubuntu software repositories
	https//help.ubuntu.com/community/restrictedformats
	+ watch video and dvd with totem movie player installed by default, the flash will also be included
	+ VLC, another video viewer, for all platform, VLC used his own codecs and if it can't play the file then the file may not be played
- personal video recorders
	+ commercial personal video recorder, TiVo
	+ Myth TV, turn PC to digital video recorder, www.mythtv.org
	+ ubuntu used MythBuntu project at www.mythbuntu.org
- video editing
	+ PiTiVi, it's not install by default, supported GStreamer
- references
www.videolan.org, VideoLAN project 
http://fy.chalmers.se/~appro/linux/DVD+RW/, the dvd+rw+r-r[w]  for linux
www.gimp.org, home page of GIMP
http://f-spot.org, home page of the F-Spot project
www.sane-project.org, home page of the SANE(scanner access now easy)
http://gimp.net/tutorials/, 


# Other ubuntu desktops official tutorials for the GIMP
- unity is not the only option
- desktop environment, GUI
	+ ubuntu software repositories contain all the 
	+ GNOME window manager is called Metacity if your video card is better enough then ubuntu will install Compiz, began back in 1996, which is the top two desktop in linux region, GNOME file manager(Nautilus), GNOME3 is quit similar to unity
	+ Enlightenment, which does things in a unique and interesting manner
	+ KDE, kubuntu is a version Ubuntu + GNOME + KDE
	+ Xfce, lighter desktop environment, requires less memory and processing power than either GNOME or KDE, developed use UNIX philosophy, Xfce(Thunar) file manager, Xbuntu was designed to create a lighter-weight version of unbuntu, it use less memory and fewer CUP cycles than a standard ubuntu
	+ LXDE and Lubuntu a new experimental, based on LXDE, extremely fast desktop environment, much faster than previous, Knoppix which is a linux distribution that runs from a live, bootable CD or DVD, now uses LXDE, long time favorite for sysadmins for emergency repairs
- UNIX philosophy of software, do one thing, do it well and play well with others
- references
www.xfce.org, official site for Xfce
www.xubuntu.org
www.lxde.org, official site for LXDE
http://lubuntu.net
www.gnome.org
www.knoppix.net, site for the live CD/DVD linux distribution
http://xwinman.org, a nice discussion of manay of the different window managers and desktop environments avaliable for linux and unix systems
http://en.wikipedia.org/wiki/desktop_environment, detailed definition of a desktop environment, how they work and why they exists
http://en.wikipedia.org/wiki/comparison_of_X_window_system_desktop_environments


# Games
- ubuntu gaming
	+ default games
	solitaire, sudoku
	+ Emulators, LucasArts ScummVM games, search for Dgen/SDL, DosBox, xtrs, FCEUltra, GnGeo, SDLMama, ScummVM and Stella
- install propriety video drivers
AMD and Nvidia support Linux now, ubuntu does not ship with native 3d drivers but easily installed
for older card there are lots of development going totally free and open-source drivers
Dash search for additional drivers
- installing game in ubuntu
	+ Warsow, a free and fast-paced first-person shooter game, avaliable for all platform
	+ Scorched 3D, based on an old dos game called scorched earth, try to destroy other player by missile like weapon, (3d crazy tank)
	+ Frozen Bubble, like bubble dragon
	+ SuperTux, flashy and pretty-looking games, 2d side scroller
	+ Battle for Wesnoth, most popular games currently for linux, a strategy game featuring both single and multiplayer, come with a map editor
	http://wesnoth.org
	+ Frets on Fire, similar game like Guitar Hero
	+ Games for kids, 
	+ Commercial games
- playing windows games, use project Wine to play window games, Wine is not a emulator, it's a compatible layer
www.winehq.org
Cedega is a fork of the Wine project
www.transgaming.com
- references
www.nvidia.com/object/unix.html
support.amd.com/us/gpudownload/Pages/index.aspx


# Managing Software
- ubuntu software center 
- Synaptic for software management, for reconfigure your installation system, such as a library. Synaptic, it could be installed by software center which is not install by default. it's seems like Add/Remove application window
- staying up-to-date, manage software updates through Synaptic, ubuntu provide a Update Manager(launched through System)
- working on command line, apt command, Advanced Package Tool, a person posting on Slashdot.com, it's the first system handles dependencies in software, apt automatic find and install dependencies
	+ day to day usage
	$ sudo apt-get update  ..update the avaliable update list
	$ sudo apt-get upgrade  ..update the software
	+ basic apt-ge will not remove or add new software
	+ when something need to remove or add may call upgrade failed then need to use
	$ sudo apt-get dist-upgrade
	+ adding software
	$ sudo apt-get install software_name
	install mysql
	$ sudo apt-get install mysql-server
	add mysql's mailx package
	$ sudo apt-get install mysql-server mailx
	+ removing software
	$ sudo apt-get remove name
	+ find which file is included in which package by 
	$ sudo apt-file find file_name
- finding software, general search tool apt-cache
$ apt-cache search kde
will return a software list contain the name with kde
then you could narrow down the list by 
$ apt-cache -n search kde
apt-cache is also support regular expression search
$ apt-cache -n search ^kde
	+ using apt-cache with grep to search within search result
	$ apt-cache search games | grep kde
	+ display the additional info
	$ apt-cache showpkg mysql-server-5.0
	this show package on reverse depends
- compiling software from source
	+ use code in the ubuntu repositories
	+ use code by upstream developers
	+ pre request to compile need to install package build-essential to ensure that you have the tools
	+ also need to install automake and checkinstall, which are build tools
	+ compiling from tarball, compressed source tarballs that is a tar files have been compressed using gzip or bzip
		* compile software as a regular user to prevent potential damage
		* create a source directory in the home directory
		* uncompress the tarball
		$ tar zxvf packagename.tgz -C ~/source
		$ tar zxvf packagename.tar.tgz -C ~/source
		$ tar zxvf packagename.bz -C ~/source
		$ tar zxvf packagename.tar.bz2 -C ~/source
		* check the file compress type
		$ file packagename
		* check the readme or install file contain
		* normal install steps
		~/source/packagename$ ./configure	..check all dependencies are met
		~/source/packagename$ make
		~/source/packagename$ sudo make install
		if anything goes wrong clean the build environment and fixed and recompile
		~/source/packagename$ make clean
		~/source/packagename$ make make uninstall
		remove the software
	+ compiling from source from the ubuntu repositories
		* get the source 
		$ apt-get source foo
		* install the build dependencies
		$ sudo apt-get build-dep foo
		* switch to the package directory
		$ cd foo-4.2.2
		* make whatever changes you want to make by ./configure and make command
		* create a new debian/changelog entry
		$ dch -i
		* package number pattern for ubuntu
		inherited from debian with no changes would be 4.5.2-1
		changed for ubuntu would be 4.5.2-1ubuntu1(and then ubuntu2 for a second version)
		package with debian but which create for ubuntu be 4.5.2-0ubuntu1
		* build the source package
		$ debuild -S
		* finally a *.deb package will be created and you can install
		$ sudo dpkg -0i *.deb
- Server/Configuration Management, tools may by useful
	+ Puppet, configuration management tool, written in ruby, make it easier to deploy server and scale application across a network. It has been longer than Chef both are commonly used
	+ Chef, configuration management tool also written in ruby. you write recipes that describe how you want your server or specific server software to be configured
	+ Juju is ubuntu's entry into this field, new and not yet considered stable or ready for anything but testing, hopefully by 12.04. designed to do same things as Chef and Puppet but more as well, ubuntu in cloud
	+ Landscape, an enterprise-focused systems management and monitoring tool, avaliable from Canonical. It's a part of ubuntu Advantage program(www.canonical.com/enterprise-servers/ubuntu-advantage), it could monitor local as well as cloud servers
	+ dotdee, linux-based systems find series of directories that end with a .d and called 'dot dee' directories
	in ubuntu or other Debian-based systems, it is a violation of etiquette(and Debian policy) for any software package to be allowed to directly change the configuration files of another package This can be a problem when you want to use system configuration management software
	dotdee solves this problems by take any flat file and replace it with symlink pointing to a file that is generated from a .d-style directory. It then saves the original file and then updates the generated file automatically
	dotdee used inotify to dynamically and instantly update the master file. the master file can be build three different ways
		* using flat files
		* using diff/patch files
		* using executables, process stdin and dump to stdout
- references
http://www.nongnu.org/synaptic/, home of Synaptic package manager
http://www.debian.org/doc/manuals/project-history/ch-detailed.en.html
http://www.ubuntu.com/usn, offical list of ubuntu security notices
http://www.opscode.com/chef/
http://www.puppetlabs.com
juju.ubuntu.com
http://help.ubuntu.com/11.10/serverguide/C/index.html, ubuntu server guide


# Command-line quickstart
- CLI, command-line interface
- many commands were created by GNU project as free software analogs to previously existing proprietary UNIX commands
www.gnu.org/gnu/thegnuproject.html
- most linux server have no GUI and all administration is done using command-line
- introduce
	+ routine tasks, login/out password, listing/navigating
	+ basic file management
	+ basic system management
- easy to transferred to other UNIX and UNIX-like operating systems, such as Solaris, OpenBSD, FreeBSD, even Mac OS X
- accessing the command line, 
	+ desktop termial is gnome-terminal
	+ press ctrl+alt+f1 to f6 to different console, ubuntu will switches to a black screen and a login prompt, ctrl+alt+f7 to switch back to graphic interface
	tty1, one of six virtual consoles that ubuntu provides, switch the virtual environment by press alt+cursor key (left or right)
	+ remote connect by telent or ssh commands
- text based console login, login at home directory and linux use '~' as shortcut
- check current location, pwd, path work directory
- log out, use 
exit, logout, ctrl+d 
- log in/out from a remote computer
	+ use ssh, secure shell client, login session are encrypted
	ssh IP
- user accounts
	+ user-based security
		* normal user
		* super-user, use sudo follow with other command
	+ example of destructive nature of working as super user can be 
		* erase everything
		sudo rm -rf
	+ ubuntu is different with traditional linux distributions have a specific user account called root
	in this case, if your account login as root privilege the command-line promoter will be # instead of $
	root account is disable by default in ubuntu
		* make current account run command under root privilege
		$ sudo passwd
		then enter your password, if your account have super-user privilege then you don't need to type sudo before each command to run as super-user
		$ sudo -i
		have the same behavior
		* reference
		https://help.ubuntu.com/community/RootSudo
- understanding the linux file system hierarchy, standard linux distro
/	   the root dir
/bin	essential commands
/bbot   boot loader files, linux kernel
/dev	device files
/etc	system configuration files
/home   user home dir
/lib	shared libraries, kernel modules
/lost+found directory for recovered files(if found after a file system check)
/media  mount point for removable media, such as dvds and floppy disks
/mnt	usual mount point for local, remove file systems
/opt	add-on software packages
/root   super user(root) home
/sbin   system commands(mostly root only)
/srv	holds information relating to services that run on your system
/sys	real-time information on devices used by the kernel
/tmp	temporary files
/usr	software not essential for system operation, such as applications
/var	variable data(such as logs); spooled files
- isolating directories form one another for server security, such as /boot that doesn't change often on its own partition and making it read-only 
- essential commands in /bin and /sbin, often the commands in these two directories are statically linked commands don't depend on software libraries residing under the /lib or /usr/lib directories
nearly all the other applications on your system are dynamically linked
- configuration files in /etc, software package such as Apache, OpenSSH and xinetd have their own subdirectories in /etc filled with configuration files, others like crontab or fstab use one file
	+ fstab, file system table is a text file listing each hard drive, cd-rom, floppy or other storage device.
	fstab can be manipulated by root using mount command
	+ moprobe.d, folder holds all the instruction fo load kernel modules
	+ passwd, the list of users for the system include syslog and CouchDB
	+ sudoers, a list of users or user groups with super user access
- user directories, /home
	+ named by default according to account username
	/home/user_name
	+ segregating the system and user data is good
		* local PC
		place /home on the different partition
		* cloud 
		place remote computer on the network
- using the contents of the /proc directory to interact with kernel, many linux utilities extract information from dynamically created directories and files under this directory, also know as a virtual file system
	+ free command obtains it's information from a file named meminfo
	+ get the same info by cat command, see the content of meminfo file
	$ cat /proc/meminfo
	+ dynamically alter the behaviour of a running linux kernel echoing numerical values to specific files under the /proc/sys/ directory
	turn on kernel protection against one type of denial-of-service(DoS) attack known as SYN flooding
	$ sudo echo 1 >/proc/sys/net/ipv4/tcp_syncookies
	+ other ways
		* getting cpu info, /proc/cpuinfo
		* viewing important networking info under /proc/net
		* retrieving system info
		* reporting media mount point information via USB, such as /dev/sda, if usb device plug in use dmesg command to check info
		* getting the kernel version /proc/version
		* performance, uptime, /proc/uptime
		* statistcs, such as CPU load, swap file, /proc/stat
- working with shared data in /usr directory, /usr/share/man
- temp file, /tmp
- accessing variable data files, /var, contain subdirectories used by various system services for spooling and logging, incoming email is usually directed to files under /var/spool/mail
Transfer Protocol(FTP) directory under /var/ftp
apache web server's initial home, /var/www/html
	* recent trend to move data that is served from /var/www and /var/ftp to /srv
- navigating the linux file system
	+ listing the content of a directory with ls
	ls
	ls -a ..see hidden
	
	ls -al ..list owner and group info and hidden
	details format are
	-rw-r--r-- 1 matt matt 335 2011-05-16 16:48 file.txt
	filetype, permission, link count, owner, group, file size, last access date/time, file name
	
	ls -R
	scans and lists all contents of the subdirectories of the current directory
	+ change directory by cd
	$ cd $home
	$ cd ~
	+ find current dir, pwd
- working with permissions
	+ examine the default permission by using umask command list default permission
	+ use touch and ls to list long-format listing
	$ touch file
	$ ls -l file
	-rw-r--r-- 1 matt matt 335 2011-05-16 16:48 file.txt
		* touch command used to create a file
	+ assigning permissions, linux grouped permission by owner, group and others
	owner group others, permission can indicated by memonic or ocatal characters
		* r, read
		* w, write
		* x, execute
		* 4 read
		* 2 write
		* 1 execute
	in octal notation set a file of 664(read+write, read+write, read-only)
- directory permissions
	+ create dir mkdir
	+ ls -ld directory to check permision
	drwxr-xr-xr 2 matthew matthew 4096 2010-06-30 13:23 directory
	such as 755(read+write+execute, read+execute, read+execute)
	the first d character means it's a directory
	+ directories need execute permission for anyone to be able to view their contents
	+ example the device file for a linux serial port
	$ ls -l /dev/ttyS0
	crw-rw---- 1 root dialout 4, 64 2010-06-30 08:13 /dev/ttyS0
	it means /dev/ttyS0 is a character device(such as a serial communication port and designated by a c) owned by root and available to anyone in the dialout group, the device has permission 660(read+write, read+write, no permission)
	+ check IDE port
	$ ls -l /dev/sda
	brw-rw-- -- 1 root disk 8, 0 2010-06-30 08:13 /dev/sda
	b designates a block device( a device that transfers and caches data in blocks). 
	+ other device entries you will run across on linux include symbolic links, designate by s
- altering file permission with chmod, support mnemonic form or octal(such as u,g,o or a and rwx, and so on), support add remove, directory permissions except to the root account
	+ u, add or remove user read, write or execute
	+ g, add or remove group read, write or execute
	+ o, add or remove read, write, or execute for other not in a file's group
	+ a, add or remove read, write or execute permission for all users
	+ r, add or remove read permission
	+ w, add or remove write
	+ x, add or remove execute
	+ example
	$ chmod u+rw readme.txt
	$ chmod 600 readme.txt
- file permissions with chgrp, change the group to which a file belongs
$ chgrp wheel filename
- change file owner chown
$ chown matthew filename
- ?understanding set user id and set group id permissions, to enable any user running that program to have program owner or group owner permissions for that program
	+ suid, set user, id, SGID (Set User ID up on execution)
	+ sgid, set group id, SGID (Set Group ID up on execution)
	+ setting SGID(+s) to group who owns this file
	$ chmod g+s file1.txt
	$ chmod 2750 file1.txt
	$ ls -l	..check
	-rwxr--r-- 1 xyz xyzgroup 148 Dec 22 03:46 file1.txt  ..before set
	-rwxr-sr-- 1 xyz xyzgroup 148 Dec 22 03:46 file1.txt  ..after set
	+ common use command
	$ ls -l /usr/bin/passwd, allowed normal users to execute the command to make changes to a root-only-accessible file /etc/passwd
	-rwsr-xr-x 1 root root 42856 2010-01-26 10:09 /usr/bin/passwd
	+ other files might have suid or guid permission include at, rcp, rlogin, rsh, chage, chsh, ssh, crontab, sudo, sendmail, ping, mount and several UNIX-to-UNIX Copy(UUCP) utilities, many program such as games might also have this type of permission
	files or programs that have suid or guid permissions can sometimes present security holes. the problem is compounded if the permission extends to an executable binary
	+ suggest to keep the number of suid and guid file minium, use find command to list all the such files
	matthew@seymour:~$ sudo find / -type f -perm /6000 -exec ls -l {} \;
	+ such as mount to be suid might not be the best security policy, to limit user use CD-ROMs or other media
	+ other candidates for suid permission change could chsh, at or chage command
	+ you also can assign similar permission with the chfn command, this command allows users to update or change finger information in /etc/passwd
	You accomplish this permission modification by using a leading 4(or the mnemonic s) in front of the three octal values.
- working with files 
	+ create a file with touch command
	$ touch filename
	will create a empty file, also could use touch to update time and date fino about the file
	support relative path and absolute path
	matthew@seymour:~$ touch randomdirectory/newfile
	matthew@seymour:~$ touch /home/matthew/randomdirectory/newfile
	+ create a directory with mkdir
	+ delete a directory with rmdir, the directory must be empty
	+ delete a file or directory with rm, the directory must not be empty
		* use rm with -R options, its a recursive switch which work with many commands
		$ rm -R /somedir
		it works like rmtree in python
	+ recover the deleted file is not easy from the command line, only a professional data recovery service is likely to do so
	+ move or rename a file with mv
	$ mv src dst
	$ mv oldname newname
	+ copy a file with cp
	+ display the content of a file with cat, the file is display on the screen but can't edit
	$ cat filename
	+ display a content of a file with less, to view a longer text file, support arrow up/down, page up/down to scroll through the content
	$ less filename
	in early days of UNIX a program more do the same task
	+ using wildcards and regular expressions, each of above command support pattern-matching with wildcard or regular expression
	$ rm abc*
	+ learn more by reading the grep manual (man grep)
- working as root	
	+ keep caution, example of destroyable command
	$ sudo rm -rf /
	this command will not only deletes files and directories but also wipe out file systems on other partitions and even remote computers
	+ knowing how to run commands as the super user(root) without logging in as root can help avoid serious missteps
	execute single commands as root
	$ sudo nano -w /etc/fstab
	edit system's file system table
	+ CAUTION
	before editing any important configuration file, make a backup. Make sure to launch your text editor with line wrapping disabled, or you could insert spurious carrage returns make causing configured service to fail restarting.
	nearly all configuration file formatted for 80-character text width, vi and emacs don't use wrap by default
- creating users, also a user's directory created at home directory
	+ add user
	$ sudo useradd heather
	+ initial user's password, without this the created user can't login
	$ sudo passwd username
	+ check useradd command's default setting by 
	$ useradd -D
	learn more by check man useradd
	+ deleting users
	$ sudo userdel -r username 
	the -r option will set to not only remove user's entry in the /etc/passwd file but also remove all the user's files and directories
	without -r you have to manually delete the user's directory at /home and user's /var/spool/mail queque
- shutting down the system, cleanly shutdown linux is used the -h or halt option
$ sudo shutdown -h now
$ sudo shutdown -h 0
$ sudo shutdown -h 18:30
	+ display message for all the user login the system by editing Message of Day(MOTD) mod file
	/etc/motd
	ubuntu support automatically and regularly update information in the MOTD using cron
	+ TIP
	Do not shutdown your system if you suspect that intruders have infiltrated your system; instead disconnect the machine and make a backup copy of your hard drives.
	Keep the machine running to examine the contents of memory and system logs
	+ rebooting the system
	$ sudo shutdown -r now
- reading documentation and check the official website for more information
	+ using apropros command, linux is self document system use this command to check relative info
	$ apropos partition
	which will list all the command relative to the word partition
	to find command and its documentation you can use whereis command
	whereis fdisk
- using man pages, learn more about a command or program by man command, the less command will be used to display the message, press forward slash (/) to enter a search string, press 'q' to quit
$ man command
	* use info command for check more detail of a command
	$ info command
	such as $ info info
	+ build-in shell commands are commonly used
		* managing users and groups
		chage, chfn, edquota, gpasswd, groupadd, groupdel, groupmod, groups, mkpasswd, newgrp, newusers, passwd, umask, useradd, userdel, usermod
		* managing files and file system
		cat, cd, chattr, chmod, chown, compress, cp, dd, fdisk, find, gzip, ln, mkdir, mksfs, mount, mv, rm, rmdir, rpm, sort, swapon, swapoff, tar, touch, umount, uncompress, uniq, unzip, zip
		* managing running programs
		bg, fg, kill, killall, nice, ps, pstree, renice, top, watch
		* getting information
		apropos, cal, cat, cmp, date, diff, df, dir, dmesg, du, env, file, free, grep, head, info, last, less, locate, ls, lsattr, man, more, pinfo, ps, pwd, stat, strings, tac, tail, top, uname, uptime, vdir, vmstat, w, wc, whatis, whereis, which, who, whoami
		* console text editors
		ed, jed, joe, mcedit, nano, red, sed, vim
		* console internet and network commands
		bing, elm, ftp, host, hostname, ifconfig, links, lynx, mail, mutt, ncftp, netconfig, netstat, pine, ping, pump, rdate, route, scp, sftp, ssh, tcpdump, traceroute, whois, wire-test
- references
https://help.ubuntu.com/community/usingTheTerminal, help page for using the terminal
https://help.ubuntu.com/community/rootSudo, explaining sudo the philosophy behind using it by default
https://help.ubuntu.com/community/linuxFileSystemTreeOverview, overview of the linux file system tree
	  
	  
# Command-Line Master Class
- to believe two short chapters will make any reader a true master is foolish
- give enough information to enable you to perform all basic and vital tasks from command line
- reference book
The Art of Unix Programming, Eric Raymond wrote  a short story to describe why use command line
- basic commands
cat	 print the contents of a file
cd	  change directories
chmod   change file access permission
gcc	 compiles c/c+=/fortran programs
grep	search for a string input
awk  Finds and Replaces text, database sort/validate/index, awk command searches files for text containing a pattern. When a line or text matches, awk performs a specific action on that line/text.
less	filter for paging through output
ln	  creates links between files
locate  find files from an index
ls	  lists files in the current directory	  
make	compile and install program
man	 display manual pages
mkdir   makes directories
mv	  mv files
ps	  list processes
rm	  deletes file and directories
ssh	 connect to other machine using a secure shell connection		  
tail	print the last line of a file   
top	 print resource useage	  
vim	 edits text files
which   prints the location of a command
xargs   executes commands from its input
both emacs and vim have text-based interface
- print content of a file with cat, common parameters
	+ used -n numbers the lines in the output
	+ used -s ('squeeze') print a maximum of one blank line at a time
	+ used -s shows the first line of text
	+ example check cpu info
	$ cat -sn /proc/cpuinfo
	+ print several files together
	$ cat -s file0.txt, file1.txt
- change directories
cd command relative path define in the CDPATH environment variable
	+ switch base relative path
	$ export CDPATH=/usr
	$ cd local
	/usr/local
	then relative change directory will base on /usr
	+ multiple relative directory
	matthew@seymour:~/empty$ export CDPATH=.:/usr
	matthew@seymour:~/empty$ cd local
	/home/matthew/empty/local
	matthew@seymour:~/empty/local$
	CDPATH is set to be .:/usr. The :is the directory separator, so this means bashshould look
	first in the current directory, .,and then in the /usrdirectory.
- change file access permissions with chmod
$ chmod -c 600 *
output the result of operation
	+ specify a file to use as a template
	$ chmod --reference /home/matthew/myfile.txt *
	+ recursive operation
	$ chmod -cR 600 *
- copying files with cp, two parameters
	+ create same parents path to the copied directory, --parents, this will be a create help for create backup files
	$ cp --parents src dst
	+ copy only the different with -u
	$ copy -Ru src dst
- printing disk usage with du, prints the size of each file and directory that is inside the current directory
	+ print individual files, -a
	+ make du use human-readable like 18M rather than 17532(bytes) with -h
	+ total size of file -c
	$ du -ahc /home
	+ exclude files with --exclude specify a pattern
	$ du --exclude="*.xml"
	+ with -X to specify a file contain the pattern such as *.xml
	$ du -X xml_exclude.xml
	+ running du in a directory where several files are hard-linked to the same inode counts the size only once, disable it by -l (lowercase L)
- parameter pattern
	+ UNIX standard is -c, -s
	+ GNU standard is --dosomething
	+ X-style parameters merge the two by having words preceded by only one dash
- finding files by searching with find, need to use X-style parameters
	+ find have too many parameter, capable of doing most things you want
	+ basic use
	$ find -name "*.txt"
	which will do a recursive search with case sensitive
	$ find -iname "*.txt"
	without case sensitive
	+ filter with file size
	$ find ~ -name "*.txt" -size [100k|+100k|-100k]
	-100k will match less equal than 100k
	+100k will match great than 100k
	+ filter with owner by -user username
	+ flip the condition by adding -not before the parameter
	+ filter with permission, -perm, which is specify with chmod. same rule u for user, g for group, o for others, r for read, w for write and x for execute
	without +/- need match exactly
	plus will need to match any modes
	minus will need to match all the modes may be it contain more mode
	$ find /home -perm -o=r
	$ find /home -perm +o=r, u=w
	$ find /home -perm +guo=r
	+ execute a external program each time a match is made by -exec. your command will follow immediately after -exec, terminated by \; you can insert filename match at any point using {} 
	$ find / -name “*.txt” -size +10k -user matthew -not -perm +o=r -exec chmod o+r {} \;
	match all text files on the entire system(/ from root) over 10KB, owned by matthew, that are not readable by other users, and then use chmodto enable reading
- searches for string in input with grep, like find, grep process any text whether in files or just in standard input
	+ basic 
	$ grep "some text" *
	search all the file in the current directory(without sub) for the string
	enable recursive search in sub with -r/-R
	each time a string is matched with in a file the filename and the match are printed
	+ force grep to print the name of each file that contains at least one match, without print the match content by -l (lowercase L)
	+ pring each filename was search include the number of matchs at end by -c
	+ invert match by -v
	+ use regular expression
	$ grep "[cms]at" my.txt
	+ without case sensitive, -i
	+ print line number, -n
	+ color the search tern, --color, using GREP_COLOR, export GREP_COLOR=36 gives you cyan, export GREP_COLOR=32 gives you lime green
- paging through output with less, less has the infamy of being one of the few linux commands that have a parameter for every letter of the alphabet, some letters even differentiating between upper and lower-case, these are only used when invoking less
after viewing even more commands, less is a complex beast to master, most often used
	+ adding -M(different from -m), print filename, verbose prompting line number 
	+ adding -N(different with -n) enables line numbering
	+ adding, +, allow you to pass a command to less for it to execute as it starts
	$ less +?hello myfile.txt
	search hello backward after open
	+ basic command, after open text file
		* /, text search forward, press enter to go each line
		* ?, search backward
		* ng, to go to specify line such as 50g
		* np, go to n percent
		* m, mark a position with a single letter, press ` then enter the same letter to return
		* v, open file in text editor, default to vim, change this by setting EDITOR environment
	+ support view several file together
	$ less -MN 1.txt 2.txt
	the -M parameter will let less display different file names such as 1.txt(file 1 of 3) at bottom
	navigate between file by typing a colon(:) and pressing n to go
		* after go to next file, p to go back, could be reference as :n, :p
		* open another file by typing :e providing a filname(use TAB to complete filename)
		* close a file by :d
		* by default less search in one file, search within all files when type / or ? follow with * will ignore EOF  
		* repeating search by press Esc + n|N, n forward, N backward
		* execute shell command in less by !, and type exit to return less, use % to reference the current file
		du -h %
		will display the size of current file
- creating links between files with ln, allows you create links between files that look and work like normal files, two type links
	+ hard link
	each filename on your system points to what is known as an inode, which is the absolute location of a file, linux allows you to point more than on filename to a give inode and the result is hard link, each of these files shares the same content and attributes
	used to create backup
		* create hard links
		$ ln myfile.txt mylink
	+ symbolic links, called a soft link
	is a redirect to the real file, symlinks are really just dumb pointers you can link to something that does not exist(and created it later if you want)
	it's popular because the allow a file to appear to be in a different location, you could stare your website in /var/www/liv and under-construction holding page in /var/www/construction, then you could have Apache point to a symlink /var/www/html that is redirected to either the live or construction directory, depend on what you need
	it's also allowed multiple version of library exist on the system without worry of conflict
		* create symbolic link
		$ ln -s myfile.txt mylink
		* use ls to check
		$ ls -l
		lrwxrwxrwx 1 matthew matthew 5 Feb 19 12:39 mylink -> myfile.txt
		the leading l(lowercase of L) is represent for link
	+ TIP to delete file
	shred command overwrites a file's contents with random data, allowing for safe deletion.
- finding file from an index with locate, which is different with find command that is search recursively through each directory, but locate is only search file name or 
	+ example
	$ locate myfile.txt
	$ locate -i myfile.txt
	+ use updatedb command to update the index database
	+ this database is built daily at 6:25 a.m. by default, does not contain pathnames to files created during the working day
	change the cron job that calls the command. there is a script in /etc/cron.daily called mlocate that does this as part of the daily maintenance for the database, you could remove it or change it to config the cron job
- listing file in the current directory with ls, some wide used parameters
	+ parameters, -a include hidden files, -h use human readable sizes, -l(lowercase L) enables long listing, -r reverse order, -R recursively lists directories, --sort sorts the listing, -s show sizes
	+ support wildcard characater
	+ if a user have execute access but read access he could only access a directory but can't list it's content
	+ the directory ls number is the number of subdirectories(including . and ..)
	drwxrwxr-x 24 matthew matthew 4096 Dec 24 21:33 arch
	+ for the file it's the number of hard links
	+ support sort by various things, most popular are extension, size, and time
	$ ls --sort size -r *.ogg
- read manual pages with man, two other commands work colosely with man which are whatis and apropos
whatis command return a one-line only description of another command, which is the same text you get from that command's man page
apropos command takes a search string as its parameter and returns all man pages that match the search
so use apropos to help you find commands and use whatis to tell you what a command does
	+ example
	$ apropos mixer
	alsamixer (1) - soundcard mixer for ALSA soundcard driver
	...
	whatis alsamixer
	+ by default man use less command open the manual page
	+ other commands to search the file
		* whereis, returns the location of the command
		* whatis, return a one-line synopsis from the command's man page
		* locate, return locations of all match fines
	+ type name, returns how a name would be interpreted if used as a command, example type ls is aliased to 'ls -color=auto'
- making directories with mkdir, one parameter should be aware of, -p, used to create a directory in another directory which is not exist
- moving files with mv, helpful parameters to mv, -f which overwrites files without asking and -u which moves the source file only if it is newer than the destination file
- listing processes with ps
	+ ps use BSD-style parameters, the used single letters without a dash
	+ list all your process attached to any terminal by adding the x parameter
	$ ps x
	+ list all process for all users with a parameter 
	$ ps a
	+ use a combine with x
	$ ps ax
	+ use u option to enables user-oriented output
	+ use f parameter create a process forest by using ASCII art to connect parent commands with their chidren
	+ combine all the above options
	$ ps aufx
	+ control the order by --sort parameter, takes either a+ or a- (+ is default) followed by the field you want to sort by: command, %cpu, pid and user are all popular options
	$ ps aux --sort=-%cup
	lists all processes ordered by CPU usage desending
- deleting files and directories with rm, one parameter of interest --preserve-root, issuing rm -rf / with sudo will destroy you linux, -r recursive and -f means force( not confirmation), it is possible for a clumsy person to issue this command by accident by putting a space in the wrong place
	+ with --preserve-root will prevent this happen, add it to .bashrc file in your /home directory will prevent type this option each time use with rm
	alias rm='rm --preserve-root'
	+ ubuntu and debian have set this be default
- printing the last lines of a file with tail which prints the last few lines of a file and updates as new lines are added
$ tail /var/log/httpd/access_log
to get tail remain running and update as the file changes, add the -f parameter(follow)
$ tail -f /var/log/httpd/access_log
you can tie the lifespan of a tial follow to the existence of a process by specifying the --pid parameter, tail continues to follow the file until the process is no longer running
support input multiple file and use ctrl+c to terminate tail follow mode
- print the resource usage with top, open two terminal windows, in the first run program yes and leave it running, the second run top
default sort order most CPU-intensive tasks
there are lots of command, first filter out other users and focus on the user running yes. press u and enter the username
next kill the PID of the yes
	+ the relative fields
	PID, process id
	User, owner of the process
	PR, priority
	NI, Niceness
	Virt, virtual image size in kilobytes
	Res, Resident size in kilobytes
	Shr, Shared memory size in kilobytes
	S, Status
	%CPU, CPU usage
	%Mem, memory usage
	Time+, CPU time
	Command, the command being run
	+ polite to kill a process, type k enter that PID and press enter, prompt a signal number(the manner in which you want the process killed), with 15 provide as the default, Signal 15(also know as SIGTERM for terminate) is polite way of asking a process to shutdown
	+ force to kill a process, type k enter the PID and press enter, type 9 for the signal send SIGKILL, 'terminate whether you like it or not'
	+ choose the fields to display by pressing f, selected fields are marked with an asterisk and have their letter as follow
	* A:PID = Process Id
	+ press f to select the field you want to use for sorting
	+ press B, enable the text bolding, use y to toggle bolding of running process
	+ press r to renice or adjust the nice value of a process, 19 is the lowest and -20 is the highest, anything less than 0 is considered high and should be used sparingly
- printing the location of a commnad with which
- redirect Output and Input, sometimes the output is too long to view with cat or less, use (>) sometimes people read this as 'into'
$ cat /proc/cpuinfo > file.txt
before redirect the output make sure the output file is not exist
	+ example, use ubuntu software packageing system apt's stable command dpkg, list all software has been installed using apt
	$ sudo dpkg --get--selections > pkg.list
	use this list to record the installed software and on another system use it as input to install the same software environment, tells dpkg to mark for installation in the list that are not already installed. then use apt-get to get and dselect the software
	$ sudo dpkg --set-selections < pkg.list
	$ sudo apt-get -u dselect -upgrade
- combining commands, with two ways, pip and redirect
	+ pip
		* check current user who is playing nethack
		$ ps aux | grep nethack
		filters out all lines that do not contain the word nethack
		ubuntu support pip as many commands as you can sanely string together
		wc command, count total line numbers with -l will prints only the line count
		* instead use with -exec with find to pip the output and use xargs command to extract the output into another command
		$ find / -name "*.txt" -size +10k -user matthew -not -perm +o=r -exec chmod o+r {} \;
		it is same with 
		$ find / -name "*.txt" -size +10k -user matthew -not -perm +o=r | xargs chmod o+r
		which eliminate the confusing {} \; from the end of the command and does the samething and faster too
		* xargs command automatically places the input at the end of the line just like execute
		$ xargs chmod o+r file1.txt file2.txt ...
		with -l parameter xargs executes its command once for each line in its input, with -p to check what the exact command is executed, with -i to control which matching line to be placed
		$ find /home/matthew -size +10000k | xargs -i cp {} ./home/matthew/archive
		* using find with xargs is a unique case. people use pips when aparmeters would do the job just as well
		$ ps aux --sort=-%cpu | grep -v 'whoami'
		$ ps -N ux --sort=-%cpu
		the two commands have the same result. but the first is better understanding with a little poor efficiency
- using environment variables
	+ environment variables
	PWD, provides current working directory
	USER, user's name
	LANG, default language
	SHELL, the name and location of the current shell such as /bin/bash.
	PATH, Sets the default locations of executable files, such as /bin, /usr/bin and so on
	TERM, type of terminal in use, such as vt100, which can be important when using screen-oriented programs such as text editors
	+ display the variable
	$ echo $USER
	+ display all the environment variable, env or printenv
	these variables are set by configuration or resource files contain in /etc, /etc/skel or in the user's /home directory, example find /etc/profile
	check the manual for the detail fo the configuration files
	.bash_profile in your /home directory to add path into PATH variable
	PATH=$PATH:$HOME/bin:/sbin
	+ change the sell prompt, use built-in export command to change the environment variable
	export $PS1='$OSTYPE rz00lz'
- using common text editors, console based editors
	+ emacs
	+ nano, simple text editor to the classic pico text editor included with once-common pine email program
	+ vim, an imporved compatible version of the vi text editor
	+ graphic desktop editor
	gedit GNU text editor installed by default with ubuntu, kate KDE editor, kedit another simple KDE text editor
	+ remote desktop use a textbased editor
	+ nano and vi almost universall installed
- working with nano	  
	+ open file 
	nano file.txt
	+ commands
	cursor movement, arrow keys, ^y and ^v page up and page down
	exit, ^x
	get help, ^g
- working with vim, vi start in the viewing mode, press i to start editing, use Esc key to toggle out of the insert or append modes and into the viewing mode(or command mode). To enter a command type a colon(:) follow by the command such as w to write the file and press enter
	+ basic vi commands
	Cursor movement, h,j,k,l
	Delete caracter, x
	Delete line, dd
	Mode toggle, Esc, Insert (or i)
	Quit, :q
	Quit without saving :q!
	Run a shell command, :sh (use 'exit' to return)
	Save file, :w
	Text search, /
	+ use vimtutor command to quickly learn how to use vi's keybord commands
	+ end editing by :wq
- working with emacs, Richard M.Stallman's GNU emacs editor like vi, is included with ubuntu and early every other linux distribution, contains a built-in language interpreter uses the Elisp(emacs LISP) programming language
	+ start emacs in terminal 
	$ emacs -nw file.txt
	+ emacs editor uses an extensive set of keystroke and named commands 
	abort			   ctrl+g
	cursor left		 ctrl+b
	cursor right		ctrl+n
	cursor up		   ctrl+p
	delete char		 ctrl+d
	delete line		 ctrl+k
	go to start of line ctrl+a
	go to end of line   ctrl+e
	help				ctrl+h
	quit				ctrl+x, ctrl+c
	save as			 ctrl+x, ctrl+w
	save file		   ctrl+x, ctrl+s
	search backward	 ctrl+r
	search forward	  ctrl+s
	start tutorial	  ctrl+h, t
	undo				ctrl+x, u
	+ emacs nearly universal installed on all linux distribution call nearly use same keystrokes to edit commands on the bash shell command line, even mac os
- working with compressed files
	+ linux include compress relative commands
	bunzip2, expands a compressed file
	bzip2, compress or expands files and directories
	gunzip, expand a compressed file
	gzip, compress or expands files and directories
	tar, creates, expands or lists the content of compressed or uncompressed file or directory archives known as tape archives or tarballs
	+ create a compressed archive of a directory use tar's czf options
	$ tar czf compressedfilename.tgz directoryname
	+ add the letter v to display compressioning info
	+ list content use c option and the letter t
	$ tar tzf archive
	+ expand the contents of a compressed archive
	$ tar zxf archive
- using multiple terminals with byobu, many linux used screen command to use one terminal to control several terminal sessions, a bettern one is byobu, byobu is a Japanese term for decorative, multipanel vertically folding screens
these allowed run multiple terminals inside on terminal
	+ example
	run byobu, run top and press F2 run the uptime command, press F3 to return the previous run top terminal, type F4 to go back to your uptime terminal
	+ basic commands
	F2, create a new window
	F3, go ot the previous
	F4, goto the next window
	F9, open the byobu menu for help and configuration
	using exit or Ctrl+d to end a terminal
	F12, locks access to your screen data until you enter your system password
	F6, disconnect, when byobu disconnect all the running program are keep running, someone kill byobu in a locked state(with Ctrl+a+x) it will auto disconnect for you. even close the terminal start the byobu(or screen) which will not effect the running process
	Reconnect by screen/byobu -r
- References
www.gnu.org, the website of the GNU project
http://oreilly.com/linux/command-directory, a widw selection of linux commands and explanations of what they do take from O'Reilly's excellent book Linux in a Nutshell
www.tuxfiles.org/linuxhelp/cli.html, several short command-line tutorials
www.linuxcommand.org, describes itself as 'your one-stop command-line shop'
Best books, The Art of Unix Programming, (ISBN: 0-13-142901-9), which focuses on the philosophy behind UNIX and manages to mix in much about the command line
www.vim.org
www.gnu.org/software/emacs/emacs.html
www.nano-editor.org
O'Reilly's The UNIX CD Bookshelf (ISBN:0-596-00392-7)


# Managing Users
- user management include
manage /home directories, putting in place good password policies and applying an effective security policy include such as disk quotas and file and directory access permissions
- user accounts, ubuntu uses the /etc/passwd file to store information on the user accounts that are present on the system
encrypted field for the password(which contains an X to denote that a password is present) and a group ID(commonly referred to the GID)
The last two fields show the location of the /home directory(usually /home/username) and the default shell for the user(/bin/bash is the default to new users)
A field called called GECOS that uses a comma-delimited list to record information about the account or the user
all password are stored in /etc/shadow in an encrypted format for safekeeping
everything in system(UNIX-style operating systems) is treated as a file, and all files(which can include directories and devices) can be assigned one or more of read, write and execute permissions, these three categories can also be assigned as desired to each of three categories, the owner of the file, a member of a group or any one else on the system
- super user and root user
	+ there is one super user account, commonly referred to root
	+ root user is unique in that has a UID of 0 and GID of 0
	+ execute command with root/super user privileges using the sudo command
	+ root user 
		* ubuntu will set root privilege to the first login user and disable the root user
		get root prompt in ubuntu with sudo -i
		* other linux could use command su -nd and then entering the root password, the prompt show as (#)
		* use exit to quit root mode
	+ control other user profile by ~/.profile, and set the directory
- user IDs and Group IDs, computer is number-oriented machine, user name is a ease of use. root is UID 0, numbers from 1 through 499 and number 65534 are the system, sometimes called logical or pseudo-user. Regular users have UIDs beginning with 1000; 
with only a few exceptions the GID is the same as the UID
unbuntu creates a private GID for every UID of 1000 and greater. The system admin can add other users to a GID or create a totally new group, a group can't be a member of another group in ubuntu(which different with windows NT and some UNIX variants)
- file permissions, three types, read, write and execute(r, w, x)
	+ relative commands
	chgrp, changes the group ownership of a file
	chown, changes the owner of a file or directory
	chmod, changes the access permissions of a file or directory
	+ user stereotypes, understanding these stereotypes allows you to better define the appropriate and inapropriate roles of system adminitrators
	http://en.wikipedia.org/wiki/BOFH
	http://en.wikipedia.org/wiki/Luser
- managing groups
	+ group listing, ubuntu use a scheme called UPG(user private group) where, by default, all users are assigned to a group with their own name
	all group are list in /etc/group file
	system services groups allow those services to have ownership and control of their files
	+ group management tools
	groupadd
	groupdel
	groupmod, create a group name or GIDs but donesn't add or delete members from a group
	useradd -G, the -G parameter adds a user to a group during the initial user creation
	usermod -G, add add a user to a group so long as the user is not logged in at the time
	grpck, check the /etc/group file for typos
	gpasswd -A username, assign the group password to a user
	+ example sys add let a regular user ryan to have permission to access a dvd-rw device(/dev/scd0) the process to grant ryan the access
		* add a new group with groupadd command
		$ sudo groupadd dvdrw
		* change the group ownership of the device to the new group with chgrp
		$ sudo chgrp dvdrw /dev/scd0
		* add the approved user to the group with the usermod command
		$ sudo usermod -G dvdrw ryan
		* make user ryan the group administrator with the gpasswd command so that he can add new users to the group
		$ sudo gpasswd -A ryan
	+ use the system menu to access the graphic group manage interface
- managing users, users must be created assigned a UID, provided a /home directory, initial set of files, assigned to groups
	+ user management tools
	useradd
	useradd -D, sets the system default for creating the user's /home directory, the default set of files for a user are found in /etc/skel, the useradd command copy all the files to the new home directory and reset the file owner from root to the new user
	userdel, complete remove a user's account
	passwd, updates the authentication tokens by the password management system
	usermod, changes several user attributes -s to change sell and -u to change the UID, no change can be made while the user is logged or running a process
	chsh, change the user's default shell
	+ example lock a user out of his account
	$ sudo passwd -l username
	this command prepends a !(exclamation point or called bang) to the user's encrypted password
	reverse the process by use -u option
	+ adding new users (variant found on some UNIX system is adduser which is a symbolic link to useradd)
	$ sudo useradd heather -p password_ -s /bin/zsh -u 1042
	p is the password, s is the default shell, u is the UID
- monitoring user activity on the system 
	+ w command tell the system admin who is logged in
	+ ac command provides information about the total connect time of a user measured in hours to use the ac command need to install the acct package, it accesses the /var/log/wtmp file
	+ lastb command is useful for determining whether a legitimate user is having trouble or a hacker is attempting access
	+ time warp can occur when an entry in the wtmp file jumps back into the past and ac shows unusual amounts of time accounted for users. This may a security breach need to be checked
	+ check depths of the accounting system use the GNU info system, info accounting
- managing password
	+ system password policy
		* allowed and forbidden passwords
		* frequency of mandated password changes
		* retrieval or replacement of lost or forgotten password
		* password handling by users
	+ password file, /etc/passwd is the database formate is 
	username:password:uid:gid:gecos:homedir:shell
	gecos field is for miscellaneous information about the user such as the full name offcie location... this fied is little used today by system administrator should aware of its existence because it is used by traditional UNIX programs such as finger and mail
	if a colon separates all fields if no information for a field the field is empty but the colon remain
	if a asterisk appears in the password field that the user will not be permitted to log on
	several service run as pesudo-users with root permission, for security reasons they are assigned /sbin/nologin or /bin/false as their shell, which prohibits any logins from these account
	+ shadow password
	the real password is store in /etc/shadow a file that can only be read by the system administrator (and PAM, the pluggable authentication modules authentication manager) and keep field of password in the /etc/passwd is X
	ubuntu automatic enable shadow password feature
		* shadow file format
			root:!:14547:0:99999:7:::
		user's login name
		the encrypted password for the user
		day of which the last password change occurred, measure in the number of days since January 1, 1970. It's UNIX circles as the epoch. 
		the number of days before the password can be changed
		the number of days after which the password must be changed
		the number of days before the password expiration
		the number of days after the password expiration
		similar to the password change date
		the final field is a 'reserved' field and is not currently allocated for any use
		* the permission for /etc/shaow should be 600
	+ PAM explained, pluggable authentication modules(PAM) is a system of libraries that handle the tasks of authentication
		* configuration file of PAM in ubuntu in /etc/pam.d these files are named for the service they control the format is as follows
		type control module-path module-arguments
		backup the configuration file before change it
		* PAM is installed in ubuntu by default
		* PAM sys admin's guid 
		http://www.kernel.org/pub/linux/libs/pam/Linux-PAM-html/Linux-PAM_SAG.html
- Managing Password Security for Users
	+ configure the how often the password should be changed, setting by super user using text editor or the chage command
	+ changing passwords in a batch, with chpasswd command which accepts input as a name/password pair line in the following form
	$ sudo chpasswd username:password
- Granting system administrator privileges to Regular users
	+ temporarily changing user identity with su command, su command is abbreviation of substitute user 
	use su to become super user root need to type absolute path to the command such as in /bin or /sbin,  because you don't inherite the path variable of root. 
	Ubuntu disable the root account by default and need to enable it by type sudo password from the first created user to change his password. after that the root account is active
		* su command
		$ su option username arguments
		with -c COMMAND to pass a single command to the shell 
		with -m do not reset environment variables
		with -l a full login simulation for the substitute user
		* use su alone will keep your regular user environment, check with printenv
		* use su -, becom root but inherit the super user's environment
		* use su - other_user, to become other user
		* use exit command to return to your regular user
	please check the reason http://help.ubuntu.com/community/RootSudo
	+ communication port /dev/ttyS0 when using a modem
	+ sound device /dev/audio 
	+ granting root privileges on Occasion, the sudo command
	man pages are associated with sudo, sudoers, and visudo, the associated file is /etc/sudoers file
		* content of sudoers file
		# Members of the admin group may gain root privileges
		%admin ALL=(ALL) ALL
		the % in front identifies a name as a group
		* basic format of sudoers line
		user host_computer=command
		* example
		%wheel ALL=(ALL) NOPASSWD: ALL
		any user we add to the wheel group can execute any command without a password
		
		john ALL=/users-admin
		john permission across the network to be able to add users with the graphic interface
		
		john 192.168.1.87=/usr/bin/users-admin
		grant permission only on her local computer
		
		ALL localhost=NOPASSWD:/sbin/mount /dev/scd0 /mnt/cdrom /sbin/umount /mnt/cdrom
		give every user permission with no password required to mount the CD
		* sudoers file also support wildcard compare, aliases can be used to. large company configure notes at
		http://www.komar.org/pres/sudo/toc.html
		* sudo home page, also contain useful resources
		http://www.sudo.ws/ 
		check the command list by 
		$ sudo -l
- disk quotas, need the quota and quotatool packages installed on your system
	+ use quota commands such as quotacheck to initialize the quota database files
	+ edquota to set and edit user quotas
	+ setquota to configure disk quotaas and quotaon or quotaoff
	+ warnquota for auotmatically sending mail to users over their disk space usage limit
	+ implementing quotas, check the /etc/fstab file to see the quotas
		* example quotas are enabled for the /home partition
		/dev/hda5 /home ext3 defaults, usrquota, grpquota 1 1
		* the root partition with quotas enabled will have files quota.user or quota.group in them
		* initalize disk quotas the partitions must be remounted
		$ sudo mount -o ro,remount partition_to_be_remounted mount_point
	+ console tools (complete with man pages) 
	quotaon, quotaoff
	repquota, a summary status report on users and groups
	quotacheck, updates the status of quotas(compares new and old tables of disk usage), run after fsck
	edquota, basic quota management command
	+ manually configuring quotas, invovles changing entries in your system's /etc/fstab file
		* simple file system quota management can be enabled like
		LABEL=/ / ext3 defaults,usrquota 1 1
		* group-level quotas can also be enabled by using the grpquota option
		$ sudo touch /quota.user
		as the root operator you must then create a file named quota.user in the designed portion of the file system, like so
		$ sudo touch /quota.user
		then turn on the use of quotas using the quotaon command
		$ sudo quotaon -av
		use edquota to set hard and soft limits on file system use
		check the quota using by 
		quota -v
	+ reference manual, quota mini-HOWTO
	http://www.tldp.org/HOWTO/Quota.html
- related ubuntu commands
ac		   user account-statistics
change	   sets or modifies user password expiration policies
chfn		 creates or modifies user finger information in /etc/passwd
chgrp		modifies group memberships
chmod		changes file permissions
chpasswd	 batch command to modify user passwords
chsh		 modifies a user's shell
groups	   displays existing group memberships
logname	  displays a user's login name
newusers	 batch user management command
passwd	   creates or modifies user passwords
su		   executes shell or command as another user
sudo		 manages selected user execution permissions
useradd	  creates, modifies or manages users
usermod	  edit a user's login profile
- references
http://tldp.org, check ubuntu management article
http://tldp.org/LDP/sag/html/index.html, a general guide linux system admin security guide
http://tldp.org/HOWTO/Security-HOWTO/
http://kernel.org
http://www.gnu.org/software/finger/manual/html_mono/finger.html#SEC1, gnu finger which use to check if the user is login and support display detail information. such as display MIT login persons


# Automating tasks and shell scripting
- introduce bash, one of the most popular shell in linux default in ubuntu
- scheduling tasks, there are three ways
	+ the at command, which specifies a command to run at a specific time, fit for you don't want to keep login and want system to run at specify time
		* type at command the there will be a new promote, everthing you type here until you type ctrl+d will be the commands you want to run
		* example, simple commands
		$ at now + 7 hours
		at> wget http://www.kernel.org/pub/linux/kernel/v3.0/linux-3.0.tar.bz2
		at> tar xvfjp linux-3.0.tar.bz2
		at> <EOT>
		* example, read commands from a file
		at -f myjob.job tomorrow
		* parameters
		special times, including tomorrow, midnight, noon, or teatime(4p.m.), without time the job will start precisely 24 hours from the current time
		using the now parameter, you can specify minutes, hours, days or weeks relative to the current time
		specify an exact date and time using HH:MM MM/DD/YY format( ex, 16:40 22/12/12 for 4:40 p.m. on the 22nd of December 2012)
		the job also captures all your environment variables and stores them along with the job, will be queue in the 'a' by default
	+ batch command, actually a script that redirects you to the at command with extra options, these options(-q b -m now), queue b(-q b), mailing the user on completion(-m), and running immediately, queue b will only execute when the system laod falls below 0.8, with lower niceness (4) compare to queue a (2)
	the load is only check before the job. There are also other queue such as c (8), d (16)
	+ cron daemon, which is the linux way of executing tasks at a given time, to run jobs repeatedly, cron have a similar permission list as at which is cron.allow and cron.deny, there are two type of jobs
		* system job, controlled through the /etc/crontab file, default looks like
		SHELL=/bin/sh
		PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
		# minute hour day-of-month month day-of-week user command
		# m h dom mon dow user command
		17 * * * * root cd / && run-parts --report /etc/cron.hourly
		first two lines specify which shell should be used to execute the job(usually /bin/bash), It's important that you avoid using environment variables in this path statement because they may not be set when the job runs
		start with # means comment line
		the * means "all values" (every minute , everyhour, ...)
		the first job runs at minute 17, every hour of every day of every month and executes the command run-parts /etc/cron.hourly. The run-parts command is a simple script that runs all programs inside a given directory
		* CAUTION
		cron daemon reads all the system crontab files and all user crontab files once a minute to check for changes. delete or insert a job of the next minute will not influence the cron job
		* alternative ways of specifying dates by using hyphens or commas
		9-15, */4 for every 4, 0-12/3 means 0 to 12 each 3
		three-character abbreviations for day and month such as Sun, Mon, Tue, Fri, Sat for days, or Jan, Feb, Mar, Oct, Nov, Dec for months
		* user job, for those have the correct permissions. stored in /var/spool/cron/philip or /var/spool/cron/root, the contents contain the jobs the user wants to run, the format is same with the system job without the job owner
		* edit the your own crontab file type crontab -e
	+ use command atq command to check job list
	+ delete a job by atrm command follow by the id
	+ access control for at and batch command at /etc/at.allow and /etc/at.deny, type user name into at.deny one per line to deny user scheduling jobs
	at.allow which not exist by default, if a blank at.allow file means only the root could schedule jobs
- basic shell control
	+ shells with ubuntu
	bash		the Bourne Again SHell	  /bin/bash
	ksh		 the Korn shell			  /bin/ksh, /usr/bin/ksh
	pdksh	   A symbolic link to ksh	  /usr/bin/pdksh
	rsh		 A restricted shell(for network operation) /usr/bin/rsh
	sh		  A symbolic link to bash	 /bin/sh
	tcsh		A csh-compatible shell	  /bin/tcsh
	zsh		 A compatible csh, ksh and sh shell  /bin/zsh
	+ learn more about shell
	/usr/share/doc, bash have 100 pages, zshall use man zshall to read
- shell command line, perform tasks
	+ searching files
	+ getting data and sending data
	+ feeding or filtering a program's output to another
	+ have built-in job-control commands to launch the command line as a background process, suspend a running program, selectively retrieve or kill running or suspended programs
	+ long shell command lines can be extended inside shell scripts or at the command line by using the backslash
- Grokking grep, book name Mastering Regular Expressions by Jeffrey E.F.Freidl(O'Reilly, ISBN: 0-596-52812-4)
- shell pattern matching support, supported by GNU utilities such as grep is used to serach through files or directories or to filter data input to or out of commands, shell pattern matching include 
	+ * matches any
	+ ? matches a single
	+ [xxx] or [x-x] match a range
	+ \x matches or escape a character
	+ $ will have two wildly different meaning(single character patterm matching in expressions and variable assignment in scripts)
- redirecting input and output, such as >, <, << or >>
- piping data, |
- background processing, shell allows you to start a command and then launch it into the background as a process by using an ampersand(&) at the end of a command line
	+ example launch another terminal window 
	$ xterm &
	+ kill the job or process
	$ kill %job_id
	$ kill PID
- writing and executing a shell script
	+ such as startx command used to start an X window session
	+ reference, book Advanced Bash-Scripting Guide, Sams Teach Yourself Shell Programming in 24 Hours
	+ use vim to edit shell script which is disable line wrap by default, use -w flag for nano to disable line wrap
	+ write aliases(command synonyms) and execute it when ever you login, these configuration files save under /etc for systemwide shell, save in your home directories with .bashrc, .cshrc, or .bash_profile files in your home directory
		* example, create a text file (myenv) and save it, contents
		#!/bin/sh
		alias ll='ls -L'
		alias ldir='ls -aF'
		alias copy='cp'
		
		then use the chmod command change the file to executable
		$ chmod +x myenv
	+ running the new shell file
		* from command line
		$ ./myenv
		* under a particular shell such as pdksh myenv
		$ pdksh myenv
		* create a directory named bin in your home directory, then run the program without the need to specify a specific location or to use a shell
		$ mkdir bin
		$ mv myenv bin
		$ myenv
		this because ubuntu set up by default to include the executable path $HOME/bin in your shell's environment
		* check environment variable by 
		$ env | fgrep PATH
		$ echo $PATH
		* CAUTION
		never put . in your $PATH to execute files, this is a security risk
- storing shell scripts for systemwide Access
	+ make the script myenv to all user
	putting them in the /etc/bashrc file
	systemwide aliases for tcsh are contained in files with extension .csh under the /etc/profile.d directory
	+ NOTE
	to use a shell other than bash by chsh command or the system-config-users client during an X session. the new shell become default but only it is in the /etc/shells system acceptable shell list
- interpreting shell scripts through specific shells
	+ majority of shell scripts use a shebang line (#!) at the beginning to control the type of shell used to run the script
	#! /bin/sh  calls for an sh-incantation of bash
	shebang(it is short for 'sharp' and 'bang' # and !) to tell the kernel that a special command is to be used to interpret the contents
	+ shebang line, is a magic number as defined in /usr/share/magic a text base of magic numbers for linux file command. Magic numbers are used to quickly identify a type of file, the databse format is documented in the section five manual page named magic (read by using man 5 magic)
	+ wish command is a windowing-tool-control language(tcl) interpreter, that can be used as graphic clients
	#! usr/local/bin/wish
- using variable in shell scripts, there are three types
	+ environment variable
	+ built-in variable
	+ user variable
	+ shell programming variable are not typed
	+ assigning a value to a variable
		* number
		lcount=0		pdksh and bash
		set lcount=0	tcsh
		make sure there is not space between sign (=)
		* string
		myname=sedona
		myname="sedona Ly"
		* access variable
		lcount=$var
	+ positional parameters, the first parameter is stored in a variable called 1 access by using $1, the positional variable may be omitted
	+ simple example of positional parameter
	#!/bin/sh
	#Name display program
	if [$# -eq 0]
	then
		echo "Name not provided"
	else
		echo "Your name is "$1
	fi
	the built-in parameter $# provides the number of positional parameters passed to the shell program
	+ using positional parameters to access and retrieve variables from the command line
	#!/bin/sh
	# play voice message in /var/spool/voice/incoming
	rmdtopvf /var/spool/voice/incoming/$1 | pvfspeed -s 8000 | \
	pvftobasic > /dev/audio
	
	a void message can be easily played back by 
	$ pmm name_of_message
	+ simple script to automatic task
	#!/bin/sh
	# name: greplog
	# use: mail grep of designated log using keyword
	# version: v.01 08aug02
	#
	# author: bb
	#
	# usage: greplog [keyword] [logpathname]
	#
	# bugs: does not check for correct number of arguments
	# build report name using keyword search and date
	log_report=/tmp/$1.logreport.`date ‘+%m%d%y’`
	# build report header with system type, hostname, date and time
	echo “==============================================================” \
	>$log_report
	echo “ S Y S T E M M O N I T O R L O G” >>$log_report
	echo uname -a >>$log_report
	echo “Log report for” `hostname -f` “on” `date ‘+%c’` >>$log_report
	echo “==============================================================” \
	>>$log_report ; echo ““ >>$log_report
	# record log search start
	echo “Search for->” $1 “starting” `date ‘+%r’` >>$log_report
	echo ““ >>$log_report
	# get and save grep results of keyword ($1) from logfile ($2)
	grep -i $1 $2 >>$log_report
	# build report footer with time
	echo ““ >>$log_report
	echo “End of” $log_report at `date ‘+%r’` >>$log_report
	# mail report to root
	mail -s “Log Analysis for $1” root <$log_report
	# clean up and remove report
	rm $log_report
	exit 0
	
	run the script
	$ greplog FAILED /var/log/messages
	
	Note that your system should be running thesyslogddaemon. If any login failures have
	occurred on your system, the root operator might get an email message that looks like this:
	Date: Thu, 23 Oct 2003 16:23:24 -0400
	From: root <root@stinkpad.home.org>
	To: root@stinkpad.home.org
	Subject: FAILED
	+ built-in variables
	$#, number of positional parameters
	$?, completion code of the last command or shell program executed within the shell program
	$0, the name of the shell program
	$*, a single string of all arguments passed at the time of invocation of the shell program
- special characters, special for linux shell
$   beginning of a shell variable name
|   pipes standard output to next command
#   starts a comment
&   executes a process in the background
?   matches on character
*   match any
>   output redirection
<   input redirection
`   command substitution (the backquote or backtick)
>>  output redirection
<<  wait until following end-of-input string(HERE operator)
[]  range of characters
[a-z]   all a through z
[a,z] or [az]   character a or z
space   delimiter between two words
\   escape character or backslash
- using double quotes(") to resolve variables in strings with embedded spaces
- using single quotes(') to maintain unexpanded variables, surround a string with single quotes to stop the shell from expanding variables and interpreting special characters
var='test'
new='value of var is $var'
dnew="value of var is $var"
the new will be output value of var is $var
the dnew will be value of var is test
- using backslash as an escape character
\$variable_name is same as '$variable_name'
- using the Backtick (`) to replace a string with output, this is called command substitution.
	+ example count lines in a text file and save it in the variable named var
	var=`wc -l test.txt`
- comparison of expressions in pdksh and bash
	+ string comparison
	=   equal
	!=  not equal
	-n  whether the string length is greater than zero
	-z  whether the string length is equal to zero
	+ number comparison
	-eq equal
	-ge greater than or equal 
	-le less than or equal
	-ne not equal
	-gt greater than
	-lt less than
	+ file opertators
	-d  ascertain whether a file is a directory
	-f  ascertain whether is a regular file
	-r  test read permission for a file
	-s  test file exists and has a lenght greater than zero
	-w  test write permission
	-x  test execute permission
	+ example to test file permission
	if [-f $dir1]; then
		echo "dir1 is a regular file"
	else
		echo "dir1 is not a regular file"\
	fi
		
	if [ -r $file1]; then
		echo "file1" has read permission
	else
		echo "file1 does not have read permission"
	fi
	+ logical operators
	!   negate a logical expression
	-a  logically AND
	-o  logically OR
	+ comparing expression with tcsh, it's different with bash. The operator more likely seems to C
		* file operators
		-d  is directory
		-e  file is exists
		-f  if ile
		-o  is owner of a file
		-r  is read permission
		-w  is write permission
		-x  is execute permission
		-z  is size zero
	+ for statement pdksh and bash is follows
	for curvar in list
	do
		statements
	done
	+ for statement in tcsh
	foreach curvar (list)
		statements
	end
	
	+ while statement
		* endless loop
		#!bin/sh
		while :
			do
				/sbin/iwconfig eth0 | grep Link | tr '\n' '\r'
			Done
			
		* create a graphical monitoring client for X that outputs traffic information
		#!/bin/sh
		xterm -geometry 75x2 -e \
		bash	-c\
			"while :; do\
				/sbin/ifconfig etch0 | \
				grep 'TX bytes' |
				  tr '\n' '\r'; \
			done"
			
		* normal while
		while [ $var -lt 5]
		do
			statement
		done
			
		* linux include a command that will repeatedly execute a given command line
		$ watch "sensors -f | cut -c 1-20"
		quick report about a system's hardware health by using the sensors command
	+ the until statement, can be used to execute a series of commands until a specified condition is true
	until expression
	do
		statements
	done
	
	+ repeat statement(tcsh)
	
	+ select statement(pdksh), is used to generate a menu list if you are writing a shell program
	select item in item list
	do
		statement
	done
	+ shift statement, used to process the positional parameters, one at a time from left to right, effect of shift statement is that each positional parameter is moved one position to the left and the current $1 parameter is lost
	shift number
	+ if statement
		* bash
		if [expression]; then
			statements
		elif [expression]; then
			statements
		else
			statements
		fi
		* tcsh
		if (expression) then
			statements
		else if (expression) then
			statements
		else
			statements
		endif
	+ the case statments
		* bash and pdksh
		case str in 
			str1 | str2)
				statements;;
			str3 | str4)
				statements;;
			*)
				statements;;
		esac
		* example
		case $1 in
			01|1) echo "Month is January";;
		esac
		* tcsh 
		switch (str)
			case str1|str2:
				statements
				breaksw
			default:
				breaksw
		endsw
	+ break and the exit statements, break is used to break iteration loop, exit can be used to exit a shell program
	+ functions in shell
	func(){
		statements
	}
	call a function by 
	func param1 param2 param3
- references
http://www.gnu.org/software/bash/bash.html
http://www.tldp.org/LDP/abs/html/
http://www.freeos.com/guides/lsst/, linux shell scripting tutorial
http://web.cs.mun.ca/~michael/pdksh/, the pdksh home page
http://www.tcsh.org/, find out more about tcsh
http://www.zsh.org/, examine zsh in more detail here
   
	
# Boot process
- steps
power on-> executing BIOS-> boot loader starts loading the linux kernel-> login
- running services at boot, ubuntu and linux in general are a number of states in between on or off, know as runlevels define what system services are started upon boot. these services are simply applications running in the background that provide some needed function to your system
- beginning the boot loading process
	+ BIOS is an application stored in a chip on the motherboard BIOS get system ready to load and run the software that we recognize as operating system
	
	+ BIOS will looks for a special program known as the boot loader or boot code, then the loader tell BIOS where the linux kernel is located
	
	+ BIOS looks for a bootable volume such as a floopy disk, cd-rom, hard drive, ram disk, usb drive, or other media. The bootable volumn contains a special hexadecimail value written to the volumn by the boot loader(such as ubuntu's default GRUB2 since ubuntu 9.10 or GRUB in older release, or even LILO). The BIOS searches volumns in the order eastablished by the BIOS settings. Modern BIOSs allow considerable flexibility in choosing the device used for booting
	
	+ BIOS looks for boot code in the partition (Master boot record, MBR) of the first hard disk.  the MBR contains the boot loader codde and the partition table, after BIOS load the boot loader, the BIOSs job is complete, then pass control of the system to boot loader
	
	+ linux can be boot in several ways
		* cd
		* network using PXE(pronouced "pixie") or NetBoot
		* headless server with the console 
		* create a special linux BIOS at www.coreboot.org, that will expendite the boot process because linux does not need many of the services offered by the typical BIOS
- loading the linux kernel
	+ kernel assigns each process a number called a process ID(PID)
	
	+ traditionally the linux kernel loads and runs a process named init, which called "ancestor of all processes", it starts every subsequent process, ubuntu has replaced init with Upstart, Upstart compatible with init we will discuss init first
	the quiet option may be passed to the kernel at boot time to suppress many of these messages
	ubuntu does not display messages by default, instead uses a boot process created by the Fedora/Red Hat developers called Plymouth that is fast and incorporates a beautiful boot screen
	
	+ System Services and Runlevels
	the init boots linux to a specific state, commonly referred to as its runlevel
	Runlevels determine which of the many available system services are started and as well as in which order they starrt
	
	+ Runlevel definitions, in /etc/init.d some distribution use traditional /etc/inittab file to manage boot services, ubuntu has not used this and the file does not exists. If the file is created in ubuntu, it will also be read.
	Ubuntu has adopted some standards for runlevels
		* Runlevel 0, known as "halt", used to shut down the system
		* Runlevel 1, special runlevel defined as "single" which boots ubuntu to a root access shell prompt where only the root user may log in. It turn off networking, X and multi-user access, this is the maintenance or rescue mode
		* Runlevel 2, Default for ubuntu
		* Runlevel 3-5, aren't used in ubuntu but are often used in other linux distributions
		* Runlevel 6, reboot the system
	+ trespassers with physical access to the machine can also use runlevel 1 to access your system
	+ booting into the default runlevel, runlevel 2 needs to load by looking in the rc*.d directories in /etc, ubuntu contains directories for rc0.d through to rc5.d and rcS.d
	assume the value is 1 the rc script then executes all /etc/rc.1 directory and then launches the graphical login
	if ubuntu is boot to runlevel 1 for example, scripts beginning with the letter K followed by scripts beginning with the letter S under the /etc/rc1.d directory are then executed
	These script as with all rc*.d are actually symbolic links to system service scripts that reside in the /etc/init.d directory
	These prefixes indicate whether a particular service should be killed(K) or started(S) and pass a value of stop or start to the appropriate /etc/init.d script
- Understanding init Scripts and the Final Stage of Initialization
	+ /etc/init.d script or init script contains logic that determines what to do when receiving a start or stop value, logic might be simple as switch statement
	case "$1" in
		start)
			start;;
		stop)
			stop;;
		restart)
			restart;;
		reload)
			reload;;
		status)
			rhstatus;;
		condrestart)
			[ -f /var/lock/subsystem/smb] && restart ||:;;
		*)
			echo $"Usage: $0 {start|stop|restart|status|condrestart}"
			exit 1
	esac
	+ if your runlevel other than 5, the final act of the init processis to launch the user shell, bash, tcsh, zsh or any other
- controlling services at boot with administrative tools
	+ with ubuntu bash, search startup applications 
- changing runlevels
	+ use telinit command to change runlevels on-the-fly in ubuntu
	$ sudo telinit S
	change system to single-user mode
	
	$ sudo telinit 2
	after booting to single user mode then return to multi-user mode
	
	+ linux is full of shortcuts, if you exit the single-user shell by typing exit at the prompt you go back to the default runlevel without worrying about using telinit
- Troubleshooting runlevel problems, help to detect drivers or service failure
	+ using additional utilities such as dmesg | less command
	+ check system logging with cat /var/log/messages | less
	+ example X service is hung in a dead loop, not desktop displayed, x service only try to restart it self at runlevel 2, to check status of X servie need to switching to runlevel 1
	before change to runlevel 1 don't forget to info other users to save their works
		* switch to runlevel1 by telinit 1
		* try to start X server "naked"(without launching the window manager), a gray screen with a large x will display if successful, kil X with ctrl+alt+backspace, and look at your window manger configuration
		* if X don't run, check the log for Xorg /var/log, pay attention to any line start with (EE), examine the error log file .xsessions-error
		* search the error at www.google.com/linux, https://plus.google.com/+Linux
		* fix the configure and start x with startx
		* DO NOT FORGET TO BACK CONFIGURE BEFORE CHANGE IT
- starting and stopping services manually
	+ traditionally way to manage service is call the service's /etc/init.d with  appropriate keyword such as start, status or stop
	+ example to start Apache web server /etc/init.d/apache2
	$ sudo /etc/init.d/apache2 start
- Using upstart, developed for ubuntu by use by a growing number of distribution including Fedora(9 and later), Nokia's Maemo Platform, and google's chrome os
Upstart is an event-based replacement for the  /sbin/init daemon and System-V init system
Services may be respawned if they die unexpectedly and communication with the init daemon occurs over D-Bus
	+ example to use
	$ sudo service_name stop
	$ sudo start service_name
	+ reference
	http://upstart.ubuntu.com/	
- References
http://usr/src/linux/init/main.c, The best placye to learn about how linux boots
http://help.ubuntu.com/community/Grub2, documentation for GRUB2
http://www.ibm.com/developerworks/linux/library/l-grub2/index.html, an IBM guide for migrating to GRUB2


# System-Monitoring Tools
- console based monitoring
	+ ps, process display, command
	+ /proc file system, the virtual file system found on many UNIX flavors, the /proc file make you communicate directly with the kernel
	developers tend to use the /proc file to extracting information from the kernel
	+ reference /proc file book http://en.tldp.org/LDP/Linux-Filesystem-Hierarchy/html/proc.html
	+ start process in background
	$ gedit &
	[1] 9649
	gedit has been launched in the background and the (bash) shell reported a shell job number([1] in this case), a job number or job control is a shell specific feature such as sending or suspending program to the background and retrieving background jobs to the foreground
	second number (9649) represent the PID
	+ the proc man page has a full list command and options
	+ pip info by grep to display information about a specific program
	$ ps aux | grep bash
	returns the owner and the PID, along with other information such as percentage of CPU and memory usage
	+ kill a process by 
	$ kill PID
	this is no guarantee that a process will be kill
	$ kill -9 PID 
	force to kill a process, but this not allowed process shutdown gracefully
- using the kill command to control process
	+ signal to "hang up", 1
	$ kill -1 PID
	+ find full list in the man kill page
	kill is a UNIX basic system command, it's used to send system signals
	$ kill option PID
- using priority scheduling and control
priority is used by the kernel to help assigning system resource to running process
ubuntu contain two command to help manage the priority, nice and renice commands. 
	+ nice is part of the GNU Coreutils package
	nice command is used with its -n option, along with an argument in the range of -20 to 19, in order of highest to lowest priority
	nice command is used for disk-or CPU-intensive tasks
	$ nice -n 12 conky
	+ renice is inherite from BSD UNIX, regular user could only use it increase process priorities, super user privileges can use sudo to access full nice range (-20 to 19)
	+ use time command to get an idea of how much time and what proportion of a system's resources are required for task
	+ date command deals with civil and sidereal time. This command is used with the name of another comamnd (or script) as an argument like this
	$ sudo time -p find / -name conky
- displaying free and used memory with free
	+ use free command
	$ free
	if none of swap partition is being used and that the machine is not heavily loaded
	
	+ regular call the free command by watch command
	$ watch free
	the result will be updated in every 2 seconds, use ctrl+c to quit
	
	+ vmstat, virtual memory statistics, is another useful tool, this command reports on processes, memory, I/O, and CPU
	$ vmsta 5 10
	this run vmstat every five seconds for 10 iterations
	
	+ uptime command to see how long it beem since the last reboot and to get an idea of what the load average has been
- disk space   
	+ check the disk info by df command
	$ df
	$ df -h 
	with -h parameter will display the usage with mega ban giga bytes
- disk quotas, is used to restrict the usage of disk space either by user or by groups
- graphical process and system management tools, these tools require X session and root permission
if you view the tools locally while they are being run on the server, you must have X properly installed and configured on your local machine, you need configure pertinent X11 environment variables such as $DISPLAY, use the software or use the ssh clients's -X option when connecting to the remote host
	+ System Monitor
	+ Conky, highly configurable, rather complex system monitor tools, it can give you nearly anything, could get it from ubuntu repository
	http://conkyhardcore.com
	http://ubuntuforums.org/showthread.php?t=281865
		* configuration example, the file named conkyrc includes the configuration
		# Use Xft?
		use_xft yes
		# Xft font when Xft is enabled
		xftfont Ubuntu:size=9
		# gap is the number of pixels from the starting point under alignment
		#minimum_size 10 10
		gap_x 13
		gap_y 45
		# Text alignment, other possible values are commented#alignment top_left
		alignment top_right
		#alignment bottom_left
		#alignment bottom_right
		# Add spaces to keep things from moving about? This only affects certain objects.
		use_spacer right
		# Subtract file system buffers from used memory?
		no_buffers yes
		# Use double buffering (reduces flicker, may not work for everyone)
		double_buffer yes
		# Allows icons to appear, window to be moved, and transparency
		own_window yes
		own_window_type override
		own_window_transparent yes
		#own_window_hints undecorated,below,skip_taskbar
		# set to yes if you want Conky to be forked in the background
		background yes
		# Update interval in seconds
		update_interval 1
		cpu_avg_samples 1
		net_avg_samples 1
		# --start display config -	
		TEXT
		${alignc}${color #EA6B36}$sysname kernel $kernel
		${alignc}${color #EA6B36}${exec cat /etc/issue.net} on $machine host $nodename
		${color #EA6B36}Time:${color #E7E7E7} $time
		${color #EA6B36}Load average:${color #E7E7E7} $loadavg
		${color #EA6B36}Current CPU usage:${color #E7E7E7} ${color #EA6B36}CPU0:${color
		#E7E7E7} ${cpu cpu0}% ${color #EA6B36}CPU1:${color #E7E7E7} ${cpu cpu1}% ${color
		#EA6B36}CPU2:${color #E7E7E7} ${cpu cpu2}% ${color #EA6B36}CPU3:${color #E7E7E7}
		${cpu cpu3}%
		${color #EA6B36}CPU4:${color #E7E7E7} ${cpu cpu4}% ${color #EA6B36}CPU5:${color
		#E7E7E7} ${cpu cpu5}% ${color #EA6B36}CPU6:${color #E7E7E7} ${cpu cpu6}% ${color
		#EA6B36}CPU7:${color #E7E7E7} ${cpu cpu7}%${color #EA6B36}Updates: ${color
		#E7E7E7}${execi 3600 /usr/lib/update-notifier/apt_check.py --human-readable | grep
		updated}
		${color #EA6B36}Security: ${color #E7E7E7}${execi 3600 /usr/lib/updatenotifier/apt_check.py --human-readable | grep security}
		${color #EA6B36}Status:${color #E7E7E7} ${battery BAT0}
		${color #EA6B36}CPU usage ${alignr}PID CPU% MEM%
		${color #E7E7E7} ${top name 1}${alignr}${top pid 1} ${top cpu 1} ${top mem 1}
		${color #E7E7E7} ${top name 2}${alignr}${top pid 2} ${top cpu 2} ${top mem 2}
		${color #E7E7E7} ${top name 3}${alignr}${top pid 3} ${top cpu 3} ${top mem 3}
		${color #EA6B36}Mem usage
		${color #E7E7E7} ${top_mem name 1}${alignr}${top_mem pid 1} ${top_mem cpu 1}
		${top_mem mem 1}
		${color #E7E7E7} ${top_mem name 2}${alignr}${top_mem pid 2} ${top_mem cpu 2}
		${top_mem mem 2}
		${color #E7E7E7} ${top_mem name 3}${alignr}${top_mem pid 3} ${top_mem cpu 3}	
		${top_mem mem 3}
		${color #EA6B36}RAM Usage:${color #E7E7E7} $mem/$memmax - $memperc% $membar
		${color #EA6B36}Swap Usage:${color #E7E7E7} $swap/$swapmax - $swapperc% ${swapbar}
		${color #EA6B36}Processes:${color #E7E7E7} $processes ${color
		#EA6B36}Running:${color #E7E7E7} $running_processes ${color #EA6B36}
		${color #EA6B36}Hard disks:
		/ ${color #E7E7E7}${fs_used /}/${fs_size /} ${fs_bar /}
		${color #EA6B36}/TheLair ${color #E7E7E7}${fs_used /media/TheLair}/${fs_size
		/media/TheLair} ${fs_bar /media/TheLair}
		${color #EA6B36}Wireless Networking:
		${color #EA6B36}ESSID: ${color #E7E7E7}${wireless_essid wlan0} ${color
		#EA6B36}AP: ${color #E7E7E7}${wireless_ap wlan0}
		${color #EA6B36}${exec iwconfig wlan0 | grep “Frequency” | cut -c 24-44}
		${color #EA6B36}Mode: ${color #E7E7E7}${wireless_mode wlan0} ${color
		#EA6B36}Bitrate: ${color #E7E7E7}${wireless_bitrate wlan0}
		${color #EA6B36}Local IP ${color #E7E7E7}${addr wlan0} ${color #EA6B36}Link
		Quality: ${color #E7E7E7}${wireless_link_qual_perc wlan0}
		${color #EA6B36}total download: ${color #E7E7E7}${totaldown wlan0}
		${color #EA6B36}total upload: ${color #E7E7E7}${totalup wlan0}
		${color #EA6B36}download speed: ${color #E7E7E7}${downspeed wlan0}${color #E7E7E7}
		${color #EA6B36} upload speed: ${color #E7E7E7}${upspeed wlan0}
		${color #E7E7E7}${downspeedgraph wlan0 15,150 ff0000 0000ff} $alignr${color
		#E7E7E7}${upspeedgraph wlan0 15,150 0000ff ff0000}
		${color #EA6B36}Wired Networking:
		${color #EA6B36}Local IP ${color #E7E7E7}${addr eth0} ${color #EA6B36}
		${color #EA6B36}total download: ${color #E7E7E7}${totaldown eth0}
		${color #EA6B36}total upload: ${color #E7E7E7}${totalup eth0}
		${color #EA6B36}download speed: ${color #E7E7E7}${downspeed eth0}${color #E7E7E7}
		${color #EA6B36} upload speed: ${color #E7E7E7}${upspeed eth0}
		${color #E7E7E7}${downspeedgraph eth0 15,150 ff0000 0000ff} $alignr${color
		#E7E7E7}${upspeedgraph eth0 15,150 0000ff ff0000}
		${color #EA6B36}Public IP ${color #E7E7E7}${execi 5 curl ‘http://***a-website-thatreturns-your-ip-address–see below***’}
		${color #EA6B36}Port(s) / Connections:
		${color #EA6B36}Inbound: ${color #E7E7E7}${tcp_portmon 1 32767 count} ${color
		#EA6B36}Outbound: ${color #E7E7E7}${tcp_portmon 32768 61000 count} ${color
		#EA6B36}Total: ${color #E7E7E7}${tcp_portmon 1 65535 count}
		${color #EA6B36}Outbound Connection ${alignr} Remote Service/Port${color #E7E7E7}
		${tcp_portmon 1 65535 rhost 0} ${alignr} ${tcp_portmon 1 65535 rservice 0}
		${tcp_portmon 1 65535 rhost 1} ${alignr} ${tcp_portmon 1 65535 rservice 1}
		${tcp_portmon 1 65535 rhost 2} ${alignr} ${tcp_portmon 1 65535 rservice 2}
		${tcp_portmon 1 65535 rhost 3} ${alignr} ${tcp_portmon 1 65535 rservice 3}
		${tcp_portmon 1 65535 rhost 4} ${alignr} ${tcp_portmon 1 65535 rservice 4}
		${tcp_portmon 1 65535 rhost 5} ${alignr} ${tcp_portmon 1 65535 rservice 5	
	
		* many people choose to put their configure file at /home then write a script with the custom location. example to write a script and add it to start up will make Conky run after all the process started up
		#! /bin/bash
		sleep 45 &&
		exec conky -d -c ~/conky/conkyrc &
		# sleep 50 &&
		# exec conky -d -c ~/conky/conkyrc_weather &
		exit
		save it in /home/username/conky along with all your conky config files, make it executable and then have the startup applications process call it at boot
		you could run more than one instance of conky at a time
	+ Other, ubuntu include 
	vncviewer, AT&T's open-source remote session manager
	gnome-nettool, a GNOME-developed tool that enables system administrators to carry out a wide range of diagnostics on network interfaces, include port scanning and route tracing
	ethereal, graphical network protocol analyzer can be used to save or display packet data in real time and has intelligent filtering to recognize data signatures, include AppleTalk, Andrew File System(AFS), AOL's Instant Messenger, various Cisco protocols
- KDE Process and System-Monitoring Tools
kdf, a graphical interface to your system's file system
ksysguard, another panel applet that provides CPU load and memory use
- Enterprise Server Monitoring, include redundancy, failsafe and failover safegurads and so on
	+ Landscape, Canonical, the company that finances much of Ubuntu development has a monitoring servcie incorporated into its systems and administration tool, include unbuntu Enterprise Cloud and Amazon EC2
	http://www.canonical.com/enterprise-services/landscape
	+ Other
	Zenoss and Nagios
	http://help.ubuntu.com/community/Servers#Monitoring
- References
http://and.sourceforge.net/, home page of auto nice daemon (AND), be used to prioritize and reschedule process automatically
http://sourceforge.net/projects/schedutils/, home page for various projects offering scheduling utilities for real-time scheduling
http://www.ethereal.com/, home page for the ethereal client
https://help.ubuntu.com/community/VNC, Setting up Virtual Network Computing
	

# Backing Up
- Most common data loss
	+ hardware fails
	+ accidentally delete or overwrite
	+ natural disasters
	+ disgruntled employee
	+ some drivers have "still beta quality"
- TIP to backup 
	+ backup configuration file
	$ cp filename filename.original
	never edit or move the *.original file, try to change the mod to read only
	
	+ restore backup
	$ cp filename.original filename
- Assessing you backup need and resources
	+ what data must be safeguraded
	+ how often does the data change
	+ good strategy for home use is 
		* backup critical data frequently 
		* backup configuration and other files weekly
	+ enterprise level on a larger system   
		* have a plan, design a plan that is right for you needs
		* follow the plan
		* parctice your skills
	+ backup building blocks
		* maintain more than one copy of critical data
		* label the backups
		* store the backups in a climate-controlled and secure area
		* use secure, offsite storage of critical data
		* establish a backup policy
		* keep track of who has access to your backup media
		* routinely verify backups and practice restoring data
		* routinely inspect backups media for defects and regularly replace them
- evaluate backup strategies
	+ home user, backup any configuration files, a weekly backup is probably adequate
	+ small office, backup critical data daily
	+ small enterprise, autoloading tape
	+ large enterprise
- Backup levels
UNIX uses the concept of backup levels as a shorthand way of referring how much data is backed up
level 0, full backup
level 1, generate an incremental backup from the full backup
level 2, generate a differential backup between 0 and 1
other level will backup everything that has changed since the last backup 
- Simple Strategy, backup only configuration files and small data files
- Full backup on a periodic basis, mirror the data to another machine
- Full backup with incremental backups
modern commercial backup application such as Amanda or BRU
- Mirroring data or RAID arrays are best suited for protecting the current state of running system, RAID array will not backup deleted file
- Making the choice
	+ if the backup strategy and policy is too complicated, it will be disregarded
	+ combination of strategies use what works
- Choosing backup hardware and media
	+ Anything with wheels can take you on a cross country trip
- Removable storage media
USB, CD-RW, DVD+RW, Network Storage, Tape Drive Backup, cloud storage, Amazon's AWS and S3, Cannonical's ubuntu one
http://www.ubuntu.com/cloud
- Using backup software, Amanda works with drive as well as tapes
	+ tar, the most basic backup tool, saving entire directories full of files
	$ sudo tar cvf etc.tar /etc
	$ sudo tar cv /etc > etc.tar
		* with -z to create restore gzip archives
		* with -j works with bzipped files
		* creating full band incremental backups with tar
		$ sudo tar cjvf fullbackup.tar.bz2
		
		locate all the changed files and make the incremental backup
		$ sudo find / -never name_of_last_backup_file ! -a -type f -print
		the ! -1 -type eliminates everything but regular files
	
		pip the output to tar
		$ sudo find / -newer name_of_last_backup_file ! -type d -print | \tar czT -backup_file_name_or_device_name
		Here the -T option gets the filenames from a buffer
		* NOTE
		tar command can backup to a raw device and to a formatted partition
		$ sudo tar cvzf /dev/hdd /boot /etc /home
		
		tar command can also back up over multiple floppy disks
		$ sudo tar czvMf /dev/fd0 /home
	+ restoring files from an Archive with tar, xp option in tar restores files from a backup and preserves the file attributes, as well. the backup may create with relative or absolute paths, BEFORE RESTORE USE tvf option WITH TAR TO CHECK LIST OF THE FILES, the path start with / is absolute path
		* restore tar archive compressed with bzip2
		sudo tar xjvf ubuntutest.tar.bz2
		
		* list the contents of bzip2
		$ sudo tar tjvf ubuntutest.tar.bz2
		
	+ GNOME file Roller, (file-roller), it is just a graphic interface to the command-line tool such as tar, gzip ...
	+ KDE backup tool ark, kdat
	+ Deja dup, with a useful GUI, support local remote or cloud backups
	+ backup in time		, is a viable laternative to Deja dup
	+ Unison, is a file-synchronization tools that works on multiple platforms, free under GPL license
	http://www.cis.upenn.edu/~bcpierce/unison/
	+ Amanda backup application, network backup application, created by the university of Maryland at College Park
		* good user support and documentation
		* high capacity backup l
		* use tar and dump GNU tools
		* use Samba to backup Microsoft Windows
		* file compression can be done on either the client or server, thus lightening the computational load on less-powerful machine
		* CAUTION
		amanda does not support dump images larger than a single tape requires a new tap
		* there is no GUI for amada, configuration is at UNIX tradition of editing text configuration files located in /etc/amanda, includes a sample cron file, because it expect use cron run amanda regularly client utilities are installed with the package amanda-client, the server is called amanda-server
		* check the man page for detail
	+ Alternative Backup Software, BRU and Veritas are good example of effective commercial backup products, free software backup tools not installed with ubuntu
		* flexbackup, write with perl, http:www.edwinh.org/flexbackup/, lack of mantainance
		* afio, creates cpio formatted archives but handles input data corruption better than cpio, http://freshmeat.net/projects/afio, File archive created in the Unix CPIO (Copy In, Copy Out) format, an uncompressed file container format used for grouping files together; similar to a .TAR archive and can be compressed into a .CPGZ file using Gzip compression.
	+ look for free backup software
	http://www.freshmeat.net
- copying files, with tar, cp, rsync, even th cpio, rsync is recently added into ubuntu which is a execllent choice for mirroring sets of files especially done over a network
	+ copy with tar, create a tar -> uncompress it to the source. change to source directory and runs
	$ tar -cvf files | (cd target_directory ; tar -xpf)
	c create a archive
	v verbose, lists the files processed
	f filename of the archive(In this case, it is -)
	
	the following tar command can be useful for creating file copies for backup purposes
	l stay in the local file system (so that you do not include remote volumes)
	atime-preserve, do not change access times on files
	+ extract file
	x extract files
	p reserve permissions
	f the file name will be -, the temporary buffer that holds the files archived with tar
	
	+ copying files using cp
	$ cp -a src dst
	a   the same as giving -dpR
	d   preserve symbolic links, copies the files that the point to instead of copying the links
	p   preserve all file attribute
	R   recursively
	
	+ copying files using mc, Midnight Commander (available in the Universe repository) under the package mc; it feels Norton Commander of DOS fame, by executing mc at a shell prompt, tar files, gzip tar files(.tar .gz or .tgz), bzip files, DEB files and RPM files, press F1 to display the help file
	
- compressing, encrypting and sending tar streams   
$ tar -cvzf data_folder | ssh remote_host '(cd ~/mybackup_dir; tar -xvzf)'
tar backup files in directory data_folder and pip the result to ssh to the remote computer remote_host and extract it saved in directory ~/mybackup_dir, it is a excellent chocie for file management on servers not running in X
- Using rsync
	+ create an empty file
	$ sudo touch backup.sh
	
	+ edit the file with
	sudo rsync --force --ignore-errors --delete --delete-excluded --exclude-from/home/matthew-exclude.txt --backup --backup-dir=`date +%Y-%m-%d` -av /media/externaldrive/backup/seymour
	
	+ make the file executable
	$ sudo chmod +x backup.sh
	
	+ this script can be run at the command line using sudo sh .backup.sh or as an automated cron job
- version control for configuration files, Git, Subversion, Mercurial and Bazaar
	+ etckeeper takes all of your /etc directory and stores the configuration files from it in a version control system repository
	editing the etckeeper.conf file to store data in a Git, Mercurial, Bazaar or Subversion repository
	APT package mangement tools used by Ubuntu and automatically commits changes to /etc and files 
	by default etckeeper use Git, ubuntu changed to Bazzar(bzr) which is used by ubuntu develpor
	using version control to track passwords can be a security rish
	
	First edit /etc/etckeeper/etckeeper.conf to use your desired settings such as version controls sytem to sue, the system package manager 
	
	+ other package manager Yum for Fedora
	+ init the etckeeper
	$ etckeeper init
	
	+ make change with any /etc files and commit the change
	$ etckeeper commit "changed prompt style"
	
	+ check the versions by bzr(which is the default version control system)
	$ bzr log /etc/bash.bashrc
	
	+ reverse to previous version
	$ bzr revert -revision 2 /etc/bash.bashrc
	2 is the revision number which will display in the log of check version
- backup Dotfiles, are configuration files and directories in a user's /home direcotry, all of which begin with a dot like .bashrc because they may often customized by highly technical people to suit their desires, backing them up is a good idea, version control systems are commonly used. 
Ubuntu called dotdee performs this task
- System Rescue
	+ ubuntu rescure disc
	+ restoring the GRUB2 boot loader, will work with ubuntu 9.10 or later, only if the system uses GRUB2
		* boot from DVD
		* determine which partitions holds the ubuntu installation
		$ sudo fdisk -l
	
		the partition will probably be on a drive called sda on the first partition
		 
		$ sudo mount /dev/sda1 /mnt
		mounts the drive in the current file system at /mnt
		* reinstall GRUB2
		$ sudo grub-install -root-directory=/mnt/ /dev/sda
		
		* reboot
		$ sudo update-grub
		
		https://help.ubuntu.com/community/Grub2
- Saving files form a Nonbooting hard drive
- References
https://help.ubuntu.com/community/BackupYourSystem
http://tldp.org, linux documentation project
http://book.git-scm.com/, the git community book	   
http://svnbook.red-bean.com/, svn book
		
		
# Networking
- linux is particular can be used with Internet protocol TCP/IP, Linux can talk to all UNIX flavors including Mac OS X, Windows (with the halp of Samba), NetWare(IPX), even older protocols such as DECNET and Banyan Vines., built-in support for IPv6
- laying the foundation: the localhost interface, sometimes called a loopback interface commonly referenced as lo. 
The TCP/IP protocol uses this interface to assign an IP to your PC and is need for ubuntu to establish a PPP(point to point protocol) interface
- check for the availability of the loopback interface
	+ check networking interfaces
	$ ifconfig
	RX = receive
	TX = transmit
	
	+ Configure the loopback interface manually, ip address file in ubuntu is /etc/hosts, the file content is like this
	127.0.0.1 localhost
	127.0.1.1 seymour	
	# The following lines are desirable for IPv6 capable hosts
	::1 localhost ip6-localhost ip6-loopback
	fe00::0 ip6-localnet
	ff00::0 ip6-mcastprefix
	ff02::1 ip6-allnodes
	ff02::2 ip6-allrouters
	ff02::3 ip6-allhosts127.0.0.1 localhost		
	
	the begins ::1. This is used to define the localhostconnection for IPv6
	
	+ Recreate the loopback interface
	$ sudo /sbin/ifconfig lo 127.0.0.1
	$ sudo /sbin/route add 127.0.0.1 lo
	this command create the localhost interface in memory(such as eth0 or ppp0) and then add the IP address 127.0.0.1 to an internal(in-memory) table
	 
	+ check interface that the interface is responding properly like this
	$ ping -c 3 localhost
		
	text with ping6 command at the terminal as follows
	$ ping6 -c 3 ::1
- Networking with TCP/IP, Transport Control Protocol/Internet Protocol(TCP/IP) suit, Universal Datagram Protocol(UDP)
	+ In TCP/IP all data travels via IP packets
	+ also a connection-based protocol
	+ UDP is a connectionless protocol
	+ nearly all ethernet cards can used with linux along with many PCMCIA wired and wireless network cards, many usb wireless networking devices also work just file with linux
- Linux USB project, http://www.linux-usb.org
- Graphic network client for linux, http://www.ethereal.com/, can be used to monitor all traffic on you LAN or specific types of traffic
- Nmap can be used to scan a specific host for open ports and other running services 
http://nmap.org
- TCP/IP Addressing
	+ American Registry for Internet Numbers, available at 
	http://www.arin.net
	The agency assigns Internet service provider (ISPs) one or more blocks of IP addresses which the ISPs can then assign to their subscribers
	+ classes of network
		* class A, first octet ranging from 1 to 126
		the 10.net network is reserved for local network use and the 127. network is reserved for loopback address of 127.0.0.1. Loopback addressing is used by TCP/IP to enable linux network-related client and server programs to communicate on the same host. This address does not appear and is not accessible on you LAN.
		* class B, first two octets, ranging from 128 to 191. 128. is reserved for local network use
		* class C, consist of a network defined by the first three octets, ranging from 192 to 223. The 192. network is another that reserved for local network use
		* NOTE, 0 is not include in class a, the 0 addresss is used for network-to-network broadcasts. 
		255 is also reserved for local broadcast
		broadcast not typically seen by users
		* class D and E, class D are reserved for multicast addresses and not for use by network hosts, class E addresses are deemed experimental and thus are not open for public addressing
	+ class of network netmasks
		* class A, 255.0.0.0
		* class B, 255.255.0.0
		* class C, 255.255.255
	+ the limit of current IP addressing
	IPv4 scheme is based on 32-bit numbering and limits the number of available IP addresses to about 4.1 billion
	IPv6, base on 128-bit addresses, enough room to include global positioning server(GPS) or serial numbering
	overview difference between IPv4 and IPv6
	http://www.arin.net/knowledge/v4-v6.html
	http://www.arin.net/resources/request.html
	+ ubuntu support IPv6, but IPv6 is slow in coming
- using IP Masquerading in Ubuntu, three blocks of IP addresses are reserved for use on internal networks and hosts not directly connected to the Internet.
The address ranges are from 
10.0.0.0 to 10.255.255.255, 1 class a network 
172.16.0.0 to 172.31.255.255, 16 class b networks;
192.168.0.0 to 192.168.255.255, 256 class c networks
use these address to build your business network or home
- do not rely on a single point of protection 
- ports, servers often have to multiple roles, applications are provided ports to use to make "direct" connections for specific software services. These ports help TCP/IP distinguish services
check /etc/services/ you will see the common ports and their usage, for example FTP, HTTP and POP3
	+ example
	21  ftp
	80  http
	110 pop3
	25  simple mail transport protocol, smtp
	22  for secure shell (SSH)
	
	+ the sshd server can be configured to listen on a different port by editing its configuration file
	/etc/ssh/sshd_config which is # port 22
	
	+ restart the sshd server
	$ ssh -p 2224 remote_host_name_or_ip
- Network Organization
	+ select the local network class A,B,C base on the number of the computers in the company
	+ subnetting, class A and B can be separate the networks called subnets. Subnets are considered part of the host port of an address for network class definitions
		* example
		one computer with 128.10.10.10 and another with 128.10.200.20; these computers are on the same network(128.10), but they have different subnets(128.10.10 and 128.10.200), communicate between the two computers requires either a router or a switch. subnet can be helpful for separating workgroups within your company
		* using separate subnet for each office is the best solution
	+ subnet masks, class A network the netmask is 255.0.0.0, class B is 255.255.0.0, class C is 255.255.255.0
	changing the last octet to a number greater than zero you can break network into as many subnets as you need
	+ references
	Reference book "The Art of Subnet Masking" in Sams Teach Yourself TCP/IP Network Administration
	http://www.tldp.org/LDP/nag2/index.html, linux network administrator's guide
- Broadcast, Unicast and Multicast Addressing
Information can get to system through three types of addresses, unicast, multicast and broadcast
	+ unicast, sends inforamtion to one specific host, are used for Telnet, FTP, SSH in one-to-one exchange of information
	+ multicasting, broadcasts information to groups of computers sharing an application such as a video conferencing client or online gaming application
	+ broadcasting, transmit information to all the hosts on the network or subnet, Dynamic Host Configuration Protocol(DHCP) uses broadcast messages when the client looks for a DHCP server to get its network settings, and Reverse Address Resolution Protocol (RARP) uses broadcast messages for hardware address to IP address resolution
	broadcast message use .255 in all the host octets of the network IP address(10.2.255.255 will broadcast to every host in your class B network)
- Hardware devices for networking
	+ network interface cards, (NIC) to connect to a network
	+ token ring, developed by IBM, has a maximum transfer rate of 16Mbps, token ring uses what is called unshielded twisted pair(UTP) cable
	+ 10BASE-T, was the standard for a long time, uses UTP cable mostly uses a star architecture. transfer rate of 10Mbps, all host connect to a central location(a hub)
	+ 100BASE-T
	+ 1000BASE-T
	+ Fiber Optic and Gigabit Ethernet, fiber distributed data interface(FDDI) netowrks, a speed of 100Mbps and maximum ring length of 100 kilometers (62miles)
	+ Wireless network interfaces
	802.11g, NIC works up to 108Mbps
	802.11n promises to be up to seven times faters than 802.11g, but adoption of standardization is going slowly
	+ network cable
	coaxial, UTP, and fiber, coaxial cable (rarely used today)
		* unshielded twisted pair
		category 1(cat1), used for voice transmission, such as your phone, only one pair is used per line
		category 2(cat2), used in early token ring networks, has a transmission rate of 4Mbps, An RJ11 plug is also used for cable connections
		category 3(cat3) used for 10BASE-T network, has a transmission rate of 10Mbps, three pairs of cables are used to send and receive signals, usually deferring to the smaller RJ-11, bigger RJ45 plugs
		category 4(cat4), used in modern token ring network, transmission rate of 16Mbps, used RJ-45 plugs
		category 5(cat5) the fastest of the UTP categories with transmission rate of up to 1000 Mbps, uses four pairs of wire
		category 6(cat6), also rate at 1000Mbps
		* fiber-optic cable, is usually orange or red in color, transmission rate is 100Mbps and has a maximum length of 100 kilometers(62 miles)
	+ hubs and switches
	+ routers and bridges, are used to connect different networks to you network 
		* bridges, are used with a network to connect different subnets without filtering, referred to as a dumb gateway
		* routers can pass data from one network to another and they allow for filtering of data
- initializing new network hardware, in ubuntu it's done during installation, if you replace your hardware, you need to told linux to load a specific kernel module to support your new installed hardware. for example NIC there are more than 100 such modules are located in the /lib/modules/2.6.XX-XX/kernel/net
	+ configure linux to auto hardware detection in booting
		* manually editing the /etc/modprob.conf file to prompt the system to recognize and support the new hardware
		it may not present when you first look for it, you need to use text editor to create one and add a entry into the file, then linux will recognizes your networking hardware upon reboot
		add a RealTek NIC
		
		alias eth0 8139too
		
		entry tells the linux to load the 8139too.o kernel module to support the eth0 network 
		
		* manually loading or unloading the new device's kernel module
		using moprobe command to load device's kernel module
		$ sudo modprobe 8139too
		
		use dmesg command to check the message
		$ dmesg
		
		the IP address and other setting have not been assigned to the new add device
		linux can use multiple Ethernet interfaces, the first device is eth0 and the second is eth1, ...
- using network configuration tools
	+ configure hardware through a graphical interface, you can use ubuntu's tool for X called nm-connection-editor, found at
	System | Preferences | Network Connections
	
	+ command-line network interface configuration
	netstat command displays information about the network connections
	/sbin/ifconfig
	ifconfig is used to configure your network interface
		* active or deactive you NIC or change your NIC's mode
		* change your machine's IP address, netmask or broadcast address
		* create an IP alias to allow more than one IP address on your NIC
		* set a destination address for a piont-to-point connection
		* command format 
		ifconfig [network device] options
		* ifconfig options
		create alias		   ifconfig eth0:0_:[number] 10.10.10.10
		change ip			  ifconfig eth0 10.10.10.12
		change the netmask	 ifconfig eth0 netmask 255.255.255.0
		change the broadcast   ifconfig eth0 broadcast 10.10.10.255
		task interface down	ifconfig eth0 down
		bring interface up	 ifconfig eth0 up(ifconfig eth0 10.10.10.10)
		Set NIC promiscuous	ifconfig eth0 promisc mode on [off]
		Enable or disable	  ifconfig pointopoint
		* promiscuous mode cause NIC to receive all packets on the network. it is often to sniff a network
		multicasting mode enables the NIC to receive all multicast traffic on the network
		* keywords
		BROADCAST denotes that the interface is connected to a network that supports broadcast
		LOOPBACK shows which device (lo) is the loopback address
		MTU, maximum transmission unit, determines the size of the largest packet
		Metric is a number from 0 to 3 that relates to how much information from the interface is placed in the routing table
		
	+ configure your system to work with your LAN
	/sbin/route
	the second command used to configure your network is route command, route is used to build the routing tables (in memory) implemented for outing packets and to display the routing information. It is used after ifconfig has initialized the interface
	route is normally used to setup static routes to other networks via the gateway or to other hosts
	route [options] [commands] [parameters]
	display the routing table use the route command with no options
	$ route
		* keywords
		destination is the ip address
		default entry is the default gateway for this machine
		asterisk(*) means that packets must go directly to the host
		Genmask is the netmask
		Flags column can have several possible entries, U verifies that the route is enabled and G specifies that the Destination requires the use of a gateway
		Metric column displays the distance to the Destination
		Ref column is used by some UNIX flavors to convey the references to the route
		Use column indicates the number of times this entry has been looked up
		Iface column is the name of the interface for the corresponding entry
		with -n option will give the same information, substituing IP addresses for names and asterisks(*) and looks like this
		$ route -n
		
		* check the detail from the man page
		* use add option to add to the table
		$ sudo route add default gw 149.112.50.65
		$ sudo route add -net 208.59.243.0 netmask 255.255.255.0 dev eth0
	
	+ the netstat command is used to display the status of your network， options
	-g displays the multicast groups configured
	-i displays the interfaces configured by ifconfig
	-s lists a summary of activity for each protocol
	-v gives verbose output listing both active and inactive sockets
	-c updates output every second
	-e gives verbose output for active connections only
	-C displays information from the route cache and is good for looking at past connections
- network configuration file   
/etc/hosts, a listing of addresse, hostnames and aliases
/etc/servers, network service and port connections
/etc/nsswitch.conf, linux network information service configuration
/etc/resolv.conf, domain name service(DNS) domain(search) settings
/etc/host.conf, network information search order(by default, /etc/hosts and then DNS)
all these files have man page, check the detail from it
	+ adding host to /etc/hosts, is a map of IP to hostnames
	127.0.0.1 localhost
	127.0.1.1 optimus
	
	# the following lines are desirable for IPv6 capable hosts
	::1 ip6-localhost ip6-loopback
	fe00::0 ip6-localnet
	
	+ Service setting in /etc/services, file maps port numbers to services
	# Each line describes one service, and is of the form:
	#
	# service-name port/protocol [aliases ...] [# comment]
	tcpmux 1/tcp # TCP port service multiplexer
	tcpmux 1/udp # TCP port service multiplexer
	rje 5/tcp # Remote Job Entry
	rje 5/udp # Remote Job Entry
	echo 7/tcp
	echo 7/udp
	typically each service have two entries, TCP and UDP
	
	+ using /etc/nsswitch.config After changing naming services
	this file was initially developed by Sun Microsystems to specify the order in which services are accessed on the system
	passwd: compat
	group: compat
	shadow: compat
	hosts: files dns mdns
	networks: files
	protocols: db files
	services: db files
	this tells services that they should consult standard UNIX/linux files for passwd, shadow, and group(/etc/passwd, /etc/shadow, /etc/group, respectively)
	
	+ setting a name server with /etc/resolv.conf, is used by DNS, the following is an example of resolv.conf
	nameserver 192.172.3.8
	nameserver 192.172.3.9
	search mydomain.com
	
	+ setting DNS search order with /etc/host.conf, lists the order in which your machine will search for hostname resolution, the following is the default /etc/host.conf file
	order host, bind
	the host check the /etc/hosts file first and then perform a NDS lookup, The only reason to modify this file is if you use NIS for your name service or you want one of the optional services. The nospoof option can be a good option for system security
- using graphical configuration tools, right-click the networking icon on your top panel and choosing edit connections
- dynamic host configuration protocol, DHCP
	+ you can learn more about DHCP by reading RFC2131 "dynamic host configuration", http://wwww.ietf.org/rfc/rfc2131.txt
	+ DHCP allows a network administrator to configure all TCP/IP parameters for each host as he connects to the nectowk after activation of a NIC
	+ how DHCP works
		* network subnet/host address, used by hosts to connect to the network at will
		* subnet/hostname, enable the specified host to connect to the subnet
		* subnet/hardware address, enables a specific client to connect to the network after getting the hostname from DHCP
	+ NOTE, Dyn.com(http://www.dyndns.org/) are already offering Dynamic DNS services and have clients for linux
	+ Activating DHCP at installation and boot time
		* instruction to use DHCP for your NIC can be found under /etc/network/interfaces with a line that says dhcp
		
		* other settings are saved in the file named dhclient.conf under the /etc/dhcp3/dhclient.conf directory
		quickly configure your NIC by using the dhclient as follows
		$ sudo dhclient
		
	+ DHCP software installation and configuration, using synaptic or apt-get
		* DHCP dhclient, saved in /etc/dhclient.conf
		* CAUTION
		backup the dhclient.conf file, if anything goes wrong use the backup to resotre the original file
		
		* options of dhclient.conf
		timeout time; how long to wait before giving up trying
		retry time; how long to wait before retrying
		select-timeout time; how long to wait before selecting a DHCP offer
		reboot time; how long to wait before trying to get a previously set IP
		renew date; when to renew an IP lease

	+ DHCP Server, get the dchp3-server package or build it by yourself from http://www.isc.org/ , Internet Software Consortium website
	recommend to use ubuntu repositories duel to easy maintenance
		* install from source code
		unpack your tar file -> run ./configure from the root of the source -> run make-> run make install
		
		* check install by /etc/dhcp3/dhcp.leases is created

	+ Using DHCP to Configure Network hosts, /etc/dhcp3/dhcp.conf, to start the server at boot time, use the service or ntsysv commands
	ubuntu includes a sample dhcp.conf in /usr/share/doc/dhcp*/dhcpd.conf.sample
	DHCP server source files also contain a sample dhcp.conf file
		* setting the domain name, option 
		* setting DNS servers, options
		* setting the default and maximum lease time, default-lease-time 3600 and max-lease-time 14400
		* the dhcp.conf file requires semicolons(;)
		* example
		subnet 10.5.5.0 netmask 255.255.255.224 {
			range 10.5.5.26 10.5.5.30;
			option domain-name-servers ns1.internal.example.org;
			option domain-name “internal.example.org”;
			option routers 10.5.5.1;
			option broadcast-address 10.5.5.31;
			default-lease-time 600;
			max-lease-time 7200;
		}
		show which DNS server the subnet will connect to, which can be good for DNS server load balancing
		
		If you want your server to ignore a specific subnet, you can do so as follows:
		subnet 10.152.187.0 netmask 255.255.255.0 {
		}
		
		hardware address can be obtained by using the ifconfig
		host hopper {
			hardware ethernet 08:00:07:26:c0:a5;
			fixed-address hopper.matthewhelmke.com;
		}
		DNS lookup to assign the IP address for hopper.matthewhelmke.com to the host

		DHCP can also define and configure booting for diskless clients, like this:
		host bumblebee {
			hardware ethernet 0:0:c0:5d:bd:95;
			filename “vmunix.bumblebee”;
			server-name “kernigan.matthewhelmke.com”;
		}
		* CAUTION
		remember to avoid problems, only one DHCP server should be configured on a local network, check other machine and hardware such as router to disable DHCP
- other uses for DHCP
reference book, The DHCP Handbook, http://wwww.dhcp-handbook.com/
- Wireless networking
	+ support for wireless networking in ubuntu, tools
	iwconfig, sets the network name, encryption, transmission rate and other features
	iwlist, displays information about wireless interface
	iwpriv, used to set optional features, such as roaming, of a wireless network interface
	iwspy, shows wireless statistics of a number of nodes
	
	+ linux wireless device software usually in the form of kernel module, ifconfig will display if there is any new installed wireless card
	use iwconfig to set various parameters of the wireless device
- Blootooth use conflicts with 802.11 networks, check http://www.bluetooth.com
- IrDA, Infrared Data Association
- Advantages of Wireless Networking
- Choosing from Among Available Wireless Protocols
The Institute of Electrical and Electronics Engineers (IEEE)   
- Beyond the network and onto the internet
	+ Common configuration information
	+ Some ISPs are unaware of Linux or unwilling to support its use with their service
	+ DSL, get the DNS server from the ISP, modem information, /dev/modem
- Configuring Digital Subscriber Line Access(xDSL, include ADSL, IDSL, SDSL)
	+ ubuntu is pre configure to not listen on any network ports
	+ understanding PPP over Ethernet
	Roaring Penguin's rp-pppoe clients are available from the Roaring Pengui site(http://wwww.roaringpenguin.com/pengui/pppoe/rp-pppoe-3.8.tar.gz)
	+ configuring a PPPoE Connection Manually
		* setup the harder, environment eth0 for LAN and eth1 for DSL connection
		* run configuration script, for Roaring Penguin, use rp-pppoe
		$ sudo /sbin/ifconfig eth1 0.0.0.0 up
		use adsl-setup command to setup your system
		$ sudo /sbin/adsl-setup
		then follow the wizard
		
		Changes are made to your system’s /etc/sysconfig/network-scripts/ifcfg-ppp0,/etc/resolv.conf, /etc/ppp/pap-secrets, and /etc/ppp/chap-secretsfiles.

		* start the connection 
		$ sudo /sbin/adsl-start
		
		* add IP masquerading
		$ sudo /sbin/route add default gw 192.168.0.1
		
		* stop connection by 
		$ sudo /sbin/adsl-stop
- Configuring Dail-up internet access, setup PPP, PPP uses several components on your system, first is a daemon called pppd, which controls the use of PPP, second is a driver called the high-level data link control(HDLC), third is a routine called chart that dails the other end of the connection for you when you want it to
	+ ubuntu using pppconfig, a command-line utility to help you to configure specific dial-up connection strings
	$ sudo pppconfig
	
	+ add yourself into the dip and dialout groups by 
	$ sudo adduser YOUR_USER_NAME dip
	$ sudo adduser YOUR_USER_NAME dialout
	
	* connect and disconnect by pon, poff command
- Troubleshooting connection problems, http://www.tldp.org/
	+ modem connects and then hangs up, you are probably using wrong password or an authentication protocol problem
	+ connected can't get website, DNS problem
	+ check hardware
	+ work in window but no work with linux, software problem
	+ everything just stops working ( not see smoke ) ISP company's problem
- Related ubuntu and linux commands to managing network
dhclient, automatically acquire, and then set IP info for a NIC
etherreal, GNOME graphical network scanner
ufw, ubuntu's basic firewalling tool
ifconfig, mange network device
iwconfig, mange wireless device
route, display and manage routing table
ssh, the OpenSSH remote-login client preferred replacement for telnet
nm-connecton-editor, ubuntu's GUI for configuring network connection
- References
http://ietf.org/rfc.html, Request For Comments
http://www.ieee.org, institute of eletrical and electronics engineers


# Remote Access with SSH and Telnet
- commonly use Telnet and ssh, telnet is a earlier technical, ssh is more securer
- setting up a telnet server
	+ use synaptic or apt-get to install telnetd
	+ configuring firewal to allow connections through port 23
	+ check client and service are installed
	$ telnet local_ip
	
	+ use telnet with other port to test a service on the other computer
	$ telnet 192.168.1.102 110
	
	+ telnet is not secure but fast, it send all the information include password with plain text, so most of the time use it as the last resort only
- setting up an ssh server, OpenSSH server can be installed through Synaptic by adding the openssh-server package
	+ insntall openssh-server package in ubuntu
	+ allow connection through port 22
	+ two different SSH exists
		* ssh1
		* ss2, latter is newer, more secure which is default in ubuntu
		this is done by default, the configure file at /etc/ssh/sshd_config
		Protocol 2
		to allowed ssh1 client connect by 
		Protocol 1
	+ SSH Tools
	ssh, secure shell command
	sftp, replacement for ftp
	scp and srp
		* ftp is unsecure, it sends your data in plain text across the network, anyone could sniff out your user name and password from the packages
		* scp is used to send one file to remote server or with -r option to send a directory
		* using scp to copy individual files between machines
		$ scp text.txt 192.168.1.102:[location]
		if the location is empty, scp will copy the file to the /home directory

		$ scp text.txt 192.168.1.102:new_name.txt
		copy the file to /home and rename it

		support copy remote file to local
		
		* use sftp to copy many files between machines, sftp is a mix between ftp and scp
		$ sftp username@ipaddress[:directory]
		cd, put, mput, get, quit and so on
		
		* using ssh-keygen to enable keybased logins, generate a 1024-bit private and public key pair for your machine
		create a ssh-keygen command
		$ ssh-keygen -t dsa
		after the key is generated, change the directory to .ssh(cd ~/.ssh) which is a hidden directory where your key is stored and also where it keeps a list of safe SSH hosts
		the default setting will generate two files, id_dsa and id_dsa.pub, you need to copy public key to each server you want to connect to via key-based SSH
		using scp you can copy the public key over to your server like this
		$ scp _id_dsa.pub 192.168.1.102:
		
		use ssh connect to the remote server and set up that key as an authorized key
		[$ touch .ssh/authorized_keys]
		$ cat id_dsa.pub >> .ssh/authorized_keys
		$ chmod 400 .ssh/authorized_keys
		if the file doesn't exist use the touch command create it first
		reconnect with the ssh will prompted for you passphrase
		Edit remote server's /etc/ssh/sshd_config file, requires super user privileges. Look for PasswordAuthentication line and make sure it reads no(and not commented out with a #). Save the file and run kill -HUP 'cat /var/run/sshd.pid'. With that done, sshd accepts only connections from client with authorized keys
		TIP, for extra security, you are strongly encouraged to set PermitRootLogin to no in /etc/ssh/sshd_config, most brute-force attempts take place on the root account because it is the only account that is guaranteed to exits on a server
- virtual network computing(VNC), nearly all linux distros bundle VNC and clients, configure firewall to allow connections through port 5900   
	+ set up VNC on ubuntu
	+ tell who should be allowed to connect to, from menu
	System | Preferences | Remote Desktop
	+ Check allow other users to view your desktop to share it
	+ Check allow other users to control you desktop
	+ uncheck you must confirm each access to this machine
	+ VNC should not be considered secure over the internet or on untrusted internal networks
- References
http://www.telnet.org
https://help.ubuntu.com/community/SSH, the ubuntu community documentation for SSH		
http://www.openssh.com, the home page of the OpenSSH implementation of SSH that ubuntu uses
https://help.ubuntu.com/community/VNC, the ubuntu community documentation for VNC
http://www.realvnc.com, the home page of the team that made VNC at AT&T's cambridge research laboratory
http://www.tightvnc.com, an alternative to VNC and support SSH for encryption
https://help.ubuntu.com/community/FreeNX, The Ubuntu community documentation for FreeNX
http://www.nomachine.com/, another alternative to VNC is in the pipline, called NX, promise to work faster than VNC


# Securing Your Machines
- Built-In Protection in the Kernel, a number of networking and low-level protective services are built in to the linux kernel. using the sysctl command to control. echoing a value(using a 1 or 0 to turn a service on or off) to a kernel process file under the /proc directory
- understanding computer attacks
	+ internal 
	+ external
- Hacker Versus Cracker, Hacker did good or cool things and cracker did bad things
- a five-step checklist to secure your box
	+ assess your vulnerability. decide which machines can be attacked, which services they are running, and who has access to them
	+ configure the server for maximum security. install only what you need, run only what you must and configure a local firewall
	+ secure physical access to the server
	+ create worst-case-scenario policies
	+ keep up-to-date with security news
- Assessing your vulnerability, switching on a firewall makes them safe, most common mistakes
	+ install every package
	+ enabling unused services
	+ disabling the local firewall on the grounds that you already have a firewall at perimeter
	+ letting your machine give out more information than it needs to
	+ placing your server in an unlocked room
	+ plugging your machine into a wireless network
- which attack vectors are open on your server? in internet terms, this comes down to which services are internet facing, which ports they are running on
- two tools are often used to determine your vulnerabilities, scan your machine and queries the services running, checks their version numbers against its list of vulnerabilities and report porblems
	+ Nmap, is better
	+ Nessus, is sounds clever but not fit for ubuntu
- using Nmap
	+ download from ubuntu repositories
	+ run the front end, open a terminal and run nmapfe, enable all Nmap's options need root privileges
	+ use the output from Nmap to help you find and eliminate unwanted services
	+ suggest not use Nmap to scan other people's servers
- Protecting your machine, clamp down wireless network, lock your server physically and put scanning rpocedures in place
	+ securing a wireless network
	+ always use OpenSSH-related tools on your wireless LAN
- Wireshark is a example of a program that is useful for analyzing wireless traffic
http://www.wireshark.org
	+ keep in mind that it takes only a single rogue wireless access point hooked up to a legitimate network hub to open access to your entire systems
- Passwords and physical security
	+ force the use of a password with the GRUB bootloader, remove bootable devices such as floppy and CD-ROM or configure a network-booting server for ubuntu
	+ lock out access for former employees
- Configuring and using tripwire
	+ tripwire is a security tool that checks the integrity of normal system binaries and reports any changes to syslog or by email.
	tripwire is a good tool for ensuring that your binaries have not been replaced by trojan horse programs
	+ get it from ubuntu or the website http://www.tripwire.org	
- Devices, set a NIC to promiscuous mode (ifconfig's promisc option). it's good for monitoring traffic across the network
the tcpdump command also sets a designated interface to promiscuous mode while the program runs
the ifconfig command does not report this fact while tcpdump is running
use router is better than bridge to connect to internet which support filter package
- Viruses, it's easier to secure against viruses on linux
	+ linux never puts the current directory in your executable path, typing ls runs /bin/ls rather than any program named ls in the current directory
	+ a nonroot user can infect only the files that user has write access to which is usually only the files in the user's home directory, this is one of the most import reason for never using sudo when you dont' need to
	+ linux forces you to manually mark files as executable, so you can't accidentally run a file called myfile.txt.exe thinking it is just a text file
	+ by having more than one common web browser and email client, linux has strength through diversity: Virus writers cannot target one platform and hit 90 percent of the users
	+ example linux send virus-infected  on to windows, the linux based server would be fine but the windows client would be taken down
	+ consider a virus scanner for you machine
		* Clam AV
		http://www.clamav.net
		* others like
		Central Command, BitDefender, F-Secure, Kaspersky, McAfee for commercial solutions
- Configuring uncomplicated firewall, uncomplicated firewall(UFW) is installed in ubuntu by default.
UFW is run from the terminal
$ sudo ufw status
	+ useful command for UFW
	enable			  enable the firewall
	disable
	reload			  ensure changes are applied
	default allow|deny|reject   set default policy
	logging on|off	  toggles logging
	allow ARGS  adds allow rule
	deny ARGS   adds deny rule
	reject ARGS adds reject rule
	limit ARGS  adds limit rule
	delete RULE deletes rule
	status shows		firewall status
	status numbered	 shows firewall status as numbered list of rules
	status verbose	  shows verbose firewall status
	show REPORT		 shows fire wall report
	--version
	+ by default UFW is disabled
	+ example
	$ sudo ufw allow 80
	$ sudo ufw ssh
	$ sudo ufw allow from IP
	$ sudo ufw delete allow from IP
	+ graphic interface, called GUFW, could be installed
- AppArmor is a mandatory access control (MAC) system. it is less complicated than the better known SELinux( http://www.nsa.gov/researchselinux/ ), a MAC is created by U.S. National Security Agency(NSA)
	+ AppArmor is designed to limit what specific programs can do by restricting them to the use of predetermined resources.
	+ by default AppArmor does little, you can install some extra profiles from the ubuntu repositories by apparmor-profiles
	log issues in /var/log/messages
	+ unleash the power of AppArmor, you need to edit or create text files in /etc/apparmor.d, profiles are named for the application they restrict, including the full path to the application in the filesystem
	For example 
	sbin.syslogd
	shown here restricts the system logging daemon

	/dev/tty* w,
	/dev/xconsole rw,
	/etc/syslog.conf r,
	/sbin/syslogd rmix,
	
	r - read
	w - write
	ux - unconstrained execute
	Ux - unconstrained execute -- scrub the environment
	px - discrete profile execute
	Px - discrete profile execute -- scrub the environment
	ix - inherit execute
	m - allow PROT_EXEC with mmap(2) calls
	l - link
	
	+ Genprof is a program that helps you generate or update a profile. You supply the name of the executable
	$ sudo genprof google-chrome
	
	+ The program then writes a text file in /etc/apparmor.d using the name of the program and its path ( opt.google.chrome.google-chrome )
	
	+ commands you will use most often
	$ sudo service apparmor start
	$ sudo service apparmor stop
	$ sudo service apparmor reload | restart
	$ sudo service apparmor status
- forming a disaster recovery plan
	+ do not just pull the network cable out, this acts as an alert that the cracker has been detected
	+ only inform the people who need to know, may be one of employees behind the attack
	+ if the machine is not required and you do not want to trace the attack, you can safely remove it from the network
	+ take a copy of all log files on the system and store them somewhere else
	+ check the /etc/passwd file and look for users you do not recognize, change all the passwords and remote the bad one
	+ look in /var/www and see whether any web pages are there that should not be
	+ check the contents of the .bash_history files in the /home, are there any recent commands for your primary user
	+ if you have worked with external security companies previously, call then in for a fresh audit
	+ start collating backup tapes from previous weeks and months, to find when the attack actual succeeded
	+ download and install Rootkit Hunter from http://www.rootkit.nl/project/ , it's searches for the types of files that bad guys leave behind for their return
	+ keep your disaster recovery plan
- References 
http://www.insecure.org/nmap
http://www.tripwire.org
http://www.ubuntu.usn, offical ubuntu security notices list
https://help.ubuntu.com/community/InstallingSecurityTools
https://help.ubuntu.com/community/UFW
https://wiki.ubuntu.com/UncomplicatedFirewall
https://help.ubuntu.com/community/AppArmor
http://www.novell.com/linux/security/apparmor, Novell's excellent introduction to and documentation for AppArmor


# Performance Tuning
- Hard disk, before you undertake any under-the-hood work with linux
	+ perform a benchmark on your system, although linux is a rapidly changed system
	+ tweak only one thing at a time so you can tell what works
	+ always have a working boot disc handy, such as the live ubuntu CD or DVD
- Using the BIOS nad Kernel to Tune the Disk Drives, linux provides a limited means to interact with BIOS settings during the boot process, those commands are:
	+ idex=dma, forces DMA support to be turned on for the primary IDE bus where x=0 or the secondary bus where x =1
	
	+ idex=autotune, this command attempts to tune the interface for optimal performance
	
	+ hdx=ide-scsi, this command enables SCSI emulation of an IDE drive Required for some CD-RW drives to work properly in write mode, may provide some performance imporvements for regular CD-R drivers as well
	
	+ idebus=xx, this can be any number from 20 to 66, auto detection is attempted but this can set it manually if dmesg says that it isn't autodetected correctly or if have it set in the BIOS to a different value(overclockec). Most PCI controllers are happy with 33.
	
	+ pci=biosirq, some motherboards might cause linux to generate an error message say that you should use this, look in dmesg for it if you do not see it, you don't need to use it
	
	+ these options can be entered into /etc/lilo.conf or /boot/grub/grub.conf or GRUB2's /boot/grub/grub.cfg
- The hdparm Command can be used by root to set and tune the setting for IDE hard drives
$ hdparm command device
$ hdparm -tT /dev/hda
You must replace the /dev/hda with the location of your hard disk, a good IDE hard disk should be getting 400MBps to 500MBps for the first test, and 20MBps to 30MBps for the second, note your secore and try this
$ hdparm -m16 -d1 -u1 -c1 /dev/hda
That enables various performance-enhancing settings
Test again and if there is any increment run this command
$ hdparm -m16 -d1 -u1 -c1 -k1 /dev/hda
- File system tuning
	+ Synchronizing the File System with sync, linux uses buffers when writing to devices, write will not occur until the buffer is full, traditionally the command is given twice
	$ sync; sync
	Do it twice is overkill. Still it can be helpful before unmouting of certian types of media with slow write seconds( such as USB), but only because it delays the user from attempting to remove the media
	+ The tune2fs Command, you can adjust the tunable file system parameters on an ext2 or ext3 file system
		* disable file system checking with -c 0
		* interval between forced checks -i option
		* set the reserved blocks percentage with lower value, -m, freeing more space at the expends of fsck having less space to write any recovered files
		* decrease the number of superblock to save space with -0 sparse_super
		* more space can be freed with the -r option than sets the number of reserved blocks
	most of these uses of tune2fs free up space on the drive at the expense of the capability of fsck to recover data
	
	+ The e2fsck Command, utility checks an exet2/exet3 file system
		* check for bad blocks and then marks them as bad with -c
		* forces checking on a clean file system, -f
		* verbose mode, -v
		
	+ badblocks command, check detail from the man manual, it's for the file system experts or for file system hacking
	
	+ disable file access time, this is important for web server. if you are getting 50 request a second, your hard disk will be updating atime 50 times a second. to disable this with 
	$ chattr -R +A /path/to/directory
	changes file ssytem attributes of which "don't update atime" is one
	
- Kernel
as the linux kernel developed over time developers sought a way to fine-tune some of the kernel parameters, before sysctl those parameters had to be changed in the kernel configuration and then the kernel had to be recompiled, /proc file system. which is a "virtual window" intot the running kernel. As root user we can read values from and write values to those files
	+ some of the kernel parameters
	matthew@seymour:~$ sysctl -A
	net.ipv4.tcp_max_syn_backlog = 1024
	net.ipv4.tcp_rfc1337 = 0
	net.ipv4.tcp_stdurg = 0
	net.ipv4.tcp_abort_on_overflow = 0
	net.ipv4.tcp_tw_recycle = 0
	net.ipv4.tcp_syncookies = 0

	+ change a value with
	$ sysctl -w net.ipv4.tcp_retries 2=20
	
	+ if a particular setting is useful, you can enter it into the /etc/sysctl.conf file
	net.iv4.tcp_retries 2=20
	
- Apache
It's the most popular web server on the internet
	+ Slashdot.org, is a popular geeek news website that spawned the Slashdot Effect, the result of thousands of geeks descending on an unsuspecting website simultaneously
	Reddit.com
	+ configure apache2.conf file in /etc/apache2 the more module you load more resources are ocupided by apache
	Some of these modules can be uninstalled entirely through the Add or remove packages dialog
	+ you almost certenly need mod_mime and mod_dir also mod_log_config, you might also need mod_negotiation (a speed killer if there ever was one), mod_access( a notorious problem ). 
	+ make sure mod_deflate or mod_gzip enabled, your bottleneck is almost certainly going to be your bandwidth rather than your process power. this module compressing 10kb of html inot 3kb for supported browsers
	+ ensure that keepalives are turned off, this adds some latency to people viewing your site because the cannot download multiple files through the same connection however this reduces the number of simultaneous open connections
	+ if you are serving content that does not change, you can take the extreme step of enabling MMAP support, when you do change your pages, you need to restart apache
	Look for the EnableMMAP directive
	+ finally if speed is your greatest concern, you should do all you can to ensure that your content is static, avoid php if you can, avoid database if you can, and so on, if you know you are going to get hit by a rush of visitors, use plain HTML
	+ TIP
	some people recommend to optimizing apache to tweak HARD_SERVER_LIMIT in the apache source code and recompile. The default value is 256, which is enough to handle Slashdot Effect
- MySQL, turning your MySQL server for increase performance is exceptionally easy to do
	+ Measuring Key Buffer Usage
	log in to MySql and type SHOW STATUS LIKE '%key_read%'; this returns all the status fields that describe the hit rate of your key buffer. you should get two rows back, Key_reads and Key_read_requests, are the number of keys being read from disk and the number of keys being read from the key buffer
	you divide Key_reads by Key_read_requests, multiply the result by 100,  and then subtract the result from 100.
	1000, 100000, => 0.01 => * 100 get 1.0 => 100 - 1 => 99, means 99 percent read from RAM
	most people should be looking to get more than 95 percent
	
	+ If there are problems, check how much of your current key buffer is being used, Use SHOW VARIABLES command, look up the value of the key_buffer_size variable
	use SHOW STATUS command and look up the key_blocks_used
	Determine how much of you key buffer is being used by multiplying Key_blocks_used by 1024, dividing key_buffer_size and multiplying by 100. 
	
		* example Key_blocks_used is 8000, multiply that by 1024 then divide that by key_buffer_size(8388600) to get 0.97656 and finally multiply that by 100, 97.656, almost 98 percent of your key buffer is being used
		
		* allocate as much RAM to the key buffer as you can, up to a maximum of 25% of system RAM
		
		* Open /etc/my.cnf look for the line that contains key_buffer_size, if you don't have one, create a new one. It should be under the line [mysqld]
		try 16MB if there's no line already with
		[mysqld]
		set-variable = key_buffer_size=16M
		datadir=/var/lib/mysql
		
		* restart mysql with 
		mysqld restart
		
		* recheck the key_buffer_size with SHOW VARIABLES again
		* run your test system and check the performance
		* after your database has been access with normal usage for a short time, recalculate how much of the key buffer is being used. If you get another high score double the size again, restart
		* REMEMBER, you should never allocate more than 25% of system RAM to key buffer
	+ Using the query cache, big improve for the website read more than write
		* check query cache is enable by
		SHOW VARIABLES, look up the value of have_query_cache, look for the value of query_cache_size and query_cache_limit, first is how much RAM in bytes is allocated to the query cache, second is the maximum result size that should be cached, a good starting is 8388608(8MB) and 1048576(1MB)
		next type SHOW STATUS LIKE 'Qcache'; to see all the status information
		* change query size by /etc/my.cnf file and adding a line like 
		set-variable = query_cache_size=32M
		8MB query cache should be enough for most people, but larger sites might need 16MB or even 32MB if you are storing a particularly large amount of data
		* the cache is throw away after the connection closed
		
	+ Miscellaneous Tweaks
	MySQL has to open the file that stores the table data, how many files it keeps open at a time is defined by the table_cache setting, which is set to 64 by default. ubuntu limits on MySQL about how many files it can open at a time, Going beyond 256 is not recommanded, unless you have a DB-heavy site 
	
	Other thing you can tweak is the size of the read buffer, which is controlled by read_buffer_size and read_buffer_rnd_size, read_buffer_rnd_size should be three to four times the size of read_buffer_size
	
	+ Query Optimization
		* select as little data as possible
		* if you need only a few rows use LIMIT
		* declare fields as NOT NULL when creating tables to save space and increase speed
		* provide default values for fields, and use them where you can
		* be careful with table joins
		* if you must use join be sure to used the field are indexed, integer fields is better
		* find and fix slow queries, add log-long-format and log-slow-queries = /var/log/slow-queries.log to your /etc/my.cnf file under [mysqld]
		* use OPTIMIZE TABLE tablename to defragement tables and refresh the indexes
- References
http://www.coker.com.au/bonnie++/, the home page of bonnie, a disk benchmarking tools		
http://www.phoronix-test-suit-.com/, the Phoronix Test Suit was created by a website that does automated performance testing
http://dev.mysql.com, check the optimization 
Book references, High Performance MySQL, by Jeremy Zawodny and Derek Balling
	
	
# Kernel and Module Management
- the main reason today for people compiling their own kernel is because they want to learn to be a kernel developer
- ubuntu will not support custom build kernel
- only the kernel should be called as linux, ubuntu includes a kernel packaged with add-on software that interacts with kernel so that the user can interface with the system in a meaningful manner
- The linux Kernel, in 2011 the version moved to 3.0
- Linux Source Tree, the structure is important
	+ the /usr/src/linux-2.6 directory contain the .config and Makefile files among the others, there is no .config file by default; you must select one from the /configs subdirectory, copy the one appropriate and rename it .config
	
	+ the Documentation directory, The file 00-INDEX contains a list of the files in the main directory, and what they are. interesting documents
		* devices.txt, a list of all possible linux devices that are represented in the /dev directory, if you have ever gotten an error message that mentions char-major-xxx, this file is where that list is kept
		
		* ide.txt, if your system uses IDE hard drives, this file discusses how the kernel interacts with them and lists the various kernel commands

		* initrd.txt, this file provides much more in-depth knowledge of initial RAM disks
		
		* kernel-parameters.txt, a list of most of the arguments that you can pass at boot time to configure kernel or hardware setting
		
		* sysrq.txt, explain what that key on your keyboard marked SysRq is used for. help to recover from a system lockup. ubuntu disables this function by default for security reasons. Re-enable it by entering the command # echo "1" > /proc/sys/kernel/sysrq, disable it echoing a value of 0
		
	+ The directory named scripts contains many of the scripts that make uses
	+ make utility is a complex program, you can find complete documentation on the structure of the make files, http://wwww.gun.org/software/make/manual/make.html
	+ other directories are the source codes
	+ the interaction and control of hardware is handled by a small piece of the kernel called a device driver
- Types of Kernels
	+ monolithic kernels, these kernels compiled as a single block of code, in the early days
	+ modular kernels, module approach to building the kernel due to code grew and the number of devices that could be added to a computer
	+ typical ubuntu kernel has some drivers compiled as part of the kernel itself(called inline drivers) and others compiled as modules, on device drives compiled inline are available to the kernel during the boot process
	+ NOTE
	as a common example, drivers for SCSI disk drives must be available to the kernel if you intend to boot from SCSI disks. A way around this problem for modular kernels is to use a inital RAM disk(initrd), it loads a small kernel and the appropriate device driver, which then  can access the device to load the actual kernel you want to run
- Managing Modules, for each module check the documentation first, Linux provides the following module management tools for our use and modprobe.conf have man pages
	+ lsmod, lists the loaded modules
	+ insmod, loads the specified module into the running kernel, default location for running kernel is /lib/modules/*/, options -f which forces the module to be loaded
	+ rmmod, unloads(removes) the sepcified module more than one can be specified
	+ modprobe, a more sophisticated version of insmod and rmmod, uses the dependency file created by depmod and automatically handles loading or with the -r option removing modules. No force option, -t cause to cycle through a set of drivers unti it finds one that matches your system
		* unsure what module will work for your network card use
		$ sudo modprobe -t net
		use net because /lib/modules/*/kernel/net where all the network drivers are kept
	+ modinfo
	+ depmod, creates a dependency file for kernel modules, the file is /lib/modules/*/modules.dep, if you make changes to the file /etc/modprobe.conf, run depmod -a manually
	+ /etc/modprobe.conf, a file that controls how modprobe and depmod behave, it contains kernel module variables.
	The most common use is to alias a module and then pass it some parameters.  
		* example
		alias char-major-89 i2c-dev
		options eeprom ignore=2,0x50,2,2x51,2,0x52
		
		i2c-dev is a module used to read cpu temperature and fan speed which is a alias name of char-major-89
		
		* if ubuntu balks at loading a module, you could force it to load, which means it was compiled using a different kernel version
- When to recompile
	+ plain-vanilla linux, means normal linux kernel
	+ patched kernel, means modified version
	+ most user only need to recompile the kernel to do the following
		* accommodate an esoteric piece of new hardware
		* conduct a system update when ubuntu has not yet provide precompiled kernels
		* experiment with the system capabilities
- ubuntu supplies precompiled versions of the kernels for 32 and 64 bit processors
	+ generaic kernel works well for most users
	+ server kernel, optimized for server
	+ preempt kernel desing for in low latency servers
	+ rt kernel for times when instant response is more important
	+ virtual, available for use in virtual machines
	+ linux-backports-modules-, each with a specific set of kernel modules backported from newer mainline kernels into current version ubuntu kernels,if you need updated driver for a piece of hardware, look at the backported modules first
- Kernel Versions
	+ major version, now at 3
	+ minor version, now at 0
	+ sublevel number, indicates the current iteration of the kernel
	+ extraversion level, representing a collection of patches and additions made to the kernel by ubuntu engineers
	+ check current kernel version with 
	$ uname -r
- Obtaining the Kernel Sources
	+ install the linux-source package for ubuntu 
	
	+ get the latest vanilla version by open ftp to ftp.kernel.org, using your favorite FTP client and log in as anonymous, change directories to /pub/linux/kernel/v3.0
	use mirrors site for improve the download speed, http://www.kernel.org/mirrors/ has a list of all mirrors
	
	+ download an unpack it and move it to /usr/src or move it there
	create a symbolic link of linux-3.0 to linux-3.0.4
	$ sudo rm /usr/src/linux-3.0
	$ sudo ln -s /usr/src/linux-3.0.4 /usr/src/linux-3.0
	(otherwise some scripts will not work)
	By creating a symbolic link to /usr/src-3.0, it is possible to allow multiple kernel version to be compiled and tailored for different functions
	
	+ CAUTION, the correct symbolic link is critical to the operation of make
	
- Patching the Kernel, for those not using high-speed broadband connection
	+ example, currently v2.6.30 want to upgrade to 2.6.35, you must retrieve the 2.6.31 and 2.6.32 patch sets and so on
	these patches must be applied in succession to upgrade to 2.6.35
	+ use patch-kernel script located in the kernel source directory for the kernel version you currently use
	$ patch-kernel source_dir patch_dir stopversion
		* example the patch files have been downloaded from ftp.kernel.org and placed in the /patch directory in the source tree. Use this command to complete the patch
		$ sudo scripts/pathc-kernel /usr/src/linux-2.6.33 /usr/src/linux-2.6.33/patch
		each patch successive applied, eventually creating a 2.6.35 code tree
		if any errors files named xxx# or xxx.rej are created wehre xxx is the version of path that failed
	+ using the patch command, if you have a special nonstandard patch to apply such as a third-party for a commercail product, use the patch command rather than the special patch-kernel script, quick steps and an alternative method of creating patching code and leaving the orginal code alone:
		* create a directory in your home, such as mylinux
		
		* copy the pristine linux source code there with the following:
		$ cp -ravd /usr/src/linux-2.6/* ~/mylinux
		
		* copy the patch file  to that same directory 
		$ cp patch_filename ~/mylinux
		
		* change to the ~/mylinux directory with this command
		$ cp ~/mylinux
		
		* apply the patch like this
		$ -patch -p1 <patch_filename> mypatch.log 2>&1
		(this last bit of code saves the message output to a file so that you can look at it later)
		
		* if the patch applies successfully you are done and have not endangered any of the pristine source code. If the newly patched code doesnot work, you do not have to reinstall the original, pristine source code
		
		* copy your new code to /usr/src and make that special symbolic link described elsewhere in the chapter
- Compiling the Kernel	
	+ compiling the kernel, code => binary
	+ installing the kernel, all the compiled files in /boot and /lib and making changes to the boot loader
	+ Setps to compile and configure the kernel
		* do not delete your current kernel, back it up
		
		* apply all patches
		
		* backup the .config file, if it exists, so that you can recover from the inevitable mistake, use the following cp command
		$ sudo cp .config .config.bak
		
		* NOTE
		If you are recompiling the ubuntu default kernel, the /usr/src/linux-2.6/configs directory contains several versions of configuration files for different purposes
		ubuntu provides a full set of .config files in the subdirectory configus, all named for the type of system they were compiled for
		
		* run the make mrproper directive to perpare the kernel source tree, cleaning out any old files or binaries
		
		* restore the .config file that the command make mrproper deleted, and edit the makefile to change the EXTRAVERSION number
		
		* NOTE
		if you want to keep any current version of the kernel that was compiled with the same code tree, manually edit the makefile with your favorite text editor and add some unique string to the EXTRAVERSION variable
		
		* Modify the kernel configuration file using make config, make menuconfig, or make xconfig we recommend the last one
		
		* run make dep to create the code dependencies used later in the compilation process
		
		* If you have multiprocessor machine, you can use both processors to speed the make process by inserting -jx after the make command, x is one more than the number of processors you have. 
		
		* run make clean to prepare the sources for the actual compilation 
		
		* run make bzImage to create a binary image of the kernel
		
		* NOTE
		several choices of directives exist, the most common ones are the following
		zImage, This directive compiles the kernel, creating an uncompressed file called zImage
		bzImage, This directive creates a compressed kernel image
		bzDisk, This directive does the same thing as bzImage, but it copies the new kernel image to a floppy disk for testing purposes
	
		* Run make modules to compile any modules your new kernel needs
		
		* run make modules_install to install the modules in the /lib/modules and create dependency files
		
		* run make install to automatically copy the kernel to /boot, create any other files it needs and modify the boot loader to boot the new kernel by default
		
		* using your favorite text editor, verify the changes made to /etc/lilo.conf or /boot/grub/grub.conf; fix if necessary and rerun /sbin/lilo if needed
		
		* reboot and test the new kernel
		
		* repeat the process if necessary, choosing a configuration interface
	
	+ make config utility is a command-line tool, presents a question about kernel configuration options, user responds with a Y, N, M or ?(not case sensitive)
		* M to be compiled as a module
		* ? displays context help for that specific option
		* recommend avoid the make config utility
	+ make menuconfig, contain a simple graphic interface moving by keyboard, use arrow key to move the, space bar to toggle selection, tab key moves the focus at the bottom of the screen to either select, exit or help
	+ make xconfig, it's a true graphical interface, recommend to copy your kernel's .config file to /usr/src/linux-2.6 and run make xconfig from there
	
	* some kernel subsections for configuration
	Code maturity level options, enables development code to be compiled into the kernel, include obsolete or testing code
	General setup, contains how kernel talks to the BIOS whether it should support PCI or PCMCIA, whether it should use APM or ACPI and what kind of Linux binary formats
	Loadable module support, whether the kernel enables drivers and other nonessential code to be compiled as loadable modules which can be load and unload at runtime, which is a good idea to keeps basic kernel small
	Processor type and features, Several options dealing with architecture that will be running the kernel
	Power management options, dealing with ACPI and APM management features
	Bus options, Configuration options for the PCMCIA bus found in laptops and PCI hotplug devices
	Memory Technology Devices, Options for supporting flash memory devices, such as (MTD) EEPROMS
	Parallel port support
	Plug-and-play configuration, Options for supporting plug-and-play(PnP)PCI, ISA, and PnP BIOS support
	Block devices, section dealing with devices that communicate with the kernel in blocks of characters instead of streams, includes IDE and ATAPI devices connected via parallel ports, as well as enabling 
	ATA/IDE/MFM/RLL support, large collection of options to configrue the kernel to communicate using different types of data communication protocols to talk to mass storage devices
	SCSI device support, Options for configuring to support small computer systems interface, covers drivers for specific cards, chipset, tuable parameters ofr the SCSI protocol
	Old CD-ROM drivers, support obscure, older CD-ROM
	Multidevice support, enabling the kernel to support RAID devices in (RAID and LVM) software emulation support for logical volumn manager
	Fusion MPT device support, support for LSI's Logic Fusion Message Passing Technology, for high-performance SCSI nad LAN interfaces
	IEEE1394(firewire) support, Experimental support for firewire devcies
	I20 device support, supporting the intelligent input/ouput architecture
	Networking support
	Amateur radio support, support the AX25 protocol
	IrDA(infrared) support, infrared data association
	Bluetooth support, support for the bluetooth wireless protocol
	ISDN subsystem, support integrated servcies digital networks protocols and devices. 
	Telephony support, support for devices that enable the use of regular telephone lines to support VoIP applications, This sedction does not handle the configuration of modems
	Input device support, options for configuring Universal Serial Bus(USB) such as keyboards, mice and joysticks
	Character devices, devices that communicate to the server in sequential characters, this is a large subsection containing the drivers for several motherboard chipset
	Multimedia devices, such as video capture boards, tv cards and am/fm radio adapter cards
	Graphics support, VGA text console, video mode selections
	Sound
	USB support
	File system
	Additional device driver support, a section for third party patches
	Profiling support, aid in debugging and development
	Kernel hacking, whether the kernel will contain advanced debugging options
	Security options, NSA Security Enhanced Linux is enable
	Cryptographic options, support for cryptography hardware, (ubuntu patches not found in the vanilla kernel sources)
	Library routines, zlib compression support
	
- Creating an initial RAM disk image, named /boot/initrd.img, for special devices drivers to be loaded to mount the root file system, to create an initrd.img file use the shell script /sbin/mkinitrd
$ mkinitrd file_name kernel_version

it looks at /etc fstab, /etc/modprobe.conf and /etc/raidtab to obtain the information it needs to determine which modules should be loaded during boot
	
file name is the image file

$sudo mkinitrd initrd-2.6.7-1.img 2.6.7-1
- when something goes wrong 
	+ check error message from the screen
	+ check error message from /var/log/messages
- Errors during compile
	+ fix the error and recompile
	+ remove the offending module or option and wait for the errors to fixed by the kernel team
	+ info the linux mailing list, guideline for doing this are in the README file in the base directory of the kernel source under the section IF SOMETHING GOES WRONG:
- Runtime Errors, Boot loader problems and kernel Oops
	+ if you have GRUB2 problems, check the GRUB manual at http://www.gnu.org/software/grub/manual and https://help.ubuntu.com/community/Grub2
- References
https://wiki.ubuntu.com/Kernel, the starting point for anything you want to know about both ubuntu and its use of the linux kernel
http://www.tldp.org, the linux documentation project
http://www.minix.org, the unofficial minix websites, it contains a selection of links to minix and link to actual home page   


# File and print
- share file with unix NFS protocl and share printer with JetDirect
- CAUTION, ubuntu ships with all its network ports blocked, to share any thing, enable the relative port by use Uncomplicated Firewall(UFW)
- using the network file system, NFS, is the protocol developed by Sun Microsystems. A common use of NFS is to allow user's /home directories to appear on every local machine they use, share binary files between similar computers
- installing and starting or stopping NFS, need install the nfs-kernel-server package, consists of several programs that work together
	+ portmap, which maps NFS request to the correct daemon
	+ nfsd, which is the NFS daemon 
	+ mountd, controls the mouting and unmounting of file system
	+ ubuntu automatically adds NFS to the system startup scripts, check the server status
	$ sudo /etc/init.d/nfs-kernel-server status
	
	manually start the server
	$ sudo /etc/init.d/nfs-kernel-server start
- NFS server configuration
	+ configure the NFS server by the /etc/exports file, similar to the /etc/fstab file, entryies looks like:
	/file/system yourhost(options) *.yourdomain.com(options) 192.168.0.0/24(options)
	 
	+ options, /etc/fstab
	rw  read write
	ro  read only
	async   writes data when the server not the client feels the need
	sync	writes data as it is received
	
	+ example
	# /etc/exports: the access control list for filesystems which may be exported
	# to NFS clients. See exports(5).
	/home/matthew 192.168.0.0/24(rw,no_root_squash)
	
	this file exports(makes available) /home/matthew to any host in 192.168.0.* and allows users to read from and write to /home/matthew
	
	exports all the file systems in the /etc/exports file to a list named xtab under the /var/lib/nfs directory
	$ sudo exportfs -a
	
	the -r option stands for re-export and tells the command to rereads the entire /etc/exports file and (re)mount all the entries

	+ use exportfs to export a file temporarily
	$ /usr/sbin exportfs -o async yourhost:/usr/tmp
	
	+ support use shares-admin graphical client to set up NFS while using the X window system
- NFS Client configuration, need nfs-common package installed, edit the /etc/fstab file as you would to mount any local file system
	+ edit the /etc/fstab file as you would to mount any local file system
	+ instead of using a device name to be mounted (such as /dev/sda1) enter the remote hostname and the desired file system to be imported, one entry might look like this
	yourhost:/home/share	/export/share   none nfs 0 0
	If you use autofs on you system, you need to use proper autofs entries for your remote NFS mounts
	
	+ use mount command, as root to quickly attach a remote directory to a local file system by using a remote host's name and exported directory.
	$ sudo mount -t nfs 192.168.2.67:/music /music
	
	use the df command to check the mount directory

	+ use umount command to remove the remote file system, if you specify the / as a mount point then you need to reboot to unmount it
- Putting Samba to work, Session Message Block (SMB) protocol to enable the windows operating system to access linux files, with the help of Samba you could make your ubuntu machine look just like a windows computer to other windows computers
ubuntu access window network just like windows, most work is done
	+ Samba Web Administration Tool(SWAT) can be used to configure samba from a web browser, Samba is licensed under the GPL and is free

	+ Source code http://www.samba.org/, unpack the files, run the command ./configure, then run make, make test(if you want), make install
	
	+ build-essential, package is automatically install all the make tools plus other development utilities that you need to compile software from source code

	+ suggest to install samba-doc and sambda-doc-pdf too, after install find it at /usr/share/doc/samba-doc or /usr/share/doc/samba-doc-pdf

	+ configure file location, /etc/samba/smb.conf
	if you need is complex, check http://sambda.org/samba/docs/man/Samba3-HOWTO/
	
	+ manually configuring samba with /etc/samba/smb.conf, it broken into sections, each section is a description of the resource shared(share) should be title appropriately, three special are
		* [global]
		* [homes]
		* [printers]
	each section name should be named for the resource being shared. for example, if the resource /usr/local/programs is being shared, you should call the section [programs], when windows sees the share, it is called by whatever you name the section(programs in this example)
	[programs]
	path=/usr/local/programs
	writeable=true
	
	other parameters you could set
		* requireing a user to enter a password
		* limiting the hosts allowed to access the shared directory
		* altering permissions users are allowed to have on directory
		* limiting the time of day during which the directory is accessible
		* individual sections setting will overwrite the global setting
		* example
		[programs]
		path=/usr/local/programs
		writeable=true	
		valid users=mhelmke
		browseable=yes
		create mode=0700

		every other user could browse the files and if they create a file the other user don't permit to see due to the browseable=yes parameter
		
	+ sharing home directories using the [homes] section, to avoid linux home setting  to display for the window user, use a separate "home" to act as their windows /home directory
	
	[homes]
	comment = Home Directories
	path = /home/%u/share
	valid users = %S
	read only = No
	create mask = 0664
	directory mask = 0775
	browseable = No
		
	+ printer section
	[printers]
	comment = Ubuntu Printers
	browseable = no
	printable = yes
	path = /var/spool/samba
		
- Test samba with the testparm command, after you edit the configuration file.
$ sudo testparm /path/to/smb.conf.back-up
replace the configure file and restart samba
$ /etc/init.d/smbd restart
- starting stopping and restarting the smbd daemon, with -s option allows you to change the default samba configure location
$ sudo /etc/init.d/samba start
- using the smbstatus command reports on the current status of your samba
$ /usr/bin/smbstatus [options]		
	+ options
	option -b  brief output
	option -d  verbose output
	option -s /path/to/config  used if the configuration file used at startup is not the standard one
	option -u username   shows the status of a specific user's connection
	option -p  list current smb processes, which can prove to be useful in scripts
- connecting with the smbclient command allows users on other linux hosts to access your smb shares, most frequently is -I, you cannot mount the share on your host, but you can use it in a way similar to FTP
$ smbclient -I 10.10.10.20 -Uusername%password

- mounting samba shares, mounting a share is the same as mounting an available media partition or remote NFS directory except that the samba share is accessed using SMB
$ sudo mount -t smbfs //10.10.10.20/homes /mount/point -o username=heather, dmask=777, \ fmask=777
this command mounts heather's /home directory on your host and gives all users full permissions to the mount, the permissions are equal to the permissions on the chmod command

the second method produces the same results using the smbmount command
$ sudo smbmount //10.10.10.20/homes /mount/point -o username=heather,dmask-777,\fmask=777
		
unmount the share
$ sudo umount /mount/point 
		
- configuring samba using SWAT, Samba Web Administration Tool(SWAT), need enable root account
	+ NOTE
	enable root account by giving it a password by using the command sudo password root
	
	+ make sure you have the samba and the swat packages installed
	
	+ enable SWAT access to your system by editing the /etc/inetd.conf file, by remove the #<off># comments if present
		
	+ save the file and restart the openbsd_inetd daemon using the following command
	$ sudo /etc/init.d/openbsd-inetd restart
	
	+ start X session, launch Firefox and browse to http://localhost:901
	
	+ click global icon in SWAT's main page, set global settings
	
	+ create a samba user and set the user's password, the user name must be an existing system suer, but the password for samba access does not have to match the existing
- configuring samba with System, Administration, Shared Folders   
- network and remote printing with ubuntu		
	+ enable network printing on a LAN, use the system-config-printer client to create a new printer, available in the menu at system, administration printing
	+ install any printers you have to the servers
	+ open server, settings and enable publish shared 
	+ right-click any printer's icon and select share, done
	+ enable share manually
		* edit your /etc/cups/cupsd.conf file
- session message block printer, create a local printer entry to print to a remote shared printer using SMB, Common Unix Printing System(CUPS)
	+ use CUPS to configure samba to use your printers by editing the smb.conf file
- using the common unix printing system gui, launch a browser and browsing to http://localhost:631 CUPS provide a web-based administration interface  
	+ check detail information at
	http://www.cups.org/
- avoiding printer support problems
	+ check the www.linuxprinting.org/vendors.html to see the printer list supported in linux. include all-in-one (print/fax/scan) devices
	+ multifunction printer HP OfficeJet Linux driver at  http://hpoj.sourceforge.net/
	+ using usb and legacy printers, check http://www.linux-usb.org/
	+ related commands to manage printing services
	accept, controls print job access to the CUPS server 
	cancel, cancel a print job
	disable, control printing from the command line
	enable, controls CUPS printers
	lp, sends a specified file to the printer and allows control of the print service
	lpc, displays the status of printers and print service at the console
	lpq, views print queues(pending print jobs) at the console
	lprm, removes print jobs from the print queue via the command line
	lpstat, displays printer and server status
- References
https://help.ubuntu.com/community/SettingUpNFSHowTo, for settiing up NFS
https://help.ubuntu.com/community/SettingUpSamba
https://help.ubuntu.com/community/Swat
http://www.samba.org/, more information about Samba	
http://www.linuxprinting.org/
http://www.cups.org/
http://www.wpg.org/ipp/, home page for the internet printing protocol standards
	
	
# Apache Web Server Management 
- webserver statistics
http://news.netcraft.com/archives/2011/07/08july-2011-web-server-survey.html
- documentation,
	+ online
	http://httpd.apache.org/docs/2.2/faq/.  
	+ local installed 
	http://localhost/manual/index.html
- determine the precise version of apache with your system
$ /usr/sbin/apache2 -V
- installing the apache server
	+ download it and shutdown the current server
	+ installing from ubuntu repositories
	+ NOTE
	check apache site for security reports, browse to http://httpd.apache.org/security_report.html for links to security vulnerabilities
	http://httpd.apache.org/mail/
	+ CAUTION
	don't install experimental packages on production machine, very carefully test the packages beforehand that is not connected to a newwork
	+ apache package installs files in the following directories
	/etc/apache2, contains the configuration file, apache2.conf
	/etc/init.d, contains the ysstem startup scripts
	/var/www, the package installs the default server icons, common gateway interface(CGI) programs, and html files in the location
	/usr/share, apache documentation
	/usr/share/man, ubuntu's apache package also contains manual pages
	/usr/sbin, executable programs are placed in this directory
	/usr/bin, some of the utilities from the apache package arepalced here
	/var/log/apache2, the server log files
	when apache is being run, it also creates the file apache2.pid, containing the process ID of Apache's parent process in the /var/run/directory
	
	+ building the source yourself
		* download from, http://www.apache.org/
		* extract the source code into /tmp dirctory
		* there are two ways to compile
		one by editing makefile templates, and the new easy way using a configure script
		* TIP
		many software packges distributed in source code form for linux and other UNIX-like operation systems, extracting the source code results in a directory that contains a README and an INSTALL file. Be sure to peruse the INSTALL file
		* using ./configure to Build Apache
		~/usr/src/apache2$ sudo ./configure --prefix=/preferred/directory/
		This generate the makefile that is used to compile the server code
		* type make to compile the server code
		* make install to install
		* TIP
		a safer way to install a new version of Apache from source is to use the ln command to create symbolic links of the existing file location to the new locations of the files. 
		
		Another safe way to install a new version is backup any important configuration and files, then use apt-get to remove the server, and then install
		
		No "uninstall" option is available when installing Apache from source!!!
- Apache file locations after a build and install
	+ /usr/local/apache (or whatever directory you specified with the --prefix parameter)
	+ /usr/local/apache/conf, contains several subdirectories and the apache configuration file, httpd.conf
	+ /usr/local/apache, the cgi-bin, icons, and htdocs contain the CGI programs stand icons and default html docs
	+ /usr/local/apache/bin, the executable programs 
	+ /usr/local/apache/logs, log files
- A quick guid to getting started with apache
	+ make sure apache is installed
	+ set up a home page for your system by editing (as root) the file named index.html under /var/www directory
	+ start apache through the services window(under system administration from the menu bar)
	or with this command
	$ sudo /etc/init.d/apache2 start 
	+ for security reasons, you should not start and run Apache as root if you host is connected to the internet or company intranet, Apache set to run as the user and group www-data no matter how it is started (setting in /etc/apache2/apache2.conf)
	the user defined in /etc/passwd
- Starting and Stopping Apache
	+ set the apache server start during the system boot process reference the boot section
	+ start manually, by apache2 command (an executable file), check the help with
	$ apache2 -h
	
	+ check your configuration file before restart the server
	$ apache2 -t
	
	there is a bug in the internal username settings for apache2 in ubuntu, use this for apache2 in ubuntu that gives you the bad user name error for run test
	$ sudo APACHE_RUN_USER=www-data APACHE_RUN_GROUP=www-data apache2 -t

	+ NOTE
	when you build and install apache from source, manually start the service for two reason
		* standalone server uses the default HTTP port(port 80), only the super user can bind to internet ports that are lower than 1024
		* only processes owned by root can change their usr ID(UID) and group IP(GID) as specified by Apache's user and Group directories. If you start the server under another UID, it runs with the permissions of the user starting the process
		* start apache with sudo only for testing and building
- using /etc/init.d/apache2, the script is used to control the startup and shutdown of various services, the actual work is done by apache2ctl shell script included with Apache
	+ /etc/init.d/apache2 is a shell script 
	+ most common options to include with the apache2 script   
	start, stop, reload, restart, status
	+ features
	help
	force-reload, restarts the server while forcing it to reload configuration files, use this for you are making many changes to the various server configuration file
- runtime server configuration settings, stored in just one file apahce2.conf, which is found under the /etc/apache2 directory, apache supports nearly 3000 configuration directives, using
	directive option option ...
	+ reload configuration file by
	$ /etc/init.d/apache2 reload
	
	+ some pecial directives called sections, look like html tags
	<Directory somedir/in/your/tree>
		directive option option
		...
	</Directory>
	
	+ apache is configured with an alias that lets you view the documentation installed in /usr/share/doc using your web browser at localhost/manual

	+ editing apache2.conf
	Some of the configuration maight want to change concerning operation of your server
		* ServerRoot
		used to set absolute path to your server directory, if you install from package ServerRoot should be /etc/apache2, or /usr/local/apache, apache2 -V shows it
		
		* Listen
		in a file called ports.conf, included from apache2.conf and indicates on which port you want your server to run
		
		* User and Group
		should be set to the UID and GID the server use to process request, user www-data and group www-data 
		
		If run server with other UID and GID, the server and CGI will get the permission of that user
		
		also could specify user and group using UID and GID numbers, example:
		User apache
		Group apache
		
		User #48
		Group $48

		* TIP 
		if you find a user on your system(other than root) with a UID and GID of 0, then your system has been compromised by a malicious user
		
		* ServerAdmin
		set to the address of the webmaster managing the server, should be a valid email or alias
		
		* ServerName
		sets the hosut name the server will return, set it to a fully qualified domain name(FQDN) for example www.your.domain rather than simply www.

		* DocumentRoot
		Set this directive to the absolute path of your document tree

		* UserDir
		disables or enables and define the directory(relative to a local user's /home) where that user can put public HTML documents
		The default setting for this directive , if enabled, is public_html. Each user can create a directory called public_html under her /home, http://servername/~username

		* DirectoryIndex
		which file should be served as the index for a directory, such as which file should be served if the URL http://servername_/SomeDirectory/ is requested
		It is often useful to put a list of files here. Use a CGI run as default action in a directory or index.html.
		
- Apache Multiprocessing Modules, (MPMs)
apache can use only one MPM at any time, this enable apache work much better on a wider variety of computer platforms
the internal MPM modules relevant for linux include the following
	+ mpm_common, a set of 20 directives common to all MPM modules
	+ prefork, a nonthreaded, preforking web server that works similar to earlier versions of Apache
	+ worker, provides a hybird multiprocess multithreaded server
	+ NOTE
	other MPMs are available for apache related to other platforms, such as mpm_netware for NetWare hosts and mpm_winnt for NT platforms
- Using .htaccess Configuration files
apache also supports special configuration files, known as .htaccess files. Almost any directive that appears in apache2.conf can appear in an .htaccess file, This specified in AccessFileName directive in apache2.conf
Specify which configuration the .htaccess files can override, use the AllowOverride directive, can be set globally or per directory
	+ example
	<Directory />
		Options FollowSymLinks
		AllowOverride None
	</Directory>
	+ Options Directives
	Can be None; All; or any Combination of Indexes, Includes, FollowSymLinks, ExecCGI and MultiVews, MultiViews are not included in All, must be specified explicitly
		* Indexes, in the absence of a index.html file, listing of the files in directory is generated as a html page for display to the user
		* Includes, Server-side includes(SSIs) are permitted in this directory also be written as IncludesNoExec, allow includes but don't want to allow the exec option, For security reasons, this usually a good idea in directories over which you don't have complete control
		* MultiViews, this is part of the mod_negotiation module, when a client request a document that can't be found, the server tries to figure out document best suits the client's requirement
	+ AllowOverrides Directives
	You can set this directive individually for each directory, this capability is particularly useful for user directories where user does not have access to the main server configuration files
- File System Authentication and Access Control
	+ Allowing individual users to put web content on your server poses several important security risks
	http:///wwww.w3.org/Security/Faq/www-security-faq.html
	+ restricting Access with allow and deny
	One of the simplest way to limit access is specific group of users base on IP or hotstnames
	Default behavior of Apache is to apply all the deny directives first and then check the allow directives. you can use the order statement to change this
		* Order deny, allow   The default, not specify deny then allow
		* Order allow, deny   not specify allow then deny
		* Order mutual-failure	  Only specified in the allow and not in the deny 
		* example
		<Location /server-status>
			SetHandler server-status
			Order deny,allow
			Deny from all
			Allow from gnulix.org
		</Location>
	+ Authentication
	Ensuring visitors really are who they claim to be, there are several metods of authentication
		+ basic, supply a username and a password
			* CAUTION
			do not use /etc/passwd as a user list for authentication
			
			* Create a user file for Apache use the htpasswd command, it is include with the Apache Package. in /usr/bin, running htpasswd without any options produces the help message
			Usage:
			htpasswd [-cmdps] passwordfile username
			htpasswd -b[cmdps] passwordfile username password
			htpasswd -n[mdps] username
			htpasswd -nb[mdps] username password
			-c Create a new file.
			-n Don’t update file; display results on stdout.
			-m Force MD5 encryption of the password.
			-d Force CRYPT encryption of the password (default).
			-p Do not encrypt the password (plaintext).
			-s Force SHA encryption of the password.
			-b Use the password from the command line rather than prompting for it.
			-D Delete the specified user.
			you can also create user gourp files. The format of these files is similar to that /etc/groups. On each line, enter the group name, followed by a colon(:) and then list all users
			gnulixusers: user0 user1 
			
			* Point Apache to the user file, use the AuthUserFile directive. it takes the file path to the user file as its parameter
			relitive path start with a / relative to the ServerRoot
			
			* AuthType directive to set the type of authentication
			
			* specify which realm the resource will belong, Realms are used to group different resources that will share the same users for authorization
			The realm is defined with the AuthName directive
			
			* state which type of user is authorized to use the resource. 
			if you specify valid-user as an option, any user in the user file is allowed to access the resource
			you can specify a list of users who are allowed access with the users option
			you can specify a list of groups with the group option
			
			* example
			<Location /server-status>
				SetHandler server-status
				AuthType Basic
				AuthName “Server status”
				AuthUserFile “gnulixusers”
				Require valid-user
			</Location>
		+ Final words on Access control
		If you have host-based as well as user-based access protection on a resource, default behavior of Apache is to require the requester to satisfy both controls. But assume that you want to mix host-based and user-based protection
		You can set the satisfy directive to All(this is the default) or Any. 
		<Location /server-status>
			SetHandler server-status
			Order deny,allow
			Deny from all
			Allow from gnulix.org
			AuthType Basic
			AuthName “Server status”
			AuthUserFile “gnulixusers”
			Require valid-user
			Satisfy Any
		</Location>
- Apache Modules, apache gains its functionality from modules. Each module solves a well-defined problem by adding necessary features. Nearly 70 core modules are included 
	+ relative address
	http://modules.apache.org/
	+ modules are stored in the /usr/lib/apache2/modules directory
	+ each module adds new directives that can be used in your configuration files
	+ modules
	httpd.exp				mod_cgi.so			mod_mime_magic.so
	mod_actions.so			mod_charset_lite.so	mod_mime.so
	mod_alias.so			mod_dav_fs.so		mod_negotiation.so
	mod_asis.so				mod_dav_lock.so		mod_proxy_ajp.so
	mod_auth_basic.so		mod_dav.so			mod_proxy_balancer.so
	mod_auth_digest.so		mod_dbd.so			mod_proxy_connect.so
	mod_authn_alias.so		mod_deflate.so		mod_proxy_ftp.so
	mod_authn_anon.so		mod_dir.so			mod_proxy_http.so
	mod_authn_dbd.so		mod_disk_cache.so	mod_proxy_scgi.so
	mod_authn_dbm.so		mod_dumpio.so		mod_proxy.so
	mod_authn_default.so	mod_env.so			mod_reqtimeout.so
	mod_authn_file.so		mod_expires.so		mod_rewrite.so
	mod_authnz_ldap.so		mod_ext_filter.so	mod_setenvif.so
	mod_authz_dbm.so		mod_file_cache.so	mod_speling.so
	mod_authz_default.so	mod_filter.so		mod_ssl.so
	mod_authz_groupfile.so	mod_headers.so		mod_status.so
	mod_authz_host.so		mod_ident.so		mod_substitute.so
	mod_authz_owner.so		mod_imagemap.so		mod_suexec.so
	mod_authz_user.so		mod_include.so		mod_unique_id.so
	mod_autoindex.so		mod_info.so			mod_userdir.so
	mod_cache.so			mod_ldap.so			mod_usertrack.so
	mod_cern_meta.so		mod_log_forensic.so	mod_version.so
	mod_cgid.so				mod_mem_cache.so	mod_vhost_alias.so

	+ enable a module, need actual name of the module not the filename
	$ sudo a2enmod module_name
	
	+ disable a module, need actual name of the module not the filename
	$ sudo a2dismod module_name
	
	+ don't forget to restart apache after change any setting
- Virtual Hosting	
Apache now can dynamically host virtual servers by using the mod_vhost_alias module you read about in the preceding section of the chapter.
The module is primarily intended for ISPs and similar large sites that host a large number of virtual sites, Introduce Traditional ways of hosting virtual servers. Check detail from the apache documentation
	+ Address-Based Virtual Hosts
	Configured your linux machien with multiple IP address, setting up apache to serve them as different websites is simple. only put a VirtualHost directive in your apache2.conf file, for each addresses you want to make an independent website
	<VirtualHost 212.85.67.67>
		ServerName gnulix.org
		DocumentRoot /home/virtual/gnulix/public_html
		TransferLog /home/virtual/gnulix/logs/access_log
		ErrorLog /home/virtual/gnulix/logs/error_log
	</VirtualHost>
	
	use IP address rather than the host name, you can specify any configuration directives within the <VirtualHost> tags. for example, set AllowOverrides directives differently for virtual hosts than you do for your main server.
	
	+ Name-Based Virtual Hosts
	Enable you to run more than one host on the same IP address, you must add the names to your DNS as CNAMEs of the machien in question

		* NOTE
		some older browsers cannot name-based virtual hosts because this is a feature of HTTP 1.1
		
		* Name-based virtual hosts require just one step more than IP address-based virtual hosts
		You must first indicate which IP address has the multiple DNS names on it. This is done with the NameVirtualHost directive:
		NameVirtualHost 212.85.67.67
		
		Then have a section for each name on that address, setting the configuration for that name. as with IP-based virtual hosts, you need to set only those configuration that must be different for the host, You must set the ServerName directive because it is the only thing that distinguishes one host from another
		<VirtualHost 212.85.67.67>
			ServerName bugserver.gnulix.org
			ServerAlias bugserver
			DocumentRoot /home/bugserver/htdocs
			ScriptAlias /home/bugserver/cgi-bin
			TransferLog /home/bugserver/logs/access_log
		</VirtualHost>
		<VirtualHost 212.85.67.67>
			ServerName pts.gnulix.org
			ServerAlias pts
			DocumentRoot /home/pts/htdocs
			ScriptAlias /home/pts/cgi-bin
			TransferLog /home/pts/logs/access_log
			ErrorLog /home/pts/logs/error_log
		</VirtualHost>
	+ TIP
	use ServerAlias directive to list all valid aliases for the machine for VirtualHost to avoid redirect to wrong host
	ServerAlias ServerName
	  
- logging
logging can help with, and are saved at /var/log/apache2
	+ help to
		* tracking system resource for system resource management
		* intrusion detection
		* diagnostics, by recording errors in processing requests
	
	+ add new log format by LogFormat directive
	LogFormat "%h %l %u %t \"%r\" %>s %b" common
		  
	The common log format is a good starting place for creating your own custom log formats
	You can put a conditional infront of each variable to determine whether the variable is dispalyed
	for example, %!401u, displays the value of REMOTE_USER unless the return code is 401
	
	+ Specify the location and format of a log file using CustomLog
	CustomLog logs/access_log common
	
	if it is not specified as an absolute path, the location of the log file is assumed to be relative to ServerRoot
	

# Other HTTP Servers
- Nginx, pronounced "engine-x" a lightweight and extremely fast web server, some well-known websites such as WordPress, GitHub and SourceForge use Nginx
nor as well documented in English since it originates from Russia, it's excellent option and is quite easy to setup and use
C10K, tens of thousands of clients connected simultaneously each one making HTTP request that must be responded to 
	+ The C10K problem, canonical website for learning more about the problem, http://www.kegel.com/c10k.html

	+ Newer versions of the Apache and other modern web servers rely on a concept of threads Threads are kind of like lightweight process. 
	Thread-based servers are great because request can share memory, making them more efficient and thereby able to serve more requests more quickly
	However, a crash in one thread could bring the entire server down
	
	+ What makes Nginx different is that it uses an event-driven architecture to handle requests, instead of starting a new process or a new thread for each request. 
	There are some significant difficulties inherent in using this method. make fixing bugs, adding features and understanding code as a newcomer more difficult
	For configuration Nginx uses a system of virtual hosts
	One missing file that many people use and love in Apache is .htaccess. There is nothing similar in Nginx, if you need the ability to make changes to rewrites or configurations you need to restart the server
	
	+ well-document option is to use both Apache and Nginx together, where Apache handles any and all dynamic request and Nginx handles all static request
	
- lighttpd
Open-source lightweight server is lighttpd, or "lighty", designed for high-performance and low resource use. YouTube, Wikipedia and others are using lighttpd for its scalability, uses an event-driven architecture
	+ Compare to Nginx, lighttpd is more well supported CGI content such as websites built with PHP
	+ Configuration lighttpd is again done using a system of configuration files
	+ lighttpd ahs a nice website with professional documentation
	
- Yaws
stands for Yet Another Web Server. It is written in the Erlang programming language. Erlang is primarily designed for and used to build scalable real-time systems often found in telephony applications, instant messaging, commerce and banking and is designed to support concurrency and fault tolerance.
	+ Configure with one configuration file
	+ A good place for learning but not for host vital content currently, not very well document

- Cherokee
Cherokee claims both speed and lightness is its ease of configuration. Supports all big features, like virtual hosts, CGI, load balancing and so on. Also includes a graphic interface for configuration
Setting up Cherokee by set cherokee-admin-launcher command as root, and use a graphical user interface on the system as web server, which is something that most linux admins do not do
More friendly support GUI for your administration tasks

- Jetty
If you are hosting a personal WordPress site or a few static HTML pages, Jetty is not what you need. Jetty is a Eclipse Foundation project that is written in java
features like , google web toolkit, yahoo's hadoop, and works with Apache Maven to provide a way to run a web application locally 

- thttpd
it's a very light, decidedly not flashy or feature-filled web server. not frequently, support limit bandwidth

- Apache Tomcat, java servlet container
- References
http://www.acme.com/software/thttpd/, the main website for thttpd
http://www.lighttpd.net, the main website for lighttpd


# Remote File Serving with FTP
- File Transfer Protocol(FTP), introduce FTP software included with Ubuntu, Ubuntu also includes an FTP server software package named vsftpd
- Choosing an FTP Server
- There are two types of FTP servers and access, anonymouse and standard.
	+ A standard FTP server requires an account name and password 
	+ Anonymouse servers allow anyone to connect to the server to retrive files
- NOTE
many linux users now use OpenSSH and its suite of clients such as sftp command	
- Choosing an authenticated or anonymouse server
- ubuntu FTP Server Packages, vsftpd
- Other FTP Servers
	+ NcFTPd, http://www.ncftp.com, provides its own optimized daemon
	+ ProFTPD, license under the GNU GPL
	http://www.proftpd.org
	+ use Apache(and HTTP) for serving files
- Installing ftp software
	+ check the client software for FTP
	$ dpkg --get-selections | grep ftp | sort
	+ install vsftp from the ubuntu repositories
	+ a FTP user is created after install vsftp server, FTP user entry in /etc/passwd looks like
	ftp:x:116:124:ftp daemon,,,:/srv/ftp:/bin/false
	/bin/false is not a shell which is a program usually assigned to an account that has been locked
	+ NOTE
	the ftp user is applies to anonymous FTP configurations and server setup, Our FTP user is configured to use /srv/ftp as default direcotry
	+ shadowed password system, can add additional level of security to ubuntu, which is default installed
	+ as root inspection of the /etc/shadow file shows, it is not possible to log in to this account denoted by the * as the password
	# cat /etc/shadow
	bin:*:11899:0:99999:7:::
- Quick and Dirty FTP Servers, on security line
	+ ensure FTP server package is installed
	+ networking is enabled
	+ firewall rules on the server allow FTP
	+ if anonymous access is desired, create and populate the /srv/ftp/public directory, do this by mounting or copying your content, don't use symlinks
	+ edit and then save the appropriate configuration file, such as /etc.vsftpd.conf for vsftpd
	+ start or restart the FTP server like
	$ sudo service vsftpd restart
	+ check the server status
	$ status vsftpd
- Configuring the very secure FTP Server, which does not enable use of encrypted usernames or password
main configuration file is vsftpd.conf, which resides under the /etc directory
The configuration file installed by unbuntu allows local users to log in and then access their /home directory. usernames and passwords are passed without encryption over a network. To prevent rish and allow anonymous login need to edit the configuration file
edit the config file at /etc/vsftp.conf
# Allow anonymous FTP? (Disabled by default)
anonymous_enable=NO
#
# Uncomment this to allow local users to log in.
local_enable=YES
- Controlling anonymous access, toggle enable anonymouse access by changing vsftpd.conf file and changing related entries to YES or NO in the file to control how the server works
	+ anonymous_enable, disable by default
	+ anon_mkdir_write_enable
	+ anon_other_write_enable
	+ anon_upload_enable
	+ anon_world_readable_only
after make any changes make sure restart the ftp server
- other vsftp server configuration files, some features might require the creation and configuration of other files, such as the following
	+ /etc/vsftpd.user_list, used by the userlist_enable and the userlist_deny options; the file contains a list of usernames to be denied access to the server
	+ /tec/vsftpd.chroot_list, used by the chroot_list_enable and chroot_local_user options, a list of users who are either allowed or denied access to a home directory
	an alternate file can be specified by using the chroot_list_file option
	+ /etc/vsftpd.banned_emails, a list of anonymous password entries used to deny access if the deny_email_enable setting is enabled
	an alternate file can be specified by using the banned_email option
	+ /var/log/vsftpd.log, data transfer information is captured to this file if logging is enabled using the xferlog_enable setting
	+ TIP
	whenever editing the FTP server fiels make a backup file first. also it is always a good idea to comment out( using a pound sign, #, at the beginning of a line ) what is changed instead of deleting or overwriting entries.
	you can use dpkg command or other linux tools such as mc to extract a fresh copy of a configuration file
- default vsftpd behaviors
	+ the contents of a file named .message( if it exists in the current directory ) are displayed when a user enters the directory. 
	ftp users are not allowed to perform recursive directory listings
	specific user login controls are not set, but you can configure the controls to deny access to one or more users
	+ data transfer rate is unlimited, but you can set a maximum rate(in bytes per second) by using the anon_max_rate setting in vsftpd.conf
	+ remote client will be logged out after five minutes of idle activity or a stalled data transfer 
	+ managing system's resources
	dirlist_enable, toggles directory listing on or off
	diremessage_enable, toggles display of a message when the user enters a directory
	ls_recurse_enable, which can be used to disallow recursive directory listings
	download_enable, toogle downloading on or off
	max_clients, set a limit on the maximum number of connections
	max_per_ip, set a limit on the number of connections from the same ip address
- using the ftphosts file to allow or deny ftp server connections
create a file in /etc called ftphosts to allow or deny specific users or addresses from connecting to the FTP server. 
the format of the file is the word allow or deny, also support wildcard match
	+ example
	allow username address
	deny username address
- references
http://www.cert.org, computer emergency response team
http://www.openssh.com, the OpenSSH home page and source for the latest version of OpenSSH
http://vsftpd.beasts.org, home page for vsftd FTP server
https://help.ubuntu.com/community/FtpServer, ubuntu community documentation for setting up and using an ftp server
https://help.ubuntu.com/11.10/serverguide/C/ftp-server.html, official ubuntu documentation for setting up and using vsftp
	

# Handling Email
- How Email is sent and received
email is transmitted as plain text across networks around the world using the simple mail transfer protocol(SMTP)
Mail transfer agents (MTAs) work in the background transfering email from server to server, software such as Sendmail, Postfix, Fetchmail, Exim, or Qmail
- example of how email processed and sent
the right process
	+ matthew@seymourcray.net, composes and sends an email to heather@gracehooper.net
	+ MTA at seymourcray.net receives and queues it for delivery 
	+ MTA at seymourcray.net contacts the MTA at gracehopper.net on port 24. wait and send the email after that the connection close
	+ MTA at gracehooper.net places the mail message into Heather's incoming mail box
may be something goes wrong
	+ aim address not exist, there are also several conditions	
- The mail transport agent
	+ sendmail, it is popular at Linux/Unix/BSD world, it is easier to configure
	+ Postfix, origins as the IBM Secure Mailer, for enhanced security many postfix processes used to use the chroot facility (which restricts access to only specific parts of the file system)
	there are no setuid components in Postfxi. Current ubuntu a chrrot configuration is no longer used and is
	if you are starting from scratch, postfix is considered a better choice than sendmail
	+ qmail and Exim, qmail is a direct competitor to Ppostfix, but is not provided with ubuntu, also including web mail systems and POP3 servers
	http://www.qmail.org/
	
	Exim is yet another MTA, http://www.exim.org/ it's considered faster and more secure than  sendmail or postfix but much different to configure than either of those
	
	Exim and Qmail use the maildir format rather than mbox, both are considered "NFS" safe
	+ MDIR versus mailbox
- Choosing an MTA
Postfix's main strength is that it scales well and can handle large volumnes of email at high speed
- The mail delivery agents, MDA, the MDA transfers mail to ssytems without permanent internet connections
	+ NOTE
	Procmail or Spamassassin are examples of MDAs; both provide filtering services to the MDA while they store messages locally and then make them avaliable to the MUA or email client for reading by the user
	+ the MDA uses the Post offcie Protocol version 3(POP3) or Internet message Access Protocol(IMAP) for this process
- The mail user agent(MUA), another necessary part of the email system, is a mail client or mail reader that allows the user to read and compose email and provides the suer interface
	+ some popular unix command-line MUAs are
	elm, pine and mutt
	
	+ ubuntu also provides modern GUI MUAs 
	Evolution, Thunderbird, Mozilla mail, Baslas, Sylpheed and KMail
	
	+ non-UNIX MUAs are Microsoft Outlook, Pegasus Mail and Apple Inc.'s Mail
	+ Compare with UNIX and non-UNIX MUA
	UNIX generally rely on an external MTA such as Sendmail
	NON-UNIX such as Outlook may include some MTA functionality
		* differences is you could use different application to control email in UNIX
		use Evolution to read and compose mail
		use sendmail to send your mail
		use xbiff to notify you when you have new mail
		fetchmail to retrieve mail from a remote mail server
		use procmail to automatically sort your incoming mail based on sender, subject or many other variables
		use Spamassassin to eliminate the unwanted messages before you read them
- Basic Postfix Configuration and Operation, Postfix is the ubuntu-recommanded MTA
	+ during the Postfix installation process you will be asked several questions to configure the server
	+ go back and reconfigure the service by 
	$ sudo dpkg-reconfigure postfix
	+ Postfix configuration is maintained in files in the /etc/postfix directory with much of the configuration being handled by the file main.cf
	+ a useful command for configuring Postfix is postconf, it enables you to display and change manay ocnfiguration settings
	suggest to use grep and pip to filter the command output
		* show default parameter setting
		$ postconf -d
		
		* display changed configuration compare to the default parameter setting
		$ postconf -n
		
		* setting a parameter requires root privileges
		$ sudo postconf -e "myhostname=mail.matthewhelmke.com"
		
		this works with the parameters listed in the Postfix main.cf file
		
		* start, stop and restart Popstfix using 
		$ sudo /etc/init.d/postfix start
	+ reference book
	Postfix: The Definitive Guide, by Kyle Dent
- Configuring Masquerading
Set Postfix masquerade as a host other than the actual hostname of your system, the follow strips any message that come from matthew.gracehopper.net to just gracehopper.net
masquerade_domains = gracehopper.net
- Using Smart hosts, make Postfix send email to another sender instead of attempting to deliver the email directly in the situation you do not have a full-time connection to internet	
relayhost = mail.isp.net   
- Setting Message Delivery Intervals
ubuntu does not configure dailup connections, does not incude the pppd daemon in default installation, you can then configure Postfix to hold messages for later delivery by adding the following line to /etc/ppp/peers/ppp0
/usr/sbin/sendmail -q

To prevent Postfix auto dail out to send the mail by 
defer_transports = smtp
- TIP
if you use networking over a modem, there is a configuration file for pppd called ppp0 which is located in /etc/ppp/peers, any commands in this file automatically run each time the PPP daemon is started, you can add the line sendmail -q to this file to have your mail queue automatcailly processed each time you dail up your internet connection
- Mail Relaying
by default Postfix will not relay mail, to enable selected domains by add an entry for the domain to main.cf file
mynetworks = 192.168.2.0/24, 10.0.0.2/24, 127.0.0.8
The IP address needs to be specified in classless inter-domain routing(CIDR) format
a handy calculator, head on http://www.subnet-calculator.com/cidr.php
You need to enable this feature for a good reason
- Forwarding Email with Aliases
This allow you to have an infinite number of valid reciepient, Here is a example of an alias entry
postmaster:root
This entry forwards any mail received for postmaster to the root user, by default most of the aliases lists in the /etc/aliases file forward to root
	+ CAUTION
	reading email as root is a security hazard, you can forward your mail to another account and read it from there by 
		* add an entry to the /etc/aliases file that sends root's mail to a different account, for example
		root: foobar
		* other way is create a file named .forward in root's home directory that contains the address that the mail should forward to
	any time you change the /etc/aliases file you need to rebuild the aliases database before it take effect
- using fetchmail to retrieve mail
SMTP is designed to work with systems that have a full-time connection to the internet, if you are on a dail-up account, then you need such as POP3 or IMAP instead
- Installing Fetchmail
use synaptic or apt-get, or get latest from http://www.catb.org/~esr/fetchmail
- Configuring Fetchmail
you can create and subsequently edit the .fetchmailrc file by using any text editor
	+ CAUTION
	.fetchmailrc file is divided into three sections: global options, mail server options, and user options. these sections appear in the order listed
	+ configuring global options, here is a example may appear in global section
	set daemon 600
	set postmaster foobar
	set logfile ./.fetchmail.log
	+ configuring the mail server options
	poll mail.samplenet.org
	proto pop3
	no dns	
	+ configuring user account
	user foobar
	pass secretword
	fetchall
	no flush
	+ each section a split with gap line
	+ you could define multiple .fetchmailrc files to retrieve mail from different remote mail servers while using the same linux user account
	+ CAUTION
	because the .fetchmailrc file contains your mail server password, it should be readable only by you. This means that it should be owned by you and should have permissions no greater than 600, fetchmail will refuse to start if the .fetchmailrc file has permissions greater than this
- Choosing a mail delivery agent
	+ Procmail, a tool for advanced users, Procmail application acts as a filter for email, you must manually create a ~/.procmail file for each user, or users can create their own
	ubuntu provide three example of the files in /usr/share/doc/procmail/examples
	+ Spamassassin
		* you must first have installed and configured Procmail , /usr/share/doc/spamasssasin provide detail 
	use it to tag email erceived at special email accounts, this information is shared with the Spamassassin site where these "spam trap"  generated hits
	+ Squirrelmail
	it support IMAP and SMTP,it supports MIME attachments and an address book and folders for segregating email
	found detail in /usr/share/doc/squirrelmail/INSTALL
- Virus Scanners, such as ClamAV 
- Autoresponders, ubuntu does not include one by default, but you can find and install an autoresponder like vacation or gnarwl from the ubuntu software respositories
If you have subscribed to a mail list, it could be very annoying to others
- Alternatives to Microsoft Exchange Server
Several "drop-in" alternatives exits, none of which are fully open source, MAPI, the Microsoft Messaging Application Program Interface, MAPI is a poorly documented
http://wwww.microsoft.com/exchange/
- CommuniGate Pro, is a drop-in alternative to Microsoft Exchange Server, providing email, webmail, lightweight direcotry Access Protocol(LDAP)
http://www.stalker.com/
- Oracle Beehive, is probably the closest that you will get to an exchange replacement, it is available for linux platforms
http://www.oracle.com
- Bynari, a proprietary group of servers to act as drop-in replacement for exchange
- Open-Xchange, has a great pedigree having been owned and developed by Novell/SUSE until being spun off by itself into its own company
open source version at http://www.open-xchange.com
- phpgroupware, written in PHP, its modular nature enables you to plug in various ocmponents 
http://www.phpgroupware.org, has task list, address book and so on
- PHProjekt, open-source software
- Horde, PHP-based application framework, when combined with an http server (apache, microsoft IIS, Netscape) and MySQL, IMP/Horde offers modules that provide webmail, contact manager, calendar, CVS viewer, file manager, time tracking, email filter rules amnager
http://www.horde.org
- References
http://www.sendmail.org
http://www.postfix.org
http://help.ubuntu.com/community/Postfix
https://help.ubuntu.com/community/ClamAV
http://www.rfc-editor.org, a repository of request for comments(RFCs)
http://www.procmail.org, procmail home page


# Proxying and Reverse Proxying
- you can never have enough of two things in this world, time and bandwidth
- ubuntu come with a proxy server Squid that enables you to cache web traffic on your server
- what is a proxy server
lies between client the internet, client <-> proxy <-.-> server, it's a extra layer between client and server. It provide
	+ content control
	+ speed
	+ security
Squid is capable of all of these and more
- installing Squid
	+ install from ubuntu software repositories, check this by 
	$ ps aux|grep squid
	manual start 
	$ /etc/init.d/squid start
- configuring clients
	+ set up the local web browser to use it for its web access
	for firefox, use proxy and set same proxy for all protocols, enter 127.0.0.1 as the IP address and 3128 as the port number
- Access Control lists for Squid
	+ main Squid configuration file is /etc/squid/squid.conf
	
	+ before you start, open two terminal windows, in the first, change to the directory 
	/var/log/squid and run this command
	$ sudo tail -f  access.log cache.log
	reads the last few lines from both files, so that any changes appear in there, this allows you to watch what Squid is doing as people access it, we refer this window as "log window"
	Use another window with sudo bring up /etc/squid/squid/conf with your favorite editor, refer as configure editor
	Search for the string acl all; this brings you to the access control section, you can configure a lot elsewhere, but unless you have unusal requirements
	
	+ NOTE
	default port for squid is 3128, but you can change that by editing the http_port line, you can have Squid listen on multiple ports by having mutiple http_port lines: 80, 8000 and all popular ports for proxy servers
	
	+ the acl lines make up your access control lists (ACLs). The first 16 or so define the minimum recommended configuration that set up ports to listen to and so on, you can safely ignore these, scroll further, come to http_access lines, which are combined with the acl lines to dictate who can do what. You can mix and match acl and http_access lines to keep your configuration file easy to read
	we start at # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS. in the configure file
	
	default configuration is
	http_access allow localhost
	http_access deny all
	
	after make any change use this command to force reload the configuration
	$ kill -SIGHUP ‘cat /var/run/squid.pid

	Squid read the configuration from top to bottom and will stop at the first match

	add conditional if it fits certain criteria, define criteria by acl liens as:
	acl newssites dstdomain news.bbc.co.uk slashdot.org
	http_access allow newssites
	
	The first line defines an access category called newssites, which contains a list of domains, which allowed all user include external connection to read newssites
	
	use putting the period in fron of the domain to allowed any subdomains
	acl newssites .slashdo.org
	http_access allow newssites [freetime]
	
	freetime limit the user to browser the surf from 18:00:00 until 23:59:59

	more complex example
	acl newssites dstdomain .bbc.co.uk .slashdot.org
	acl playsites dstdomain .tomshardware.com ubuntulinux.org
	acl worktime time MTWHF 9:00-18:00
	acl freetime time MTWHF 18:00-20:00
	http_access allow newssites worktime
	http_access allow newssites freetime
	http_access allow playsites freetime
	
	NOTE
	The letter D is equivalent to MTWHF, meaning "all the days of the working week"
	
	+ use regular expression, if you want to stop people downloading windows executable files use
	acl noexes url_regex -i exe$
- Specifying Client IP Address
if your client use Dynamic Host Control Protocol(DHCP) a better solution is use classless inter domain routing(CIDR) notation, which allows you to specify addresses like this
192.0.0.0/8
192.168.0.0/16
192.168.0.0/24
the last number behind the slash is defines the range of addresses you want covered and refers to the number of bits in an IP address. 
192.0.0.0/8, means the first 8 bit is fixed and the rest is changeable
- Sample Configurations
include the domains news.bbc.co.uk and slashdot.org
and not newsimg.bbc.co.uk or www.slashdot.org.
acl newssites dstdomain news.bbc.co.uk slashdot.org

include any subdomains or bbc.co.uk or slashdot.org
acl newssites dstdomain .bbc.co.uk .slashdot.org

only include sites located in Canada
acl canadasites dstdomain .ca

only include working hours
acl workhours time MTWHF 9:00-18:00

only include lunchtimes
acl lunchtimes time MTWHF 13:00-14:00

only include weekends
acl weekends time AS 00:00-23:59

include URLs ending in “.zip”. Note: the \ is important,
because “.” has a special meaning otherwise
acl zipfiles url_regex -i \.zip$

include URLs starting with https
acl httpsurls url_regex -i ^https

include all URLs that match “hotmail”
url_regex hotmail url_regex -i hotmail

include three specific IP addresses
acl directors src 10.0.0.14 10.0.0.28 10.0.0.31

include all IPs from 192.168.0.0 to 192.168.0.255
acl internal src 192.168.0.0/24

include all IPs from 192.168.0.0 to 192.168.0.255
and all IPs from 10.0.0.0 to 10.255.255.255
acl internal src 192.168.0.0/24 10.0.0.0/8
- References
http://www.squid-cache.org/
http://www.deckle.co.za/squid-users-guide/
https://help.ubuntu.com/community/Squid
There are two excellent books on topic of web caching, the first is Squid the definitive Guid, the second is Web Caching 


# Administering Relational Database Services
- Introduce to MySQL and PostgreSQL
	+ Installing and maintaining database serviers
	+ installing and maintaining database client
	+ manage accounts and users
	+ ensuring database security
	+ ensuring data integrity
- A brief review of database basics
	+ flat file database, such as /etc/passwd, use text and other character to contain data
		* limitations
		do not scale well
		flat file database are unsuitable for multiuser environments, the file may overwriting the changes make by different user
	+ relational database management systems(RDBMSs)
	+ NOSQL database, is suitable for large and high traffic uses, that data is replicated and available instantly across a large installation
	There are many forms of NoSQL, google's bigTable and apache's Cassandra, ubuntu uses Apache's CouchDB for several applications, common ones include MongoDB and Berkeley DB
- How relational database work
stores data in tables, each column in the table is a field, each row in the table is an individual record
- understanding sql basics, (pronounced "S-Q-L" or "sequel")
- creating tables
	+ sql ignore case
- insert data
- retrieve data, with select
- in mysql you can use the CONCAT() function to combine the title and year columns
- postgreSQL string concatenation with double pip ||
- choosing a database: mysql versus postgresql
	+ oracle bought sun who owned mySql, two group forked the code to ensure that the database stays free and open source while still in active development
		* Drizzle, http://www.drizzle.org
		* MariaDB, http://kb.askmonty.org/v/MariaDB
- Speed, mysql is quit fast compare to PostgreSQL, only in certtain situations such as periods of heavy simultaneous access, PostgreSQL can be significantyly faster than MySQL
- data locking
	+ MySQL use table level locking in the old days, with ubuntu gives you the choice of using tables with table level or row-level locking. MyISAM tables use table-level locking and InnoDB tables use row-level locking
	+ PostgreSQL use row level locking, which is more fit-able for heavy load and access simultaneously
- ACID Compliance in transaction Processing to Protect Data integrity
	+ atomicity, several database operations are treated as an indivisible (atomic) unit, often called a transaction
	+ Consistency, ensures that no transaction can cause the database to be left in an inconsistent state, is rolled back or undone
	+ Isolation, multiple transactions operating on the same data are completely isolated from each other
	+ durability, ensures that after a transaction has been committed to the database, it cannot be lost in the event of a system crash
- SQL subqueries, both of mysql and postgreSQL support this feature with ubuntu
- procedural languages and triggers, use the manage the database data
- configuring mysql
	+ download
	http://www.mysql.com
	+ initialize the grant tables or permissions to access any or all databases and tables and column data within a database by
	mysql_install_db as root, this command initializes the grant tables and creates a mysql root user
	+ CAUTION
	mysql data directory needs to be owned by the user who owns the mysql process, most likely mysql. 
	only this user should have any permissions on this directory(permissions should be set to 700). any other value may creates a security hole
	
	by default mysql root user is created with no password, this is one of the first things you must change because the mysql root user has access to all aspects of the database

	+ setting a password for th emysql root user
		* connect to mysql server as the root user
	   $ mysql -u root 
	   
	   * issue a command like the following to set a password for the root user
	   mysql> SET PASSWORD FOR root=PASSWORD("secretword");
	   
- creating a database in mysql, with two ways
	+ connect to mysql with root user
	$ mysql -u -p
	
	then create database with SQL script
	
	+ use mysqladmin command as the root user
	$ sudo mysqladmin -u root -p create database_name
- grant yourself some privileges
	+ global level
	+ database level
	+ table level
	+ column level
	+ example 
		* connect mysql as root
		$ mysql -u root -p
		grant what_to_grant ON where_to_grant TO user_name IDENTIFIED BY 'password';
		
		privileges are specified with keywords as ALL, global-level, table-level, column-level
		where_to_grant is the user name
		password is the password should be assigned to the user

		* new created user could connect to the database by 
		$ mysql -h hostname -u username -p data_base
- if you need to revoke privileges from foobar you can use the REVOKE statement
REVOKE ALL ON database_name FROM username
- Configuring PostgreSQL
	+ install
	http://www.postgresql.org, or download from ubuntu
	
	+ create from ubuntu will automatic create a user account 
	$ fgrep postgres /etc/passwd
	
	otherwise you need create postgres during the installation, this user should not have login privileges, because only root should be able to use su to become this user and no one will ever log in directly as the user
- initializing the data directory in PostgreSQL
	+ CAUTION
	initdb program sets the permission on the data directory to 700, do not change this
	
	+ start postmaster program with the following command
	$ postmaster -D /usr/local/pgsql/data &
	
	+ TIP
	ubuntu makes the PostgreSQL data direcotry at /var/lib/pgsql/data, which is not very good for data store
	if you changed the data directory to something else(such as /usr/local/pgsql/data) you need to edit the PpostgreSQL startup file(named postgres) located in /etc/init.d to reflect the change
- creating a database in postgreSQL
	+ use the default name postgres
	$ su -postgres
	$ createdb database
	
	+ use psql with SQL statement
	
	+ you need to create at least one database before you can start the pgsql client program
	
	$ psql data_base
	if you don't specify the database name psql will try to connect database with the same name as which you invoke psql
	
- create database users in PostgreSQL
	+ use su to become postgre user
	$ sudo su postgre
	+ use PostgreSQL createuser command to create a user
	$ createuser user_name
	
	+ CAUTION
	without WITH PASSWORD portion of the statement will create a account with empty password
	
	+ NOTE
	press \q to quit psql shell promp
- delete user 
	+ use command dropuser
	$ dropuser user_name
	+ login and use sql statement
	
- granting and revoking privileges in PostgreSQL
	+ with GRANT AND REVOKE statements
	GRANT what_to_grant ON where_to_grant TO user_name;
   
- database clients   
	+ SSH Access to a database
	+ web access to a database
- mysql command-line client, is 
$ mysql [options] [database]
option -h hostname Connects to the remote host hostname(if the database server isn’t located
option on the local system).
option -u username Connects to the database as the user username.
option -p Prompts for a password. This option is required if the user you are connecting as needs a password to access the database. Note that this is a lowercase p.
option -P n Specifies nas the number of the port that the client should connect to.
option Note that this is an uppercase P.
option -? Displays a help message.   
	+ CAUTION
	mysql allows you to specify the password on the command line after the -p option, you should never invoke the client this way. doing so causes your password to display in the process list, the process list can be accessed by any user on the system, NEVER GIVE YOUR PASSWORD ON THE SQL COMMAND LINE
	
	+ after login use help to display all the available commands
- PostgreQSL command-line client   
$ psql [options] [database]
option -h hostname Connects to the remote host hostname(if the database server isn’t located
option on the local system).
option -p n Specifies nas the number of the port that the client should connect to. Note
option that this is a lowercase p.
option -U username Connects to the database as the user username.
option -W Prompts for a password after connecting to the database. In PostgreSQL 7
option and later, password prompting is automatic if the server requests a password
option after a connection has been established.
option -? Displays a help message.
- graphic clients
	+ mysql official GUI client is MySQLGUI
	
	+ phpMyAdmin and phpPgAdmin are web base interfaces for Mysql and PostgreSQL
	this require install php and a web server
   
   + related ubuntu and database commands
   createdb, create a postgreSQL database
   creatuser, create a new postgreSQL user account
   dropdb, deletes a postgreSQL database
   mysql, interactively queries the mysql server
   
   mysqladmin, administers the mysql server
   mysqldump, dumps or backs up mysql data or tables
   psql, accesses postgreSQL via an interactive terminal
- references
http://radar.oreilly.com/2009/07/oscon-the-saga-of-mysql.html, interesting article describing the fears of the mysql community and the creation of forks


# NoSQL database
- NoSQL should not be considered as a replacement for relational database. this is a new set of databases designed to excel in specific situations, especially in largescale, high-traffic-volumne applications
NoSQL fit for lots of reads and write at high speed and for semi real-time and low latency for end users
- database generally labeled NoSQL share
	+ store structured data
	+ do not store data relationally
- the advantages to use a NoSQL option instead of a relational database are
	+ NoSQL databases are designed for really large sets of data 
	+ NoSQL database are designed to scale as needed
	+ Commodity hardware is much cheaper than dedicated, professional relational database server hardware
	+ data models with NoSQL are much more relaxed
- disadvantages to using a NoSQL database
	+ support is not as readily available
	+ most nosql projects are fairly new
	+ nosql is not always ACID(atomicity, consistency, isolation, durability), this not means you always get the most updated data
	+ there is no particular advantage to NoSQL unless you have data that is large enough to benefit from it or your project fits neatly into specific use case
	+ it's not difficult to migrate to nosql from traditional relational database if the need arises
	may be wiser to design your site or application using known technology and then migrate later if the need arises
- an interesting development in the NoSQL world is that specification are being created for a new database query language called UnQL(pronouced "uncle"), Unstructured Query Language(http://unqlspec.org), its commands not works on tables, but rather on collections of unordered sets of objects, described using javascript object notation(JSON)
- key/value stores
key/value stores are great for things like contents of a website shopping cart, user preference lists...
- Berkeley DB, create a disk hash table that worked better than an existing solution, it also support interact with SQL and java, it's small and fast
wellknown project used it is Subversion, Postfix, OpenLDAP and backend for MySQL prior to 5.1
- Cassandra, developed by facebook for their inbox searching featur, it was released open source when they turn it over to Apache in 2008. it store and run on a flexible cluster of nodes like HBase(discussed latyer in the "Wide column store") nodes may be added and removed from the cluster. There is no central node
interesting feature is that Cassandra may be tuned to adjust the trade-off between speed of transactions and consistency of data. not all data stored in Cassandra is designed to persist over timeS
- Memcached and MemecacheDB, MemcacheDB is an implementation of the Memcached API that uses a key/value format based on Berkeley DB. it's designed as a cache solution to speed up date access from memory. MemcacheDB is designed as a persistent storage engine.. its often used by websites based on content management systems like Drupal and WordPress
- Redis, released in 2009, redis is intended for applications where performance and flexibility are more important than persistence and absolute data integrity. open-source key/value store written in C. works in RAM for speed. occasionally dumping to disk. designed so that master-slave replication is easy to set up
- document stores, is designed to store structured in some form of notation like json or xml
- CouchDB, began in 2005 as a self-funded personal project of Damien Katz. in 2008 it was given to Apache. design to serve web applications, emphasis is on scalability and fault tolerance while using commodity hardware
it's use RESTful HTTP API, all stored items have a unique uniform resource identifier(URI) and full create, read, update, and delete(CRUD)
it is designed with ability to include ACID compliance, makes it possible to use CouchDb with more consistency-sensitive data
It's write in erlang, which is a language designed for concurrency, it's designed to store JSON document object
- MongoDB, similar to CouchDB, stores for json objects like Cassandra, designed for replication and high-availability 
developer offers commercial, enterprise-class support, training and sonsulting
supports sharding, which automatically partitions data across servers for increased performance and scalability
support for indexing in a manner that is more extensive and powerful than most NoSQL solutions
- BaseX, using a BSD license in 2007, it is a simple and lightweight database that does not support a lot of features, right for specific applications rather than using json like couchdb and mongodb, design to store in xml, supports standard xml tools like xpath and xquery and also includes a lightweight GUI
creates indexes, supports w3c recommendations and standards, ACID-safe transactions large documents and various APIs like REST/JAX-RX and XML:DB
- wide column stores, referred to big table stores
- bigTable, google product that is only used by google, work with google's mapreduce framework
- HBase, used by hadoop, Hadoop is the Apache Project's free software application for processing huge amounts of data across large clusters of compute nodes in a cluster
main feature of HBaseis its ability to host very large tables on the scale of billions of rows across millions of columnS, provides a RESTful web service interface that supports manay formants and encoding and is optimized for real time queries


# Lightweight directory access protocol(LDAP)
- ubuntu use database for storing all its LDAP information, OpenLDAP uses Sleepycat Software's Berkeley DB, sticking with that default is highly recommended
- configure the server, the relative readme file at /etc/ldap/schema
	+ install client and server applications
	slapd and ldap-utils packages from the ubuntu repositories, this install three packages, odbcinst, odbcinstdebian2 and unixodbc
	by default, ubuntu configures slapd with the minimum options necessary to run the daemon
	fully qualified domain name(FQDN) 

	+ OpenLDAP now uses a separate directory containing the cn=config direcotry information tree(DIT)

	+ creating your schema, offical OpenLDAP documentation and start from scratch, for example load these three files into the directory using 
	$ sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/consine.ldif
	$ sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/nis.ldif
	$ sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/inetorgperson.ldif

	+ create a file called backend.domainname.com.ldif with these contents
	;# Load dynamic backend modules
	dn: cn=module,cn=config
	objectClass: olcModuleList
	cn: module
	olcModulepath: /usr/lib/ldap
	olcModuleload: back_hdb
	;# Database settings
	dn: olcDatabase=hdb,cn=config
	objectClass: olcDatabaseConfig
	objectClass: olcHdbConfig
	olcDatabase: {1}hdb
	olcSuffix: dc=matthewhelmke,dc=com
	olcDbDirectory: /var/lib/ldap
	olcRootDN: cn=admin,dc=matthewhelmke,dc=com
	olcRootPW: changeMEtoSOMETHINGbetter
	olcDbConfig: set_cachesize 0 2097152 0
	olcDbConfig: set_lk_max_objects 1500
	olcDbConfig: set_lk_max_locks 1500
	olcDbConfig: set_lk_max_lockers 1500
	olcDbIndex: objectClass eq
	olcLastMod: TRUE
	olcDbCheckpoint: 512 30
	olcAccess: to attrs=userPassword by dn=”cn=admin,dc=matthewhelmke,dc=com” write by
	anonymous auth by self write by * none
	olcAccess: to attrs=shadowLastChange by self write by * read
	olcAccess: to dn.base=”” by * read
	olcAccess: to * by dn=”cn=admin,dc=matthewhelmke,dc=com” write by * read

	change the matthewhelmke to aimdomain name and change olcRootPW to secure password
	add the new file to direcotry
	$ sudo ldapadd -Y EXTERNAL -H ldapi:/// -f backend.example.com.ldif

	+ populating your directory, after the backend is ready we need to populate the front end directory to make this useful
	create another file called frontend.matthewhelmke.com.ldif
- install the client
- Evolution, such as thunder bird for email
- reference
http://www.openldap.org
http://ldap.perl.org
http://www.ldapguru.com
https://help.ubuntu.com/10.10/serverguide/C/openldap-server.html
http://phpldapadmin.sourceforge.net/—The official documentation for phpLDAPadmin


# Linux Terminal Server Project(LTSP)
- it's a add-on package for linux that enables you to run multiple thin clients low-powered terminals from one main server
thin client is small energy-efficient
- the thin client will have limited processing power and speed and limited storage, all it is used for is receiving input from a device such as a keyboard or mouse, communicating with a server and displaying output to a screen  
- Edubuntu, a community led official sub-project of Ubuntu, is designed for easy LTSP
- LTSP is a add-on software for inux that creates a server that can be used to boot client computers over a network
The biggest limitations result from server and networking hardware, (switch or hub, 1GB versus 100mb ports)
- Requirements, minimum recommended specification for think clients are a processor running at 400MHz with 128MB ram and the ability to boot via PXE(a common network boot protocol)
- For the server at least two Ethernet cards and best CPU and more RAM and two RAID array hard disk
- Installation
	+ if current ubuntu is installed on the server, you install the ltsp-server-standalone and openssh-server packages from the ubuntu repositories and run the following to set up the environment
	$ sudo ltsp-build-client
	+ if your server is a 64-bit system and your thin clients use a different processor architecture, you want to run that command with the arch option 
	$ sudo ltsp-build-client --arch i386
	+ install from scratch, download the alternate install cd from ubuntu website, http://release.ubuntu.com/
		* download and install ubuntu from the cd
		* make sure all your thin clients are connected and set to boot from network using PXE but turned off
		* boot the server using the alternate cd, press f4 for the models menu and choose install an LTSP Server, A regular ubuntu installation will begin
		* after server has running and you may turn on your thin clients and they will boot from the network using the image on the server. Thin clients can run any programs installed on the server
- using LTSP
	+ thin client tretrieves the boot image from the LTSP server and use it to create the environment
	+ using Dynamic Host Control Protocol(DHCP) to assign client IP
	+ downloading the kernel and initial RAM disk from system
	+ using chroot to use this file system as the client's own root file system
	+ by default the client boots to a login screen using the LTSP display manager(LDM) in place of GDM
	+ the entire desktop is run on the server and displayed on the client
	+ One useful feature in LTSP is that you are not limited to using only one server in a network. 
		* the server taht a client uses to boot is not necessarily the same wserver that a user will log in to
		* when you add servers, one server is chosen to be the primary server and controls thin client boots, data storage and runs additional services
- References
http://www.ltsp.org, the official upstream LTSP website
https://help.ubuntu.com/community/UbuntuLTSP, the ubuntu community documentation page is easily the   
http://edubuntu.com/, the leadersof Edubuntu community project are very knowledgeable about LTSP and helpful
https://help.ubuntu.com/community/UbuntuLTSP/Tour
https://help.ubuntu.com/community/UbuntuLTSP/ThinClientHowtoNAT/
http://doc.ubuntu.com/edubuntu/edubuntu/handbook/C/
sampling companyies advertising thin clients
http://www.artecgroup.com/thincan
http://www.chippc.com/thin-clients/thin-clients.asp
http://www.devonit.com/hardware
http://www.disklessworkstations.com/
http://h10010.www1.hp.com/wwwpc/us/en/sm/WF02d/12454-12454-321959.html
http://www.lucidatech.com/
http://www.wyse.com/products/hardware/thinclients/index.asp


# Virtualization on Ubuntu
- virtualization software takes care of the details of interacting with the actual hardware which may even change without affecting the VM. This is called hardware emulation
- One neat trick is to run a set of servers locally and then add compute resources from a cloud computing pool like Amazon's EC2, Ubuntu Enterprise Cloud(using Eucalyptus), Ubuntu Cloud Infrastructure(using OpenStack), or OpenStack to start up VMs on their network as needed
- hardware support
	+ intel need to open it from the BIOS of VT-X
	+ AMD called AMD-V
- KVM, kernel-based Virtual Machine, is a part of the linux kernel. KVM does not perfrom hardware emulation, but only provides the lower level tasks. Manage VMs with KVM in ubuntu is done using libvert and QEMU
check system has the extensions enabled by installing and running the kvm-ok package, it is a simple command line tool tat exits with ouput 0 if the system is suitable or non-0 if not
- Install packages in ubuntu
	+ qemu-kvm, the necessary user-space component of KVM
	+ libvert-bin, a binary of a C toolkit to interact with the virtualization capabilities of Linux and currently supports not only KVM, but also XEN, VirtualBox and more
	+ virtinst, command line tools for creating VMs
	+ bridge-utils, utilieies for configuring ethernet connections in linux
	+ virt-viewer, provides a nice GUI and VNC interface to VMs
	+ virt-manager, a nice GUI for managing VMs
	+ log out and back in so that the automatic addition of your user to the libvertd group is certain to be made effective
- Bridged networking, if you want to change the network settings to enable the use of a VM as an outside accessible server, you need bridged networking
this only work with wired not wireless hardware
you also must not use the default network manager to control the hardware being bridged
- to start install libcap2-bin, you need to grant QEMU the ability to administrator networking by setting cap_net_admin, if you have a 64-bit system, use the following
$ sudo setcap cap_net_admin=ei /usr/bin/qemu-system-x86_64

if you have 32 bit system use
$ sudo setcap cap_net_admin=ei /usr/bin/qemu

create a bridge interface called br0 in /etc/network/interfaces by adding these lines to use DHCP or your network settings if you want to configure it yourself
auto br0
iface br0 inet dhcp
bridge_ports eth0
bridge_stp off
bridge_fd 0
bridge_maxwait 0

restart networking by 
$ sudo /etc/init.d/networking restart

you need to create guest VMs that use this bridged network

- There are several ways to create VMs for use with KVM.
	+ use vmbuilder, this is a python script, best ofr servers on which you intend to run ubuntu JeOS, a specialized, very light ubuntu server variant that includes a tuned kernel with only the base element under KVM and MVware
	
	install package python-vm-builder to run as virtual server

	run
	~$ sudo vmbuilder kvm ubuntu --suite natty --flavour virtual --arch i386 -o --libvirt qemu:///system --ip 192.168.0.100 --hostname lovelace --bridge br0
	
	to creat VM use br0 bridge

	check detail info at
	$ vmbuilder kvm ubuntu --help

	+ download ubuntu VM images, prebuilt and ubuntu-supported VM images are available for download http://cloud-images.ubuntu.com

	+ NOTE 
	preseeding ubuntu server installations,  http://blog.dustinkirkland.com/2011/03/ubuntu-server-quick-install-no.html.

	+ virtinst consists of several tools here we focus on two virt-install, to provision new virtual machines and virt-clone to clone existing virtual machines
	virt-install can also make desktop images that include a GUI
	
	sudo virt-install -n hopper -r 512 --disk path=/var/lib/libvert/images/hopper.img,size=20 -c /dev/cdrom --accelerate --connect=qemu:///system --vnc --noautoconsole -v

	copy a virtual machine use virt-clone

	start a virtual machine use
	$ virsh -c qemu: ///system start hopper
	
	stop a virtual machine use
	$ virsh -c qemu:///system shutdown hopper

	also support use ssh to connect virtual machine or use GUI client
	$ virt-viewer -c qemu:///system hopper
	$ virt-manager -c qemu://system
- virtualBox
virtualbox is much easier to use than KVM. it's design for running on another operation system for such as testing
	+ install
	http://www.virtualbox.org/
	
	+ start virtualbox at command line
	$ virtualbox
	$ virtualbox & 
- VMware
enterprise-focused virtualization platform, has better features than virtualbox. The VMware software runs on bare metal; it is the operating system that gets installed on all the servers in a VMware installation	
- Xen
open-source virtualization platform, use by researchers, hobbyists, developers and others. Web hosting companies that offer vitual servers often use Xen, Xen installs on bare metal like VMware. It can be installed on top of another operating system in a host/guest arrangement. ubuntu not support Xen, focused on KVM   
- Reference
http://www.linux-kvm.org
http://www.virtualbox.org
http://www.wmware.com
http://www.xen.org
	
	
# ubuntu in the cloud
- SysAdmin vs. DevOps, a new title has emerged combine manay of the talents and responsibilities of sysadmins and developers
- Ubuntu Cloud, formerly Ubuntu enterprise Cloud(UEC), is a stack of applications from canonical that included in the ubuntu server edition
The software is free and open source, but Canonical offers paid technical support
Ubuntu Cloud was originally based on Eucalyptus, starting with 11.10, will base on OpenStack
- Why a cloud
	+ cloud computing is designed to make that easier by providing resources such as computing power and storage as services on the internet in a way that is easy to access remotely
	+ these abstractions are referred to as infrastructure as a service(IAAS), platform as a service(PAAS), and software as a service(SAAS)
	+ VMs are created and used when needed and destroyed immediately after they are no longer needed
- ubuntu cloud and eucalyptus
	+ ubuntu choose KVM as hypervisor to monitor and presenting each VM with a virtual platform
	+ node contrller(NC) and the cluster controller(CC), NC runs as a layer between the host operating system and the CC above it
	+ cloud controller(CLC) is the main front end to the whole cloud. It provides the API for client services and deals with all requests and responses
	+ storage controller(SC) and the Walrus storage controller(WS3), the SC provides a way to create persistent storage devices similar to Amazon's elastic block storage(EBS) devices
	WS3 is a simple storage service that uses REST and SOAP APIS 
- deploy/install basics: public or private?
there are two way to deploy ubuntu cloud based on Eucalyptus, on a private cloud or on a public cloud
	+ public clould is built on Amazon's EC2 
	+ private ubuntu cloud is created on hardware you own and control
- deploy public cloud
	+ create a Amazon account
	+ tools to allow you to start and stop instances
	Euca2ools, a set of command-line tools from Eucalyptus fro interacting with Amazon EC2 and S3, which a installed by default in ubuntu server
	
	Landscape, which is a paid service from Canonical. it provides a web-based graphical user interface(GUI)
	+ many ubuntu VM images have already been published to Amazon check http://cloud-images.ubuntu.com
- deploy private cloud
at a minimum you need two servers. one serves as a front end and you need one or more nodes
	+ front end, will run one or more of these the CLC the CC the WS3/SC
	install ubuntu enterprise cloud server. tell the installer to inclue WS3, CC and SC
	
	+ for nodes which will run the node controller and instances you need to meet the requirements
	connected to the network on which the CLC is running. confirm it and finish the installation as usual

	check these steps at https://help.ubuntu.com/community/UEC/CDInstall
	running the appropriate version of uec-component-listener
	boot the CLC users need credentials. These can be retrieved using a web browser or from the command line. https://<cloud-controller-ip-address>:8448/
	user, admin, password admin
	
	click the credentials tab and download the credentials
	retrieve the credential by 
	$ sudo euca_conf --get-credentials mycreds.zip
	
	+ setup the EC2 API and AMI tools on your server, check that euca2ools package is installed
	check local cluster availablility by 
	$ euca-describe-availability-zones verbose
	
	check the detail from the ubuntu website help
- ubuntu cloud and openstack, openstack is an apache-licensed cloud computing platform. it was founded as a collaboration between NASA and Rackspace. 
	+ Quick fact
	NASA and ubuntu were early adopters of and participants with Eucalyptus. Both now focus on openstack
- openstack uses a set of APIs for its services that are compatible with Amazon EC2/S3 APIs, client tools written for those can also be used with OpenStack has three main service
	+ compute infrastructure
	+ storage infrastructure
	+ image service
- compute infrastructure(Nova), Nova has an Amazon EC2-compatible RESTful API, several components
nova-api, provides outside systems to interact with cloud
rabbit-mq, message queue to communicate with other nova components
nova-compute, carry out operations based on request received by message queue
nova-network, allocates IP, configure VLANs
nova-volume, worker manage logical volumn manager(LVM)-based storage volume
nova-scheduler, determine which compute,network or stoarge volume server should be used
- Storage infrastructure(Swift)
object store. it is scalable up to multiple petabytes and billions of objects. it is elastic 
- Imaging Service(Glance)
Glance is a lookup and retrieval system for VM images, OpenStack Object Store, S3 storage
- Installation
We use three physical machines for this simple example, all of the previously mentioned components run on server 1, except for the compute nodes which are represented by server 2, the third machine is client and is used as the human interface

settings for the basic openStack installation
		server 1							server2				 client
purpose manage all open stack components	nova-compute node	   client
NICs	eth0(public net)					eth0(public net)		eth0(public net)	
		eth1(private net)				   eth1(private net)
IP	  eth0 10.10.10.2					 eth0 10.10.10.3		 eth0 10.10.10.4
		eth1 192.168.3.1					eth1 192.168.3.2
hostname turing.example.com				 burroughs.example.com   watson.example.com
DNS	 10.10.10.3						  10.10.10.3			  10.10.10.3
gateway 10.10.10.1						  10.10.10.1			  10.10.10.1

- prepare server 1
	+ boot the physical machien from a 64-bit ubuntu 11 server cd
	install the glance and nova packages later
	
	manually partition the hard dirve, create a dedicated partition large enough to host any and all of the storage (via nova-volumn)
	network and set the partition type to Linux LVM
	
	create the first user with the name localadmin
	in the packages menu of the installation process, install only openssh-server
	
	after complete, enable the universe repository in /etc/apt/sources.list, update the package list and upgrade all your installed packages
	$ sudo apt-get update
	$ sudo apt-get upgrade
	
	+ install bridge-utils
	$ sudo apt-get install bridge-utils
	
	reboot the server and log in as localadmin
	
	replace the contents of /etc/network/interfaces with 
	auto lo
	iface lo inet loopback
	auto eth0
	iface eth0 inet static
	address 10.10.10.2
	netmask 255.255.255.0
	broadcast 10.10.10.255
	gateway 10.10.10.1
	dns-nameservers 10.10.10.100
	auto br100
	iface br100 inet static
	bridge_ports eth1
	bridge_stp off
	bridge_maxwait 0
	bridge_fd 0
	address 192.168.3.1
	netmask 255.255.0.0
	broadcast 192.168.255.255
	
	+ restart the network
	$ sudo /etc/init.d/networking restart
	this server will act as a network time protocol(NTP) server for all the nodes, we want it to sync to official NTP server, install NTP server
	$ sudo apt-get install ntp

	make sure these two lines are in /etc/ntp.conf
	server 127.127.1.0
	fudge 127.127.1.0 stratum 10

	restart the NTP service 
	$ sudo /etc/init.d/ntp restart
	
	+ install glance
	$ sudo apt-get install glance
	
	+ install mysql
	$ sudo apt-get install -y mysql-server

	+ configure mysql first, edit /etc/mysql.my.cnf to change the bind address
	bind-address=0.0.0.0
	
	restart the mysql
	$ sudo restart mysql

	replace the password if you don't set it during the installation
	$ mysqladmin -u root password secretpassword
	
	create a database named nova
	$ sudo mysql -uroot -psecretpassword -e ‘CREATE DATABASE nova;

	enable root to log in from any IP
	$ sudo mysql -uroot -psecretpassword -e "GRANT ALL PRIVILEGES ON *.* TO 'root'@'% WITH GRANT OPTION";

	set a mysql password for root logins from any IP
	$ sudo mysql -uroot -psecretpassword -e "SET PASSWORD FOR 'root'@'%'=PASSWORD('secretpassword');"
	
	+ install nova components
	$ sudo apt-get install -y rabbitmq-server nova-common nova-doc python-nova nova-api nova-network nova-volume nova-objectstore nova-scheduler novacompute
	
	+ install euca2ools
	$ sudo apt-get install -y euca2ools
	
	+ install unzip
	$ sudo apt-get install -y unzip
	
	replace the contents of /etc/nova/nova.conf with
	--dhcpbridge_flagfile=/etc/nova/nova.conf
	--dhcpbridge=/usr/bin/nova-dhcpbridge
	--logdir=/var/log/nova
	--lock_path=/var/lock/nova
	--state_path=/var/lib/nova
	--verbose
	--s3_host=10.10.10.2
	--rabbit_host=192.168.3.1
	--cc_host=192.168.3.1
	--ec2_url=http://10.10.10.2:8773/services/Cloud
	--fixed_range=192.168.0.0/16
	--network_size=8
	--FAKE_subdomain=ec2
	--routing_source_ip=192.168.3.1
	--sql_connection=mysql://root:mygreatsecret@10.10.10.2/nova
	--glance_host=192.168.3.1
	--image_service=nova.image.glance.GlanceImageService
	--iscsi_ip_prefix=192.168.
	
	enable iscsitarget
	$ sudo sed -i 's/false/true/g' /etc/default/iscsitarget
	
	restart the iscsitarget service
	$ sudo service iscsitarget restart

	+ check the detail from openstack and ubuntu website or from the book
- Juju is a APT for the cloud	
- Ubuntu Orchestra Project is a collection of best practices for deploying Ubuntu servers
from Ubuntu servers. 
- References
http://open.eucalyptus.com/wiki/Euca2oolsGuide_v1.1, The Euca2ools User
Guide.
http://www.openstack.org, The official website for OpenStack.
http://www.canonical.com/enterprise-services/ubuntu-advantage/landscape/cloudmanagement, Canonical’s Landscape is a commercial management tool for Ubuntu
Cloud and Amazon EC2 instances.
http://juju.ubuntu.com, The official Ubuntu documentation for Juju.


# Opportunistic development
- version control systems
- NOTE 
subversion and mercurial are still in heavy use, but most developers today have switched to Git and Bazaar
- managing software projects with subversion, it's a replacement for Concurrent Versioning System(CVS)
	+ install subversion from ubuntu
	+ create a new repository 
	$ svnadmin create /path/to/your_svn_repo_name
	
	move all your files into trunk 
	$ svn import project file:///your_svn_repo_name/your_project -m "first import"

	check out code from an existing central repository 
	$ svn checkout file:///your_svn_repo_name/your_project/trunk your_project

	check in code
	$ svn commit -m "this fixes bug 201341"
	
	update source code in your local repository
	$ svn update
	
	add new files to the repository
	$ svn add file_or_dir_name

	delete file
	$ svn delete file_or_dir_name

	sourceforge and git and mercurial are all works with svn
- managing software projects with bazaar
it was created by Canonical and first released 2007 to host all development files for ubuntu and other projects, doing the samething as svn and more easier to use
	+ install Bazzar from ubuntu called bzr
	+ create new repository
	create with empty folder
	$ bzr init your_project_name
	
	create with exit project, enter the top level folder and type
	$ bzr init
	$ bzr add
	
	check out
	$ bzr checkout your_project_name
	
	check changes
	$ bzr cdiff
	
	check in
	$ bzr commit -m "this fixes bug 2341324"
	in Bazaar commit not means change the remote files, onlyl change to your local copy, if you want others to see your changes you must push the changes to them
	$ bzr push sftp://path.to.main/repository
	
	update the source code
	$ bzr udpate
- Managing software with mercurial
the linux kernel used to be hosted using a version control system and that was free to use but not open source
Mercurial and Git are replacement for the previous version control, git was originally written by the main kernel developer himself, Linus Torvalds
Mercurial is design to use a distributed architecture and to be fast even when working with large codebases, commands are similar to subversion
	+ NOTE
	chemical symbol for mercury is Hg, this is why Mercurial commands use hg
	+ install Mercury from ubuntu
	+ create a new repository
	$ hg init your_project_name
	
	add your files to directory which already exist
	$ hg add
	
	check in code
	$ hg commit -m "commit"
	you must push the change to remote to update the file
	$ hg push
	
	update the source code in your local repository
	$ hg update

- Managing software projects with Git
First released in 2005, it works without a central repository
	+ install git
	+ create new repository
	$ git init
	
	check out code, you must first tell git where that repository is
	$ git remote add orgin

	pull the code from that repository to your local one
	$ git pull git://path_to_repository/direcotry/project.git
	
	add new files to repository
	$ git add file_or_dir_name
	
	delete files from the repository use
	$ git rm file_or_dir_name
	
	check in code 
	$ git commit -m "comment"
	you must push the change to change the remote file
	$ git push git://path_to_repository/direcotry/proj.git
- introduction to opportunistic development
may be you have a idea, write the program and put it on a website for others to find and hope that maybe someone would find it useful make a DEB package and upload it to either the Debian or Ubuntu repositories
	+ Launchpad, is a infrastructure created to simplify communication collaboration and process with software development, supported by Canonical
	where ubuntu development takes place. it integrates Bazaar, the version control system introduced earlier
	registered Launchpad users can create a personal package archive(PPA). 
- Quickly is a tool to help you write and package software programs, ubuntu community wrote quickly and uses it most often, uses a specific set of tools
	+ python
	+ gtk
	+ glade, a user interface designer for GTK from the GNOME community
	+ GStreamer, a media framework used in GNOME for easy integration of audio and video
	+ Desktop couch, a database format built on apache's couchDB
	+ gedit, the default text editor in the GNOME desktop environment
	+ install quickly from ubuntu name as quickly
		* check the tuturial
		$ quickly tutorial ubuntu-application
		
		* some useful command for quickly
		creates the initial tempalte for an ubuntu program
		$ quickly create ubuntu-application mattheapplication
		
		change to new application's directory and list its contents
		$ cd matthewsapplication
		$ ls -la
	
		bings up a beatuiful interface
		$ quickly design
		
		change application behavior and edit your python code in gedit
		$ quickly edit

		test your new application by 
		$ quickly run
		
		create a package build of your application which is recommended for testing 
		$ quickly package
		
		integrates with Launchpad to speed up the process of dealing with PPAs
		$ quickly release
		$ quickly share
- Ground Control(groundcontrol)
it is seeks to simplify the collaboration on software projects. it takes the technology and workflows that most developers use and know. it is essentially a version control system built in to the ubuntu desktop
	+ install
	from ubuntu repository or from https://launchpad.net/groundcontrol
- Bikeshed and other Tools, it was started by Dustin Kirkland in September 2010 as a project to package a series of tools he wrote to scratch some personal itches.
	+ apply-patchwraps the patch utility and makes it a little easier to use 
	+ bchdetermines the files that have been modified in the current Bazaar (bzr) tree
	+ bzrpis the same asbzr, except that output is piped to a pager, to make reading easier
	+ col1splits and prints a given column, where the column to print is the name of the script program you are running
	+ dmanremotely retrieves man pages from http://manpages.ubuntu.com
	+ pbgetretrieves content uploaded to a pastebin by pbputor pbputs.
	+ pbputuploads text files, binary files, or entire directory structures to a pastebin
	+ pbputsoperates exactly like pbput, except the user is prompted for a passphrase for encrypting the content with gpgbefore uploading
	+ install 
	https://launchpad.net/bikeshed
- References
http://subversion.apache.org/—The main website for Subversion.
http://bazaar.canonical.com/en/—The main website for Bazaar.
http://mercurial.selenic.com—The main website for Mercurial.
http://git-scm.com/—The main website for Git.
https://launchpad.net/—Launchpad is an open-source website that hosts code, tracks bugs, and helps developers and users collaborate and share.
https://launchpad.net/ubuntu/+ppas—Personal package archives allow source code to be uploaded and built in to .deb packages and made available to others as an apt repository.
https://launchpad.net/quickly 


# Help with ubuntu testing and QA
- test team at lauchpad at https://launchpad.net/~ubuntu-testing
- QA team, ubuntu-qa-tools, a library called Mago for the Linux Desktop Testing Project (LDTP)to simplify testing of Ubuntu within the wider realm of Linux desktops
- Bug Squad
- Test Drive
	+ any good developer who doesn’t want to repeat tasks unnecessarily, he wrote a tool to automate the process
	+ Test Drive is written in a way that enables even a nontechnical Ubuntu user to consistently test Ubuntu development releases. Test Drive is available in the Ubuntu repositories as testdrive. 


# Using Perl
- Perl (the Practical Extraction and Report Lanuage, or the Pathologically Ecletic Rubbish Lister)
- perl code file ususally ending in .pl
- perl programs are used to support a number of ubuntu services, such as system logging, if you install the logwatch package the logwatch.pl program is run every morning at 6:25a.m. other ubuntu services supported by Perl include the following
	+ Amanda for local and network backups
	+ Fax spooling with the faxrunqd program
	+ Printing supported by Perl document-filtering programs
	+ Hardware sensor monitoring setup using the sensors-detech Perl program
- Perl Versions
	+ download the perl code from http://www.perl.com
	+ check install version perl -v
- Simple Perl Program
	+ example
	#!/usr/bin/perl
	print "message";
	
	make the code executable by
	$ chmod +x trivial.pl
	
	use perl to run program
	$ perl *.pl
- code block is wrap with {};
- use perl doc
$ perldoc perldoc

get introductory information on Perl
$ perldoc perl
$ man perl

overview or table of contents of Perl's documentation 
$ perldoc perltoc

read perlfunc document, which list all the available perl functions and their usage or from website http://perldoc.perl.org/

- PerlVariables and Data Structures
perl is a weakly typed language
	+ variable types, scalars, arrays, hashes
		* Scalar variables are indicated with the $ character as in $penguin, can be numbers or strings, they can change type from one to the other as needed
		"76fasfd" if used in numberic calculation will evaluates to 76 and string will evaluate to 0
		
		* arrays are indicated with @ character, in @fish, each element of array is a scalar value, an single element in array indicated with a $ character, start from zero
		
		* hashes are indicated with the % character as in %employee, a hash is a list of name and value pairs. value are reference by name
		$employee{name}
		
		init hashes
		%mail = ( To => ‘you@there.com’,
				From => ‘me@here.com’,
				Subject => ‘A Sample Email”,
				Message => “This is a very short message”
				);

		* example display all the values in your environment, much like typing the bash shell's env command
		#!/usr/bin/perl
		foreach $key (keys %ENV){
			print "$key=$ENV{$key}\n";
		}

	+ Special variables, usually look like punctuation $_, $!, and $] are all extremely useful for shorthand code. 
	$_ is the default variable, 
	$! is the error message return by the operating system 
	$] is the Perl version number
	
	the complete listing special variable could be found at perlvar man page
	
	+ Operators
		* comparison, similar to C
		<=> returns -1 if less than, 0 if equal and, 1 if greater than
		.. range of >= first operand to <= second operand
		
		string comparison operators in perl
		eq, lt, gt, le, ge, ne not equal, cmp return -1 if less than, 0 if equal and 1 if greater than, =~ matched by regular expression
		
		* compound, similar to C 
		&&, ||, !, ()
		
		* arithmetic
		x**y, power, same as x^y
		x%y, calculates the remainder of x/y
		+,-,*,/,-,++,--, =, +=, ..  similar to C
		evaluate to the value 0 for false and 1 for true
		
		* other operators
		~x, bitwise not
		x&y, bitwise and
		x|y, bitwise or
		x^y, bitwise exclusive(XOR)
		x<<y, shift left
		x>>y, shift right
		x.y, concatenate y onto x
		a x b, repeasts string a for number times 
		x, y, comma operator evaluates x and then y
		x? y:z, conditional expression
		
		except for the comma operator and conditional expression, these operators can also be used with the assignment operator
	
	+ Special string constants
	perl supoprts string constants that have special meaning or cannot be entered from the keyboard
	\\, including a backslash
	\a, alert or bell 
	\b, backspace
	\cC, control character, (holding ctrl down and press c)
	\e, escape
	\f, formfeed
	\n, newline
	\r, carriage return
	\t, tab
	\xNN, indicates that NN is a hexadecimal number
	\0NNN, indicates that NNN is a octal number
- conditional statements
if/else and uless
	+ if
	if(condition){
		statement or block of code
	}elseif (condition) {
	} else {
	}
	everything in perl is true except 0, "0", "" and a undefined value
	
	+ unless
	unless(condition){
	}
- looping
for, foreach, while and until
	+ for
	for(start condition; end condition; increment function){
	}
	
	+ foreach, construct performs a statement block for each element in a list or array
	@names = ("a", "b");
	foreach $name(@names){
		print "$name sounding off!\n";
	}
	the loop variable is not merely set to the value of the array, it is aliased to that element, if no loop array is specified, the default variable $_ is may be used
	@names = ("a", "b");
	foreach (@names){
		print "$_ sounding off!\n";
	}

	+ while
	while performs a block of statements as long as a particular condition is true
	while($x < 10){
		print "$x \n";
		$x++;
	}
	
	+ until, exact opposite of the while, it performs a block of statement as long as the condition is false
	until(condition){
	}
	
	+ last and next
	force perl to end a loop early by last statement, similar to C break; continue with next statement
	unfortunately, these statments do not work with do...while
	
	however, you can use redo to jump to a loop(marked by a label) or inside the loop where called
	$a = 100;
	while(1){
		print "start\n";
		TEST:{
			if(($a = $a/2)>2){
				print "$a \n";
				if(--$a < 2){
					exit;
				}
				redo TEST;
			}
		}
	}
	
	+ do...while and do...until
- Regular Expressions
greatest strength is in text and file manipulation, regex library
	+ example the following line of code replaces every occurrence of the string bob or the string mary with fred in a line of text
	$string =~ s/bob|mary/fred/gi;

	$string = ~, performs this pattern match on the text found in the variable called $string
	s, substitute		
	/, begins the text to be matched
	bob|mary, match bob or mary
	fred, replaces anything that was matched with text fred
	g, does this substitution globally; that is replace the match text whereever in string
	i, not case sensitive
	; indicates the end of the line of code
	
	+ check the regex man page for detail
- Access to shell, perl can perform for you any process to the sell through the \\ syntax
prints a directory listing
$curr_dir = `pwd`
@listing = `ls -al`;
print "listing for $curr_dir\n";
foreach $file (@listing){
	print "$file";
}
	+ NOTE 
	the notation \\ uses the backtick found above the Tab key, not the single quotation mark

	+ you could also use shell module to access shel
	use Shell qw(cp);
	cp ("/home/httpd/logs/access.log", "/tmp/httpd.log");

	+ via the system function call
	$rc = 0xffff & system('cp /home/httpd/logs/access_log /tmp/httpd.og');
	if ($rc == 0){
		print "system cp successed\n";
	}else{
		print "system cp failed $rc\n";
	}

	the call can also be used with the or die clause
	system('..') == 0 or die "system cp faield: $?"
	however you cannot capture the ouput of a command executeed through the system function
- Modules and CPAN
greate strength of the Perl community is fact that it is open-source, Comprehensive Perl Archive Network(CPAN)
most of CPAN is made up of modules, which are reusable chunks of code that do useful things

if you need to use a module not installed with ubuntu, use the CPAN module to download and install other modules onto your system
http://www.perl.com/CPAN

access to CPAN
$ perl -MCPAN -e shell

load module into memory
use Time::CTime;
use looks in the directories listed in the svariable @INC for the module. in this example use looks for a directory called Time, which contains a file called CTime.pm which in turn is assumedd to contain a package called Time::CTime

standard perl modules, check perlmodlib in the perl documentation
$ perldoc perlmodlib

- code example 
	+ sending mail
- using perl to install a CPAN module
$ perl -MCPAN -e shell
to have perl examine your system and then download and install a large number of modules use install
cpan> install Bundle::CPAN

to download a desired module
cpan> get Mail::Sendmail
the source for the module will be download into the .cpan directory, you can then build and install the module using the install keyword like this
cpan> install Mail::Sendmail

the use perl command line to finish all of the above process
$ perl -MCPAN -e "install Mail::Sendmail"
- Perl does not interpolate(evaluate variables) within single quotation marks
- Purging Logs
example to remote test machine log from the log file

#!/usr/bin/perl
# Be careful using this program!
# This will remove all lines that contain a given word
# Usage: remove <word> <file>
$word=@ARGV[0];
$file=@ARGV[1];
if ($file) {
	# Open file for reading
	open (FILE, “$file”) or die “Could not open file: $!”; 
	@lines=<FILE>;
	close FILE;
	# Open file for writing
	open (FILE, “>$file”) or die “Could not open file for writing: $!”;
	for (@lines) {
		print FILE unless /$word/;
	} # End for
	close FILE;
} else {
	print “Usage: remove <word> <file>\n”;
} # End if...else
- Relative ubuntu and linux commands
You will use these commands and tools when using Perl with Linux:
a2p, A filter used to translate awkscripts into Perl.
find2perl, A utility used to create Perl code from command lines using the find command.
perldoc, A Perl utility used to read Perl documentation.
s2p, A filter used to translate sed scripts into Perl.
vi, The vi(actually vim) text editor.
- References
http://www.pm.org, The Perl Mongers are local Perl users groups
Sam teach you perl in 21 days


# Using PHP
- it as a web scripting language and a command line tool. PHP originally stood for personal home page because it was a collection of Perl scripts, it has received two major updates(PHP3 and PHP 4), substantial revision in PHP5, which is the version bundled with Ubuntu
PHP5 two big new data storage mechanisms were introduced, sqlite and simpleXML
	+ NOTE
	many package are avaliable on ubuntu, such as php5-ldap, php5-mysql, php5-pgsql
- entering and exiting php mode
<?php
<?
<%
<script language="php">
the first option is preferred method of php mode because it is guaranteed to work
- variables
all variable start with a dollar sign($), not matter what kinds of the type of the variable are.
<?php
$i = 10;
$j = “10”;
$k = “Hello, world”;
echo $i + $j;
echo $i + $k;
?>
PHP has a complex output mechanism that enables you to print to a console, send text through Apache to a web browser, send data over a network, and more.

integer, Whole numbers; for example, 1, 9, or 324809873
float, Fractional numbers; for example, 1.1, 9.09, or 3.141592654
string, Characters; for example, “a”, “sfdgh”, or “Ubuntu Unleashed”
boolean, True or false
array, Several variables of any type
resource, Any external data

Of all the complex variable the easiest to grasp are resources. for all external systems(such as call java, graphics, ...), they need to have types of data unique to them that PHP cannot represent using any of the six other data types. So, PHP stores their custom data types in resources—data types that are meaningless to PHP but can be used by the external libraries that created them.

- Arrays
array is made up of many elements
each element has a key that defines its place in the array. an array can have only one element
each element also has a value, which is the data associate with the key
each array has a cursor, which points to the current key
floating point numbers can't used as key in array

	+ add value to an array with [] operator or array() pseudo-function
	<?php
		$myarr = array(1, 2, 3, 4);
		$myarr[4] = “Hello”;
		$myarr[] = “World!”;
		$myarr[“elephant”] = “Wombat”;
		$myarr[“foo”] = array(5, 6, 7, 8);
		echo $myarr[2];
		echo $myarr[“elephant”];
		echo $myarr[“foo”][1];
		$myarr = array();
	?>

- constants
constants are frequently used in functions, create constants of your own by using the define() function
<?php
	define(“NUM_SQUIRRELS”, 10);
?>
- References
using the equal sign(=) copies the value from one variable to another so they both have their own copy of the value
instead of copy use & symbol to point to another variable
<?php
$a = 10;
$b = &$a;
?>
- Comments
three options, //, /* */ and #
	+ NOTE
	contrary to popular belief, having comments in your PHP scripts has almost no effect on the speed at which script executes. What little speed difference exists is wholly removed if you use a code cache
- Escape Sequences, the escape logic is similar to C
- Variable Substitution
define string in three methods, single quotation marks, double quotation marks, or heredoc notation, single quotation is dont do variable substitution
- Operators, similar to javascript except
., it is used to concatenate string 
- Conditional statement
if (your condition) {
} else {
}
- Special operators
$age_description = ($age < 18) ? “child” : “adult”;

`, execution operator executes the program inside the backticks, returning any text the program outputs. 
$i = `ls –l’;
- switching, similar to C
- loops
while, for, foreach, do...while

example
<?php
$i = 10;
while ($i >= 10) {
$i += 1;
echo $i;
}
?>

<?php
foreach($myarr as $key => $value) {
echo “$key is set to $value\n”;
}
?>
- including other files
include "*.php";
- basic functions
PHP has a vast number of built-in functions enable you to manipulate strings, connect to database and more, we will introduce several kinds of these
	+ strings
	The easiest function is strlen()
	<?php
	$ourstring = "asdfadsf";
	echo strlen($ourstring);
	?>
	
	trim(), str_replace(), substr()...
	
	+ Arrays
	provide function such as sort, shuffle, intersect and filter, array_unique() will remove all dummy elements, in_array() check a element is contain in an array

	$myarr = array(“foo” => “red”, “bar” => “blue”, “baz” => “green”);
	$mykeys = array_keys($myarr);
	
	array_keys will create a new array use the keys of the previous array

	+ Files
	the UNIX philosophy is that everying is a file, in PHP this is also the case A selection of basic file functions is suitable for opening and manipulating files. same functions can also be used for opening and manipulating network socket

	<?php
	$text = file_get_contents(“filea.txt”);
	file_put_contents(“fileb.txt”, $text);
	?>

	<?php
	$text = file_get_contents(“http://www.slashdot.org”);
	file_put_contents(“fileb.txt”, $text);
	?>
	
	open the file piece by piece, fopen(), fclose(), fread(), fwrite(), and feof(). 
	options
	r,Read-only; it overwrites the file.
	r+,Reading and writing; it overwrites the file.
	w,Write-only; it erases the existing contents and overwrites the file.
	w+,Reading and writing; it erases the existing content and overwrites the file.
	a,Write-only; it appends to the file.
	a+,Reading and writing; it appends to the file.
	x,Write-only, but only if the file does not exist.
	a+,Reading and writing, but only if the file does not exist.

	<?php
	$file = fopen(“filea.txt”, “rb”);
	while (!feof($file)) {
	$content = fread($file, 1024);
	echo $content;
	}
	fclose($file);
	?>
	
	+ Miscellaneous
	isset(), which takes one or more variables as its parameters and returns true if they have been set
	
	The unset()function also takes one or more variables as its parameters, simply deleting the variable and freeing up the memory. 
	
	var_dump(), which dumps out information about a variable, including its value, to the screen
	
	exit() terminates the processing of the script as soon as it is executed, meaning subsequent lines of code are not executed.
	
	regex
	Perl-Compatible Regular Expressions (PCRE)or POSIX Extended regular expressions, but there really is little to choose between them in terms of functionality offered.
	The main PCRE functions arepreg_match(), preg_match_all(), preg_replace(), and preg_split().
- Handling HTML Forms
	+ form example
	<form method=”POST” action=”thispage.php”>
	User ID: <input type=”text” name=”UserID” /><br />
	Password: <input type=”password” name=”Password” /><br />
	<input type=”submit” />
	</form>
	POST method is more securer to GET method
- Databases
PEAR is the script repository for PHP and contains numerous tools and prewritten solutions for common problems
PEAR::DBis to include the standard PEAR::DBfile, DB.php. Your PHP will be configured to look inside the PEAR directory for include()files

<?php
include(“DB.php”);
$dsn = “mysql://ubuntu:alm65z@10.0.0.1/dentists”;
$conn = DB::connect($dsn);
if (DB::isError($conn)) {
echo $conn->getMessage() . “\n”;
} else {
echo “Connected successfully!\n”;
}
?>
- References   
http://www.php.net, The best place to look for information 
http://www.phpbuilder.net, A large PHP scripts and tutorials site	 
http://www.zend.com, The home page of a company founded by two of the key developers of PHP	
http://pear.php.net/—The home of the PEAR project	
http://www.phparch.com/—There are quite a few good PHP magazines around	


# Using Python
- add she-bang to make python script executable
#!usr/bin/python
- object oriented 
	+ constructor
	__init__
	+ deconstructor
	__del__(self)


# C/C++ Programing tools for ubuntu
- Compile code is GNU Compiler Collection(gcc) 
- Why Use C or C++
If your computer costs $100 million or more, then having a programmer spend an extra month, or year, or decade working on software optimization is well worth it. Toughen up and write in C.
If your computer costs $2,000, the programmer’s time is the dominant cost.
- NOTE 
linux kerenel is mostly written in C, which is why linux works with os many different CPUs
C++ is an object-oriented extension to C. Because C++ is a superset of C, C++ compilers compile C programs correctly
- Using the C Programming Project Management Tools Provided with Ubuntu
create programs(editors)
compile programs(gcc)
create libraries(ar)
control the source(GIT, Mercurial, Subversion)
automate builds(make)
debug programs(gdb and ddd)
determine where inefficiencies lie(gprof)
- Build programs with make
	+ using make files
	make command automatically builds and updates applications by using a makefile. A makefile is a text file that contains instructions about which options to pass on to the compiler preprocessor
	make command works with nearly any program including text processing systems such as TeX
	
	+ simple command
	this will use the default makefile called Makefile, or with -f option to specify any makefile
	$ sudo make install
	$ sudo make install -f any_makefile
	
	check make's build in rules by  make -p

	+ using macros and makefile targets
	macros allow users of other operation systems to easily configure a program build by specifying local values, it could make a program portable
	such as use macros define the name of the compiler(CC), the installer program(INS), where the program should be install (INSDIR), where the linker should look for required libraries(LIBDIR), the name of the required libraries(LIBS), a source code file(SRC), the intermediate object code file(OBJS), and the name of the final program(PROG):
	
	;# a sample makefile for a skeleton program, and save as Makefile
	CC= gcc
	INS= install
	INSDIR = /usr/local/bin
	LIBDIR= -L/usr/X11R6/lib
	LIBS= -lXm -lSM -lICE -lXt -lX11
	SRC= skel.c
	OBJS= skel.o
	PROG= skel
	skel: ${OBJS}
		${CC} -o ${PROG} ${SRC} ${LIBDIR} ${LIBS}
	install: ${PROG}
		${INS} -g root -o root ${PROG} ${INSDIR}
		
	build the program like
	$ sudo make
	
	to build a specified component of a makefile
	$ sudo make skel
	
	make will automatic handle changes, compile and install program in on step by 
	$ sudo make install
	
	large software project might have a number of traditional targets in the makefile, such as following
	test, to run specific test on the final software
	man, to process an include or a troff document with the man macros
	clean, to delete any remaining object files
	archive, to clean up archive and compress the entire source code tree
	bugreport, to automatically collect and then mail a coppy of the build or error logs
	
	+ the indented lines in the previous example are indented with tabs, not spaces.

- using the auto conf utility to configure code
there are other automatic tools such as pmake(which cause a parallel make), imake(which is a dependency-driven makefile generator that is used for building X11 clients), automake, and one of the newer tools, autoconf which builds shell scripts that can be used to configure program source code package
many GNU software packages for linux that are distributed in source form requires the use of GNU's autoconf utility. This program builds an executable shell script named configure. 
root may use script like this to configure X downloaded code
$ ./configure ; make ; sudo make install
the autoconf program uses a file named configure.in that contains a basic ruleset, or set of macros. The configure.in file is created with autoscan command.
Building a properly executing configure script also requires a template for the makefile named Makefile.in
Creating the dependency-checking configure script can also be done manually
- Debug tools
three of these tools are splint, gprof and gdb
	+ using splint to check source code
	splint is similar to the UNIX lint command, it statically examine source code for possible problems
	$ splint *.c
	
	use -strict option to get a more verbose report
	
	gcc also support diagnostics through the use of extensive warnings(-Wall and -pedantic options)
	$ gcc -Wall *.c
	
	+ using gprof to track function time
	check a program is spending its time. if a program is compiled and linked with -p as a flag, a mon.out file is created when it executes, with data on how often each function is called and how much time is spent in each function
	
	+ doing symbolic debugging with gdb
	gdb is a symbolic debugger, when a program is compiled with -g flag. the symbol tables are retained and a symbolic debugger can be used to track program bugs. The basic technique is to invoke gdb after a core dump( a file containing a snapshot of the memory used by a program that has crashed) and get a stack trace
	
	gdb also provides an environment for debugging in programs interactively. invoking gdb with a program enables to set breakpoints, examine the value of a variables
	
	a graphical X window interface to gdb is called the data display debugger or ddd
- using the GNU C compiler(gcc)
	+ GNU c compiler is a part of the GNU compiler collection, which also includes compilers for several other languages
	+ build a C program using gccc, take in several steps
		* C preprocessor parses the file and performs macro replacement
		* the compiler parse the modified code to determine whether the correct syntax is used, builds a symbol table and creates an intermediate object format
		* last compilation stage, linking, ties together different files and libraries and the links the files by resolving the symbols that had not previously been resolved
	
	+ Most C programs compile with a C++ compiler if you follow strict ANSI rules, for example you can compile the standard hello.c program with the GNU C++ compiler, typically you name the file like *.cc, *.c *.c++, *.cxx
- graphical development tools
if you want to build client software for KDE or GNOME, you might find KDevelop, Qt Designer and Glade programs to be helpful
	+ using the KDevelop client
	$ kdevelop &
	+ Glade client for developing in GNOME
	Glade GTK+ GUI builder


# Using Mono
- Mono is a free re-implementation of .net available under the GPL license
could compile with .net 1.0 and 1.1 frameworks (much of the 2.0 framework too)
- Mono 2.10 is installed by default in Ubuntu 11.10
- Why mono
	+ C# language corrects many of the irritations in Java
	+ .NET is designed to let you compile multiple languages 
	+ Mono even has a special project (known as IKVM) that compiles Java source code down to .NET code that can be run on Mono
	+ mono is open source
	+ The C# compiler includes all of the features to compile C# 1.0, 2.0, and 3.0 (ECMA) code.
- c# compiler gives you your pick of several services that applications may consume
mcs, references the 1.0 profile libraries and suports c# 1.0 and c# 3.0 minus generics and any features that depend on generics call ed mono-cms in ubuntu
gmcs, reference the 2.0 profile libraries and support the full c# 3.0 language, install by default ubuntu 1.0
smcs, reference the 2.1 profile libiraries, support full c# 3.0 and is used for creating moonlight(mono version of silverlight), it is not available in ubuntu software repositories
dmcs, target the c# 4.0 language, is not available in the ubuntu software
- Mono also runs on Gtk+, LDAP, and POSIX
- MonoDevelop
	+ install mono-complete and monodevelop from the ubuntu repositories, these are the basic mono development tool
	+ gnome-sharp2 and gtk-sharp2 is used to develop GUI program
- Create gui wiith Gtk#, monoDevelop comes with his own GUI designer called GTK# Designer
- References
http://www.mono-project.com/, the home page of the Mono project
http://www.monodevelop.com/, the monodevelop project 
http://icsharpcode.net/OpenSource/SD/, MonoDevelop started life as a port of SharpDevelop. If you happend to dual-boot on windown this might prove very useful to you
http://msdn.microsoft.com/vscharp/, the home page of c# project
general book on the .net framework and all the features it provides you might find at .NET Framework Essentials(ISBN:0-596-00505-9)


# Using Other popular programming language
- Ada is based on pascal, named after Ada Lovelace, its use in embedded system and efficient real-time language
- Clojure, a newer dialect of Lisp, runs on the JVM, you don't need to install itself, it's just a library that's loaded into the JVM
- COBOL, common business oriented language, dominant language for (legacy) business applications
- Erlang, is a programming language used to build massively scable soft real-time systems. strength is ability to create a large number of concurrent processes
- Forth, interactive procedural, imperative language with typeless data that runs as a shell. Sets of instruction can be saved and compiled as byte code programs, very small and useful in boot loaders and embedded systems, even been used by NASA
- Fortran, developed by IBM, for engineering and scientific application. ideal for high-performance computing
- Groovy, designed for the JVM, to enable features like closures and dynamic typing from popular languages
- Haskell, purely functional programming language. It has built-in concurrency and parallelism and good support for integration with other languages, it's similar to erlang
- Java
- JavaScript, object-oriented functional programming language designed primarily for scripting
- Lisp, popular as a program for reserach in artificial intelligence, the most commonly used "regular lisp" is ANSI Common Lisp, use clisp in ubuntu
another lisp dialect is scheme
- lua, created in brazil, it is a dynamically typed procedural language with memory management and garbage collection, it's small nad often used for embedded applications
- ruby
- Scala, scalabel language, designed to grow with its user's needs, runs on JVM suited for both functional and object-oriented programming
- Vala, very new, designed to make the lives of the developers of GNOME easier by bringing features from modern language into C for use in GNOME desktop environment, syntax is very similar to C#, it's a compiled language

# Begin Mobile Development for Android
- install the ADT Eclipse Plug-in
- Android SDK and AVD Manager
- Install virtual environment

# Ubuntu and Linux Internet Resources
- technical support a 24/7 onsite basis by phone by email, http://www.ubuntu.com/support
- Linux certification courses
http://www.ubuntu.com/support/training/
- commercial support
http://www.ubuntu.com/support/services
http://www.hp.com/linux
http://www.ibm.com/linux
- Joining a linux user group
http://www.tux.org/luglist.html for ubuntu specific groups, check out Ubuntu Local Community, or LoCo Teams at http://locol.ubuntu.com/
- Documentation
/usr/share/doc/directory
- linux guides
advanced bash-scripting guide
lpd author guide
linux administration made easy
linux consultants guide
linux from scratch
linux kernel module programming guide
securing and optimizing linux
the linux network administrator's guide second edition
- ubuntu
http://www.ubuntforums.org
http://www.ubuntu.com
- mini-cd linux distributions
create a linux bootable business card, typically fitting on a 40MB or 50MB credit card-size
http://www.lnx-bbc.com, home page for linux bbc a 40MB image hosting a rather complete live linux session with X, a web browser and a host of networking tools
http://crux.nu/, home page of the CRUX i686-optimized linux distribution
http://www.smoothwall.org, 69MB smooth wall distribution is used to install a web administratered firewall
- various intel-based linux distributions
many linux users prefer Red Hat's distro because of its excellent support, commercial support options. check the http://www.distrowatch.com
http://www.debian.org, the debian linux distribution consisting only of software distributed under GPL
http://www.slackware.com, newest version of one of the oldest linux distribution, slackware
http://www.openuse.org, home page for SUSE linux
http://fedoraproject.org/, popular distribution closely related to Red Hat
http://www.redhat.com/, one of the most used distributions in the corporate and server world
- powerPC-based linux distributions
powerPC processors were created by an alliance between Apple, IBM and Motorola, used extensively in video game consoles, embedded applications and were the processor used in Apple's Macintosh computers from 1994 to 2006
http://penguinppc.org/ home page for the powerPC GNU/linux distributtion
http://www.yellowdoglinux.com, for Terra soft solution's yellow dog, based on Fedora
- Linux on laptops and PDAs
http://www.linux-laptop.net
http://www.tuxmobil.org
- The X Window system
http://www.x.org
- usenet newsgroups
http://groups.google.com, primary linux nad linux related newsgroups are as follows
alt.os.linux.dial-up—Using PPP for dial-up
alt.os.linux.mandriva—All about Mandriva Linux
alt.os.linux.slackware—Using Slackware Linux
alt.os.linux.ubuntu—Using Ubuntu Linux
comp.os.linux.advocacy—Heated discussions about Linux and other related issues
comp.os.linux.alpha—Using Linux on the Alpha CPU
comp.os.linux.announce—General Linux announcements
comp.os.linux.answers—Releases of new Linux FAQs and other information
comp.os.linux.development.apps—Using Linux development tools
comp.os.linux.development.system—Building the Linux kernel
comp.os.linux.embedded—Linux embedded device development
comp.os.linux.hardware—Configuring Linux for various hardware devices
comp.os.linux.m68k—Linux on Motorola’s 68K-family CPUs
comp.os.linux.misc—Miscellaneous Linux topics
comp.os.linux.networking—Networking and Linux
comp.os.linux.portable—Using Linux on laptops
comp.os.linux.powerpc—Using PPC Linux
comp.os.linux.security—Linux security issues
comp.os.linux.setup—Linux installation topics
comp.os.linux.x—Linux and the X Window System
comp.windows.x.apps—Using X-based clients
comp.windows.x.i386unix—X for UNIX PCs
comp.windows.x.intrinsics—X Toolkit library topics
comp.windows.x.kde—Using KDE and X discussions
comp.windows.x.motif—All about Motif programming
comp.windows.x—Discussions about X
linux.admin.*—Two newsgroups for Linux administrators
linux.debian.*—30 newsgroups about Debian
linux.dev.*—25 or more Linux development newsgroups
linux.help—Get help with Linux
linux.kernel—The Linux kernel
- ubuntu project mailing lists
http://lists.ubuntu.com/mailman/listinfo/ubuntu-security-announce
http://lists.ubuntu.com/mailman/listinfo/ubuntu-announce
http://lists.ubuntu.com/mailman/listinfo/ubuntu-users
http://lists.ubuntu.com/mailman/listinfo/ubuntu-devel
- Internet relay chat
Internet relay chat(IRC) is a popular form and forum of communication for many linux developers and users
http://www.freenode.net/
http://www.irchelp.org

	
# ubuntu issues
- fix login problems
http://askubuntu.com/questions/278762/ubuntu-12-04-login-returns-to-login


# clear screen 
$ clear 

# Set up git server on ubuntu server 
cd /etc/network 

sudo cp interfaces interfaces.backup 
sudo vi interfaces 
//check documentation of ubuntu 

$ apt-get install ssh-server 
$ sudo vi /etc/ssh/sshd_config 

//start ssh 
$ sudo restart ssh 

//add user 
$ sudo adduser gituser
$ su gituser
$ cd ~
$ mkdir .ssh 
$ chmod 700 .ssh 
$ touch .ssh/authorized_keys 
$ chmod 600 .ssh/authorized_keys 

//check git document and generate client public and private keys 
$ ssh-keygen
$ cd ~/.ssh 
//copy the id_rsa.pub file to the server 
$ rename the file to id_rsa.username.pub 
//append the content to the authorized_keys file 
$ cat /tmp/id_rsa.username.pub >> ~/.ssh/authorized_keys 

//after this we could ssh to the git server from the client 
$ ssh gituser@server-ip 

//install server on the server 
$ apt-get install git 
$ cat /etc/shells 
// we change the default login shell for the gituser to improve security 
//add git shell path to /etc/shells 
$ which git-shell
$ sudo vi /etc/shells 

//switch the default shell 
$ sudo chsh git 
/usr/bin/git-shell 

now the client can not login the command prompt for the server 
//create a bare repo on the server 
$ cd /opt 
$ mkdir git 
$ cd git 
$ mkdir project-name.git 
$ cd project-name.git 
$ git init --bare 
$ sudo chown -R git:git git 

//install git on the client 
// add git information 
$ git config --global user.name ''
$ git config --global user.email ''
$ git config --global core.editor gedit 

//create a git repo and add remote and point to the server 
$ git init 
$ git remote add origin git@gitserver:/opt/git/project-name.git 
$ git push origin master 
$ apt-get install git-gui 

- reference 
https://www.bing.com/videos/search?q=set+up+git+server+on+ubuntu&view=detail&mid=F02419A0A7DAE5CC55A1F02419A0A7DAE5CC55A1&FORM=VIRE

# Shotcuts 
Ctrl + C = Copy the highlighted content to clipboard
Ctrl + V = Paste the clipboard content
Ctrl + N = New (Create a new document, not in terminal)
Ctrl + O = Open a document
Ctrl + S = Save the current document
Ctrl + P = Print the current document
Ctrl + W = Close the close document
Ctrl + Q = Quit the current application
Keyboard shortcuts for GNOME desktop

Ctrl + Alt + F1 = Switch to the first virtual terminal
Ctrl + Alt + F2(F3)(F4)(F5)(F6) = Select the different virtual terminals
Ctrl + Alt + F7 = Restore back to the current terminal session with X
Ctrl + Alt + Backspace = Restart GNOME
Alt + Tab = Switch between open programs
Ctrl + Alt + L = Lock the screen.
Alt + F1 = opens the Applications menu
Alt + F2 = opens the Run Application dialog box.
Alt + F3 = opens the Deskbar Applet
Alt + F4 = closes the current window.
Alt + F5 = unmaximizes the current window.
Alt + F7 = move the current window
Alt + F8 = resizes the current window.
Alt + F9 = minimizes the current window.
Alt + F10 =  maximizes the current window.
Alt + Space = opens the window menu.
Ctrl + Alt + + = Switch to next X resolution
Ctrl + Alt + – = Switch to previous X resolution
Ctrl + Alt + Left/Right = move to the next/previous workspace
Keyboard shortcuts for Terminal

Ctrl + A = Move cursor to beginning of line
Ctrl + E = Move cursor to end of line
Ctrl + C = kills the current process.
Ctrl + Z = sends the current process to the background.
Ctrl + D = logs you out.
Ctrl + R = finds the last command matching the entered letters.
Enter a letter, followed by Tab + Tab = lists the available commands beginning with those letters.
Ctrl + U = deletes the current line.
Ctrl + K = deletes the command from the cursor right.
Ctrl + W = deletes the word before the cursor.
Ctrl + L = clears the terminal output
Shift + Ctrl + C = copy the highlighted command to the clipboard.
Shift + Ctrl + V (or Shift + Insert) = pastes the contents of the clipboard.
Alt + F = moves forward one word.
Alt + B = moves backward one word.
Arrow Up/Down = browse command history
Shift + PageUp / PageDown = Scroll terminal output
Keyboard shortcuts for Compiz

Alt + Tab = switch between open windows
Win + Tab = switch between open windows with Shift Switcher or Ring Switcher effect
Win + E = Expo, show all workspace
Ctrl + Alt + Down = Film Effect
Ctrl + Alt + Left mouse button = Rotate Desktop Cube
Alt + Shift + Up = Scale Windows
Ctrl + Alt + D = Show Desktop
Win + Left mouse button = take screenshot on selected area
Win + Mousewheel = Zoom In/Out
Alt + Mousewheel = Transparent Window
Alt + F8 = Resize Window
Alt + F7 = Move Window
Win + P = Add Helper
F9 = show widget layer
Shift + F9 = show water effects
Win + Shift + Left mouse button = Fire Effects
Win + Shift + C = Clear Fire Effects
Win + Left mouse button = Annotate: Draw
Win + 1 = Start annotation
Win + 3 = End annotation
Win + S = selects windows for grouping
Win + T = Group Windows together
Win + U = Ungroup Windows
Win + Left/Right = Flip Windows
Keyboard shortcut for Nautilus

Shift + Ctrl + N = Create New Folder
Ctrl + T = Delete selected file(s) to trash
Alt + ENTER = Show File/Folder Properties
Ctrl + 1 = Toggle View As Icons
Ctrl + 2 = Toggle View As List
Shift + Right = Open Directory (Only in List View)
Shift + Left = Close Directory (Only in List View)
Ctrl + S = Select Pattern
F2 = Rename File
Ctrl + A = Select all files and folders
Ctrl + W = Close Window
Ctrl + Shift + W = Close All Nautilus Windows
Ctrl + R = Reload Nautilus Window
Alt + Up = Open parent directory
Alt + Left = Back
Alt + Right = Forward
Alt + Home = go to Home folder
Ctrl + L = go to location bar
F9 = Show sidepane
Ctrl + H = Show Hidden Files
Ctrl + + = Zoom In
Ctrl + – = Zoom Out
Ctrl + 0 = Normal Size

- reference 
https://help.ubuntu.com/stable/ubuntu-help/shell-keyboard-shortcuts.html


# Set proxy for ubuntu apt-get 20
- add file 
/etc/apt/apt.conf

Acquire::http::proxy "http://<proxy>";
Acquire::https::proxy "https://<proxy>";
Acquire::ftp::proxy "ftp://<proxy>";
Acquire::socks::proxy "socks:<proxy>";

- ubuntu install vmware tool 
1. 执行sudo apt-get update, 如果无法安装说明网络连接出现问题, 修改网络链接方式为桥接模式. 
2. 执行sudo apt-get install net-tools, 安装net-tools. 
3 .然后重新执行安装vmware-tools.pl , 成功. 重启系统就可使用tools. 
(vmware-tools要有可执行权限)	

- global proxy 

$ export http_proxy=http://child-prc.intel.com:913
$ export https_proxy=http://child-prc.intel.com:913
$ export socket5_proxy=http://child-prc.intel.com:913

    + save to user profle 
    
$ nano ~/.bash_profile
 
or 

$ nano ~/.bashrc

- specific proxy 
~/.wgetrc 
~/.curlrc
$ git config — global http.proxy ...
$ npm set proxy ...
	
	
# Instal development tools 
- update and install 
$ sudo apt update
$ sudo apt upgrade 
$ sudo apt install g++ gdb make ninja-build rsync zip


# Manually install python 310 
- reference 
https://tecadmin.net/how-to-install-python-3-10-on-ubuntu-debian-linuxmint/

$ sudo apt update && sudo apt upgrade  
$ sudo apt install wget build-essential libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev libffi-dev zlib1g-dev 
$ wget https://www.python.org/ftp/python/3.10.2/Python-3.10.2.tgz 
$ tar xzf Python-3.10.2.tgz 
$ cd Python-3.10.2 
$ ./configure --enable-optimizations 
$ make altinstall 

    + build and install normal package 
$ make
$ sudo make install	
    
# Add environment variable 
export PATH=$PATH:/path/to/executable	


