Docker Deep Dive - Fifth Edition=Nigel;Note=Erxin

# Big picture stuff 
- old days 
vmware 
wmwarts 
containers 
- docker container relative standards
- un Linux containers on Windows systems that have the WSL 2 backend installed.
- Wasm (WebAssembly) is a modern binary instruction set that builds applications that are smaller, faster, more secure, and more portable than containers.

- docker client and engine 

cli -> server (engine)
- container relative standard projects 

 Some of these include:

The OCI
The CNCF
The Moby Project

-  OCI maintains three standards called specs,  Open Container Initiative (OCI) is a governance council responsible for low-level container-related standards.

The image-spec
The runtime-spec
The distribution-spec

Docker implement all three OCI specs. For example:

The Docker builder (BuildKit) creates OCI compliant-images
Docker uses an OCI-compliant runtime to create OCI-compliant containers
Docker Hub implements the OCI distribution spec and is an OCI-compliant registry

- The Cloud Native Computing Foundation (CNCF) is another Linux Foundation project that is influential in the container ecosystem. I

three phases or stages:

Sandbox
Incubating
Graduated

- Docker uses at least two CNCF technologies — containerd and Notary.


# Get docker 
- docker desktop, Docker Desktop is a desktop app from Docker, Inc. and is the best way to work with containers. You get the Docker Engine, a slick UI, all the latest plugins and features

    + Windows requires all of the following:

64-bit version of Windows 10/11
Hardware virtualization support must be enabled in your system’s BIOS
WSL 2
- multipass, Multipass installations don’t ship with out-of-the-box support for features such as docker scout, docker debug, and docker init.

only need three commands:

$ multipass launch 
$ multipass ls
$ multipass shell

$ multipass launch docker --name node1

$ multipass launch docker --name node1

$ docker --version
Docker version 26.1.0, build 9714adc

$ docker info

- server installed on linux 

latest instructions.

$ sudo snap install docker

$ sudo docker --version
Docker version 27.2.0, build 3ab4256

$ sudo docker info

create a docker group and add your user account to it.

$ sudo groupadd docker

. Yours may be different.

$ sudo service docker start


# The big picture 
- ops perspective 

 a container image, and running it as a container.

 - you’ll complete all of the following:

Check Docker is working
Download an image
Start a container from the image
Execute a command inside the container
Delete the container

- Copying new images onto your Docker host is called pulling. Pull the ubuntu:latest image.

$ docker pull nginx:latest

- docker images command.

$ docker images

-  see the running container.

$ docker ps
CONTAINER ID   IMAGE          COMMAND        CREATED      STATUS      PORTS                  NAMES
e08c35352ff3   nginx:latest   "/docker..."   3 mins ago   Up 2 mins   0.0.0.0:8080->80/tcp   test


- docker run command to start a new container called test from the ubuntu:latest image.

$ docker run --name test -d -p 8080:80 nginx:latest

- Run the following command to attach your shell to a new Bash process inside the container.

$ docker exec -it test bash

-  ps command to list running processes.

root@e08c35352ff3:/# ps -elf

- kill the container using the docker stop and docker rm commands.

$ docker stop test

 take a few seconds for the container to stop.

$ docker rm test

- docker ps command with the -a flag to list all containers

$ docker ps -a
CONTAINER ID    IMAGE    COMMAND    CREATED    STATUS    PORTS    NAMES

- DEV perspective 

Clone an app from a GitHub repo
Inspect the app’s Dockerfile
Containerize the app
Run the app as a container

git CLI for this to work.

$ git clone https://github.com/nigelpoulton/psweb.git

    + List the contents of the application’s Dockerfile.

$ cat Dockerfile
```
FROM alpine
LABEL maintainer="nigelpoulton@hotmail.com"
RUN apk add --update nodejs npm curl
COPY . /src
WORKDIR /src
RUN  npm install
EXPOSE 8080
ENTRYPOINT ["node", "./app.js"]
```

Run the following docker build command to create a new image based on the instructions in the Dockerfile.

$ docker build -t test:latest .

after build check the images. there should have an image called test:latest.

$ docker images
REPO     TAG      IMAGE ID        CREATED          SIZE
test     latest   0435f2738cf6    21 seconds ago   160MB

    + run the commands on single line to run app as a container 

$ docker run -d \
  --name web1 \
  --publish 8080:8080 \
  test:latest

on Docker Desktop, connect to localhost:8080 or 127.0.0.1:8080. If you’re following along on Multipass, connect to your Multipass VM’s 192.168 address on port 8080. Run an ip a | grep 192 command from within the Multipass VM, or run a multipass ls from your local machine to find the address.

    + clean up, terminate the container and delete the image.

$ docker rm web1 -f


# The technical stuff 
- docker engine 

Docker Engine – The TLDR
The Docker Engine
The influence of the Open Container Initiative (OCI)
runc
containerd
Starting a new container (example)
What’s the shim all about
How it’s implemented on Linux

- components of docker 

docker cli -> {docker engine -> plugins & other 
                daemon  
                container d 
                runc 
              }

- Docker Engine had two major components:

The Docker daemon (sometimes referred to as just “the daemon”)
LXC

Relying on LXC posed several problems for the Docker project.

First, LXC is Linux-specific, and Docker had aspirations of being multi-platform.

- Open Container Initiative (OCI)
Around the same time that Docker, Inc. was refactoring the Engine, the OCI was in the process of defining two container-related standards:

Image Specification (image-spec)
Runtime Specification (runtime-spec)

As previously mentioned, runc (pronounced “run see” and always written with a lowercase “r”) is the reference implementation of the OCI runtime-spec. 

- containerd (pronounced “container dee” and always written with a lowercase “c”) is another tool that Docker created while stripping functionality out of the daemon.

Cloud Native Computing Foundation (CNCF). At the time of writing, containerd is a graduated CNCF project, meaning it’s stable and production-ready. 

https://github.com/containerd/containerd/releases

- starting a new container example 
$ docker run -d --name ctr1 nginx

Run a docker ps command to see if the container is running.
$ docker ps
CONTAINER ID   IMAGE    COMMAND                  CREATED         STATUS         PORTS    NAMES
9cfb0c9aacb2   nginx    "/docker-entrypoint.…"   9 seconds ago   Up 9 seconds   80/tcp   ctr1

The daemon can expose the API on a local socket or over the network. 

The daemon communicates with containerd via a CRUD-style API over gRPC.

If you started the NGINX container earlier, you should delete it using the following command.

$ docker rm ctr1 -f

    + graph 

$ docker CLI 
|
v
daemon 
|
V
containerd 
| 
V
ship 
{RC RUNC}

- shim, diagrams in the chapter have shown a shim component.

following benefits:

Daemonless containers
Improved efficiency
Pluggable OCI layer

It reports on the container’s status and performs low-level tasks such as keeping the container’s STDIN and STDOUT streams open.

- implemented on linux, separate binaries:

/usr/bin/dockerd (the Docker daemon)
/usr/bin/containerd
/usr/bin/containerd-shim-runc-v2
/usr/bin/runc


# Working with images 
- follows:

Docker images – The TLDR
Intro to images
Pulling images
Image registries
Image naming and tagging
Images and layers
Pulling images by digest
Multi-architecture images
Vulnerability scanning with Docker Scout
Deleting images

-  Image, Docker image, container image, and OCI image.

image build -> containers(s) Run 

The docker run command is the most common way to start a container from an image.

multiple containers use the same image, you can only delete the image after you’ve deleted all the containers using it.

Windows-based images can be gigabytes in size and take a long time to push and pull.

- pulling

    + example pull redis imageg 

$ docker pull redis

an image that shares a common layer with the Redis image. You’ll learn about this very soon, but images can share layers

- image registries 

build > share > run pipeline.

The most common registry is Docker Hub, but others exist, including 3rd-party internet-based registries and secure on-premises registries. 

Docker Hub has the concept of official repositories that are home to images vetted and curated by Docker and the application vendor.

    + Docker Hub namespace:

nginx: https://hub.docker.com/_/nginx/
busybox: https://hub.docker.com/_/busybox/
redis: https://hub.docker.com/_/redis/
mongo: https://hub.docker.com/_/mongo/

- unofficial repositories that you should be very careful when using.

nigelpoulton/gsd — https://hub.docker.com/r/nigelpoulton/gsd-book/
nigelpoulton/k8sbook — https://hub.docker.com/r/nigelpoulton/k8sbook/

- format for a docker pull command pulling an image from an official repository is

$ docker pull redis:latest

    + Pulling images from unofficial repositories is almost the same as pulling from official repositories — you just need to add a Docker Hub username or organization name before the repository name. 

$ docker pull nigelpoulton/tu-demo:v2

    + pulls the latest image from Brandon Mitchell’s regclient/regsync repo on GitHub Container Registry (ghcr.io).

$ docker pull ghcr.io/regclient/regsync:latest

- You can give a single image as many tags as you want.

two — the b4210d0aa52f image is tagged as latest and v1.

$ docker images
REPOSITORY               TAG       IMAGE ID       CREATED          SIZE
nigelpoulton/tu-demo     latest    b4210d0aa52f   2 days ago       115MB
nigelpoulton/tu-demo     v1        b4210d0aa52f   2 days ago       115MB
nigelpoulton/tu-demo     v2        6ba12825d092   12 minutes ago   115MB

he latest tag refers to the same image as the v1 tag, which is actually older than the v2 image.

- look at all of the following ways to inspect layer information:

Pull operations
The docker inspect command
The docker history command

    + pull the node:latest image and observe it pulling the individual layers. 

$ docker pull node:latest
latest: Pulling from library/ubuntu
952132ac251a: Pull complete
82659f8f1b76: Pull complete
c19118ca682d: Pull complete
8296858250fe: Pull complete
24e0251a0e2c: Pull complete
Digest: sha256:f4691c96e6bbaa99d...28ae95a60369c506dd6e6f6ab
Status: Downloaded newer image for node:latest
docker.io/node:latest

    + Another way to see image layers is to inspect the image with the docker inspect command.

$ docker inspect node:latest
[
    {
        "Id": "sha256:bd3d4369ae.......fa2645f5699037d7d8c6b415a10",
        "RepoTags": [
            "node:latest"

        <Snip>

        "RootFS": {
            "Type": "layers",
            "Layers": [
                "sha256:c8a75145fc...894129005e461a43875a094b93412",
                "sha256:c6f2b330b6...7214ed6aac305dd03f70b95cdc610",
                "sha256:055757a193...3a9565d78962c7f368d5ac5984998",
                "sha256:4837348061...12695f548406ea77feb5074e195e3",
                "sha256:0cad5e07ba...4bae4cfc66b376265e16c32a0aae9"
            ]
        }
    }
]

    + docker history command to inspect an image and see its layer data. However, this command shows the build history of an image and is not a strict list of layers in the final image.

- Base layers
All Docker images start with a base layer, and every time you add new content, Docker adds a new layer.

 you update files and make other changes to images by adding new layers containing the changes.

 . Almost all Docker setups use the overlay2 driver, but zfs, btrfs, and vfs are alternative options.

 the layers will be merged as the unified view of multi layer image to be used in the container.

 - Layers are also shared on the registry side. This means you can store lots of similar images in a registry, and the registry will save space

 already pulled an image by name, you can see its digest by running a docker images command with the --digests flag as shown.

$ docker images --digests alpine

before pulling it, you can use the docker buildx imagetools command. The following example retrieves the image digest for the nigelpoulton/k8sbook/latest image on Docker Hub.

$ docker buildx imagetools inspect nigelpoulton/k8sbook:latest

 use the digest to pull the image. I’ve trimmed the command and the output for readability.

$ docker pull nigelpoulton/k8sbook@sha256:13dd59a0...bce2e14b

The following curl command queries Docker Hub for the digest of the same image.

$ curl "https://hub.docker.com/v2/repositories/nigelpoulton/k8sbook/tags/?name=latest" \
  |jq '.results[].digest'

- own digests as follows:

Images digests are a crypto hash of the image’s manifest file
Layer digests are a crypto hash of the layer’s contents

two hashes:

Content hash (uncompressed)
Distribution hash (compressed)

- different architectures supported behind the alpine:latest tag.

$ docker buildx imagetools inspect alpine

$ docker run --rm golang go version

Windows users should replace the grep command with Select-String architecture,os

$ docker manifest inspect golang | grep 'architecture\|os'

- multi-architecture imaes 

the Registry API supports two important constructs:

    Manifest lists
    Manifests
The manifest list is exactly what it sounds like — a list of architectures supported by an image tag.

    + This means you can do a docker pull alpine on any architecture and get the correct version of the image. 

    + example 
$ docker buildx imagetools inspect alpine
Name:      docker.io/library/alpine:latest
MediaType: application/vnd.docker.distribution.manifest.list.v2+json
Digest:    sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b

Manifests:
  Name:      docker.io/library/alpine:latest@sha256:6457d53f...628977d0
  MediaType: application/vnd.docker.distribution.manifest.v2+json
  Platform:  linux/amd64

  Name:      docker.io/library/alpine:latest@sha256:b229a851...d144c1d8
  MediaType: application/vnd.docker.distribution.manifest.v2+json
  Platform:  linux/arm/v6

  Name:      docker.io/library/alpine:latest@sha256:ec299a7b...33b4c6fe
  MediaType: application/vnd.docker.distribution.manifest.v2+json
  Platform:  linux/arm/v7

  Name:      docker.io/library/alpine:latest@sha256:a0264d60...93467a46
  MediaType: application/vnd.docker.distribution.manifest.v2+json
  Platform:  linux/arm64/v8

  Name:      docker.io/library/alpine:latest@sha256:15c46ced...ab073171
  MediaType: application/vnd.docker.distribution.manifest.v2+json
  Platform:  linux/386

  Name:      docker.io/library/alpine:latest@sha256:b12b826d...ba52a3a2
  MediaType: application/vnd.docker.distribution.manifest.v2+json
  Platform:  linux/ppc64le

  echo mediatype, *.json is the manifest file 

- example golang 
Linux on arm64 example:

$ docker run --rm golang go version
<Snip>
go version go1.23.4 linux/arm64
Windows on x64 example:

> docker run --rm golang go version
<Snip>
go version go1.23.4 windows/amd64

- Windows users should replace the grep command with Select-String architecture,os

$ docker manifest inspect golang | grep 'architecture\|os'
            "architecture": "amd64",
            "os": "linux"
            "architecture": "arm",
            "os": "linux",
            "architecture": "arm64",
            "os": "linux",
            "architecture": "386",
            "os": "linux"
            "architecture": "mips64le",
            "os": "linux"
            "architecture": "ppc64le",
            "os": "linux"
            "architecture": "s390x",
            "os": "linux"
            "architecture": "amd64",
            "os": "windows",
            "os.version": "10.0.20348.2227"
            "architecture": "amd64",
            "os": "windows",
            "os.version": "10.0.17763.5329"

The docker buildx command makes it easy to create multi-architecture images. It offer two ways:

Emulation
Build Cloud

build AMD and ARM versions of the nigelpoulton/tu-demo image using Docker Build Cloud.

$ docker buildx build \
  --builder=cloud-nigelpoulton-ddd-cloud \
  --platform=linux/amd64,linux/arm64 \
  -t nigelpoulton/tu-demo:latest --push .

  - scan docker image, docker scout cves command to get more detailed information, including remediation advice.

$ docker scout cves nigelpoulton/tu-demo:latest several things are clear:

It has detected one vulnerable package containing two vulnerabilities
The affected package is called expat and the vulnerable version we’re running is 2.5.0-r2
It lists the vulnerability as CVE-2023-52425
It includes a link to a Scout report containing more info
It suggests we update to version 2.6.0-r0 which contains the fix

- delete images using the docker rmi command. rmi is short for remove image.

Docker won’t delete layers shared by multiple images until you delete all images that reference them. delete images by name, short ID, or SHA

$ docker rmi redis:latest af111729d35a sha256:c5b1261d...f8e1ad6b

    + delete all images on linux/windows powershell 

$ docker rmi $(docker images -q) -f

- summary 

Images – The commands
docker pull is the command to download images from remote registries. It defaults to Docker Hub but works with other registries. The following command will pull the image tagged as latest from the alpine repository on Docker Hub: docker pull alpine:latest.
docker images lists all the images in your Docker host’s local repository (image cache). You can add the --digests flag to see the SHA256 hashes.
docker inspect gives you a wealth of image-related metadata in a nicely formatted view.
docker manifest inspect lets you inspect the manifest list of images stored in registries. The following command will show the manifest list for the regctl image on GitHub Container Registry (GHCR): docker manifest inspect ghcr.io/regclient/regctl.
docker buildx is a Docker CLI plugin that works with Docker’s latest build engine features. You saw how to use the imagetools sub-command to query manifest-related data from images.
docker scout is a Docker CLI plugin that integrates with the Docker Scout backend to perform image vulnerability scanning. It scans images, provides reports on vulnerabilities, and even suggests remediation actions.
docker rmi is the command to delete images. It deletes all layer data stored in the local filesystem, and you cannot delete images that are in use by containers.


# Working with containers 
- following sections:

Container – The TLDR
Containers vs VMs
Images and containers
Check Docker is running
Starting a container
How containers start apps
Connecting to a running container
Inspecting container processes
The docker inspect command
Writing data to a container
Stopping, restarting, and deleting a container
Killing a container’s main process
Debugging slim images and containers with Docker Debug
Self-healing containers with restart policies
The commands

- containers the TLDR, shows multiple containers started from a single image

- virtualize are very different:

VMs virtualize hardware
Containers virtualize operating systems

- Run one of the following commands to check the status of the daemon.

Linux systems not using Systemd.

$ service docker status
docker start/running, process 29393
Linux systems using Systemd.

$ systemctl is-active docker
active

- Starting a container
The docker run command is the simplest and most common way to start a new container.

$ docker run -d --name webserver -p 5005:8080 nigelpoulton/ddd-book:web0.1
Unable to find image 'nigelpoulton/ddd-book:web0.1' locally
web0.1: Pulling from nigelpoulton/ddd-book
4f4fb700ef54: Already exists
cf2a607f33f7: Download complete
0a1f0c111e9a: Download complete
c1af4b5db242: Download complete
Digest: sha256:3f5b281b914b1e39df8a1fbc189270a5672ff9e98bfac03193b42d1c02c43ef0
Status: Downloaded newer image for nigelpoulton/ddd-book:web0.1
b5594b3b8b3fdce544d2ca048e4340d176bce9f5dc430812a20f1852c395e96b

- verify Docker pulled the image and started the webserver container.

- Docker how to start an app in a container:

Entrypoint instructions cannot be overridden on the CLI, and anything you pass in via the CLI will be appended to the Entrypoint instruction as an argument.

    +  three ways you can tell Docker how to start an app in a container

An Entrypoint instruction in the image
A Cmd instruction in the image
A CLI argument

$ docker inspect nigelpoulton/ddd-book:web0.1 | grep Entrypoint -A 3

- docker run command is:

$ docker run <arguments> <image> <command>

$ docker run --rm -d alpine sleep 60

The --rm argument automatically cleans up the exited container.

    + two modes:

Interactive
Remote execution

-  start an interactive exec session by creating a new shell process (sh) inside the webserver container sh is a minimal shell program installed in the container.

$ docker exec -it webserver sh

- Inspecting container processes
Most containers only run a single process. 

Type exit to quit the exec session and return to your local terminal. 

- docker exec command without specifying the -it flags. This will remotely execute the command without creating an interactive session

$ docker exec webserver ps

- docker inspect command  detailed information about images and containers.

$ docker inspect webserver
<Snip>
"State": {
    "Status": "running"
},
"Name": "/webserver", 
    "PortBindings": { 
        "8080/tcp": [ 
            {
                "HostIp": "",          
                "HostPort": "5005"      
            }
        ]
    },
    "RestartPolicy": {
        "Name": "no",
        "MaximumRetryCount": 0
    "Image": "nigelpoulton/ddd-book:web0.1",
    "WorkingDir": "/src",
    "Entrypoint": [
        "node",
        "./app.js"
    ],
        }
<Snip>

- a new interactive exec session to the webserver container with the following command.

$ docker exec -it webserver sh

- stop, docker stop command. It will take up to 10 seconds to gracefully stop.

$ docker stop webserver

$ docker ps -a
CONTAINER ID   IMAGE         COMMAND           STATUS                           NAMES
b5594b3b8b3f   nigelpou...   "node ./app.js"   Exited (137) About a minute ago  webserver

- restarting and deleting 

$ docker restart webserver
webserver

You can also run the following command to return the contents of the file directly from the container’s filesystem.

$ docker exec webserver cat views/home.pug

Be careful forcing operations like this, as Docker doesn’t ask you to confirm.

    + delete the container.
$ docker rm webserver -f
webserver

- killing a container's main process 

Run the following command to start a new interactive container called ddd-ctr based on the Ubuntu image and tell it to run a Bash shell as its main process.

$ docker run --name ddd-ctr -it ubuntu:24.04 bash

- following two commands to restart it and attach your shell to its main process.

$ docker restart ddd-ctr

- Type Ctrl PQ to exit the container and run another docker ps command to verify the container 

- Docker Debug 

Docker Debug works by attaching a shell to a container and mounting a toolbox loaded with debugging tools. This toolbox is mounted as a directory called /nix and is available during your debugging session but is never visible to the container.

 running container called ddd-ctr. If you don’t, you can start one by running 
 $ docker run --name ddd-ctr -it ubuntu:24.04 bash.

 docker attach command is similar to the docker exec commands you learned earlier but automatically connects to a container’s main process. don’t need to run the docker attach command if you’re already connected to the container.

 $ docker attach ddd-ctr
root@d3c892ad0eb3:/#

Type Ctrl PQ to gracefully disconnect from the container without killing the Bash process.

- log in to Docker to use Docker Debug, and it only works if you have a Pro, Team, or Business license.

$ docker login

Once you’re logged in and have the plugin installed

Debug session to the running container called ddd-ctr. 

docker debug <image>|<container>

$ docker debug ddd-ctr

You can use Docker Debug’s built-in install command to add any package listed on search.nixos.org.

install the bind package (which includes the nslookup tool), and then run the nslookup command again.

Docker will automatically pull the image from Docker Hub if you don’t have a local copy.

$ docker debug nigelpoulton/ddd-book:web0.1

    + Run the following entrypoint command to reveal the default command

docker > entrypoint --print
node ./app.js

Type exit to quit the debug session.

- self healing containers with restart policies 

restart policies per container, and Docker supports the following four policies:

no (default)
on-failure
always
unless-stopped

start an interactive container called neversaydie with the always restart policy.
$ docker run --name neversaydie -it --restart always alpine sh

on Windows using PowerShell.

$ docker inspect neversaydie | grep RestartCount

- clean up 
You can also delete all containers and all images with the following two commands. 

$ docker rm $(docker ps -aq) -f
ac165419214f
5bd3741185fa

$ docker rmi $(docker images -q)

- summary Containers – The commands
docker run is the command to start new containers. You give it the name of an image and it starts a container from it. This example starts an interactive container from the Ubuntu image and tells it to run the Bash shell: docker run -it ubuntu bash.
Ctrl-PQ is how you detach from a container without killing the process you’re attached to. You’ll use it frequently to detach from running containers without killing them.
docker ps lists all running containers, and you can add the -a flag to also see containers in the stopped (Exited) state.
docker exec allows you to run commands inside containers. The following command will start a new Bash shell inside a running container and connect your terminal to it: docker exec -it <container-name> bash. This next command runs a ps command inside a running container without opening an interactive shell session: docker exec <container-name> ps. For these to work, the container must include the Bash shell.
docker stop stops a running container and puts it in the Exited (137) state. It issues a SIGTERM to the container’s PID 1 process and allows the container 10 seconds to gracefully quit. If the process hasn’t cleaned up and stopped within 10 seconds, it sends a SIGKILL to force the container to terminate immediately.
docker restart restarts a stopped container.
docker rm deletes a stopped container. You can add the -f flag to delete the container without having to stop it first.
docker inspect shows you detailed configuration and run-time information about a container.
docker debug attaches a debug shell to a container or image and lets you run commands that aren’t available inside the container or image. It requires a Pro, Team, or Business Docker subscription.


# Containerizing an app 
- sections 
Containerizing an app – The TLDR
Containerize a single-container app
Moving to production with multi-stage-builds
Buildx, BuildKit, drivers, and Build Cloud
Multi-architecture builds
A few good practices

- basic flow 

build image -> push image -> ship / registry image -> run as container 

- containerize a simple Node.js app:

Get the application code from GitHub
Create the Dockerfile
Containerize the app
Run the app
Test the app
Look a bit closer

- create a new directory called ddd-book.

$ git clone https://github.com/nigelpoulton/ddd-book.git

    + ddd-book/node-app directory and list its contents.

$ cd ddd-book/node-app

    + create Dockerfiles manually. Fortunately, newer versions of Docker support the docker init command that reads your build context, analyzes your application, and automatically creates a Dockerfile implementing good practices.

$ docker init
Welcome to the Docker Init CLI!
<Snip>
? What application platform does your project use? Node
? What version of Node do you want to use? 23.3.0    <<---- Newer versions are OK
? Which package manager do you want to use? npm
? What command do you want to use to start the app? node app.js
? What port does your server listen on? 8080

CREATED: .dockerignore
CREATED: Dockerfile
CREATED: compose.yaml
CREATED: README.Docker.md

✔ Your Docker files are ready!

    + content 
```
1. ARG NODE_VERSION=20.8.0
2. FROM node:${NODE_VERSION}-alpine
3. ENV NODE_ENV production
4. WORKDIR /usr/src/app
5. RUN --mount=type=bind,source=package.json,target=package.json \
    --mount=type=bind,source=package-lock.json,target=package-lock.json \
    --mount=type=cache,target=/root/.npm \
    npm ci --omit=dev
6. USER node
7. COPY . .
8. EXPOSE 8080
9. CMD node app.js
```

IF YOU don't have init plugin then we need to manual add the docker file 
    + build context is the directory where your app files live.

$ docker build -t ddd-book:ch8.node .

    + Check the image exists in your Docker host’s local repository.

$ docker images

    + Run a docker inspect ddd-book:ch8.node command to verify the image and see the settings from the Dockerfile.

    + Push the image to Docker Hub
This is an optional section, and you’ll need a Docker Hub account to follow along. Go to hub.docker.com

Login to Docker Hub
Re-tag the image
Push the image

$ docker login
USING WEB-BASED LOGIN

    + The format of the command is docker tag <current-tag> <new-tag>, and it creates an additional tag for the same image.

    + run the app 

$ docker run -d --name c1 \
  -p 5005:8080 \
  nigelpoulton/ddd-book:ch8.node

The -d flag runs the container in the background, and the --name flag calls it c1. The -p 5005:8080 maps port 5005 on your Docker host to port 8080 inside the container

verify the port mapping.

$ docker ps

Run a docker ps command to ensure the c1 container is running
Check port mapping is correct — 0.0.0.0:5005->8080/tcp

- instructions create new layers, Examples of instructions that create new layers are FROM, RUN, COPY and WORKDIR. Examples that create metadata include EXPOSE, ENV, CMD, and ENTRYPOINT. The premise is this:

You can run a docker history command against any image to see the instructions that created it.

$ docker history ddd-book:ch8.node

IMAGE         CREATED BY                                    SIZE      COMMENT
24dd...a06b   CMD ["/bin/sh" "-c" "node app.js"]            0B        buildkit.dockerfile.v0
<missing>     EXPOSE map[8080/tcp:{}]                       0B        buildkit.dockerfile.v0
<missing>     COPY . . # buildkit                           98kB      buildkit.dockerfile.v0
<missing>     USER node                                     0B        buildkit.dockerfile.v0
<missing>     RUN /bin/sh -c npm ci --omit=dev # buildkit   13.6MB    buildkit.dockerfile.v0
<missing>     WORKDIR /usr/src/app                          16.4kB    buildkit.dockerfile.v0
<missing>     ENV NODE_ENV=production                       0B        buildkit.dockerfile.v0
<Snip>
<missing>     ADD alpine-minirootfs-3.21.0-aarch64.tar.gz   8.84MB    8.35MB

- Moving to production with multi-stage builds
When it comes to container images… big is bad! For example:

Big means slow
Big means more potential vulnerabilities
Big means a larger attack surface

    + Dockerfile:
```
FROM golang:1.23.4-alpine AS base             <<---- Stage 0
WORKDIR /src
COPY go.mod go.sum .
RUN go mod download
COPY . .

FROM base AS build-client                     <<---- Stage 1
RUN go build -o /bin/client ./cmd/client

FROM base AS build-server                     <<---- Stage 2
RUN go build -o /bin/server ./cmd/server

FROM scratch AS prod                          <<---- Stage 3
COPY --from=build-client /bin/client /bin/
COPY --from=build-server /bin/server /bin/
ENTRYPOINT [ "/bin/server" ]
```
Stage 0 is called base and builds an image with compilation tools, etc
Stage 1 is called build-client and compiles the client executable
Stage 2 is called build-server and compiles the server executable
Stage 3 is called prod and copies the client and server executables into a slim image. The prod stage pulls the minimal scratch image. It

builders will always try run stage by parallel 

    + build stages are included in the final production image.

$ docker history multi:full

    + build multiple images from a single Dockerfile.

separate image for each by splitting the final prod stage into two stages as follows:

```
FROM golang:1.20-alpine AS base
WORKDIR /src
COPY go.mod go.sum .
RUN go mod download
COPY . .

FROM base AS build-client
RUN go build -o /bin/client ./cmd/client

FROM base AS build-server
RUN go build -o /bin/server ./cmd/server

FROM scratch AS prod-client                 <<---- New stage
COPY --from=build-client /bin/client /bin/
ENTRYPOINT [ "/bin/client" ]

FROM scratch AS prod-server                 <<---- New stage
COPY --from=build-server /bin/server /bin/
ENTRYPOINT [ "/bin/server" ]
```

Run the following two commands to create two different images from the same Dockerfile-final file

$ docker build -t multi:client --target prod-client -f Dockerfile-final .
<Snip>

$ docker build -t multi:server --target prod-server -f Dockerfile-final .
<Snip>

- major build components Docker’s build system has a client and server, Buildx, BuildKit, drivers, and Build Cloud

Client: Buildx
Server: BuildKit


$ docker buildx ls
NAME/NODE                  DRIVER/ENDPOINT         PLATFORMS
builder *                  docker-container

Run a docker buildx inspect command against one of your builders.

$ docker buildx inspect cloud-nigelpoulton-ddd

Name:          cloud-nigelpoulton-ddd
Driver:        cloud
Nodes:
Name:      linux-arm64
Endpoint:  cloud://nigelpoulton/ddd_linux-arm64
Status:    running
Buildkit:  v0.16.0

- Multi-architecture builds
You can use the docker build command to build images for multiple platforms and CPU architectures


a builder is an instance of BuildKit that will perform builds.

$ docker buildx ls
NAME/NODE                  DRIVER/ENDPOINT         PLATFORMS
builder *                  docker-container
  builder0                 desktop-linux           linux/arm64, linux/amd64, linux/amd64/v2, 
                                                   linux/riscv64, linux/ppc64le, linux/s390x, 
                                                   linux/386, linux/mips64le, linux/mips64,
                                                   linux/arm/v7, linux/arm/v6
cloud-nigelpoulton-ddd     cloud

    + create new builder 

$ docker buildx create --driver=docker-container --name=container

Make it the default builder.

$ docker buildx use container

- Docker Hub account or don’t want to push the images, you can replace the --push with --load.

$ docker buildx build --builder=container \
  --platform=linux/amd64,linux/arm64 \
  -t nigelpoulton/ddd-book:ch8.1 --push .


 two important things:

Each Dockerfile instruction executed twice — once for AMD and once for ARM
The last few lines show the image layers being pushed directly to Docker Hub

    + cloud builder, Docker subscription that grants you access to Build Cloud, you can go to build.docker.com and configure your first cloud builder

$ docker buildx create --driver cloud nigelpoulton/ddd

$ docker buildx build \
  --builder=cloud-nigelpoulton-ddd \
  --platform=linux/amd64,linux/arm64 \
  -t nigelpoulton/ddd-book:ch8.1 --push .


BuildKit uses a cache to speed up builds.

to understand that COPY and ADD instructions include logic to ensure the content you’re copying into the image hasn’t changed since the last build.

    + delete the container.

$ docker rm c1 -f

$ docker rmi \
  multi:full multi:client multi:server ddd-book:ch8.node nigelpoulton/ddd-book:ch8.node

- the commands to containerizing an app 

Containerizing an app – The commands
docker build containerizes applications. It reads a Dockerfile and follows the instructions to create an OCI image. The -t flag tags the image, and the -f flag lets you specify the name and location of the Dockerfile. The build context is where your application files exist and can be a directory on your local Docker host or a remote Git repo.
The Dockerfile FROM instruction specifies the base image. It’s usually the first instruction in a Dockerfile, and it’s considered a good practice to build from Docker Official Images or images from Verified Publishers. FROM is also used to identify new build stages in multi-stage builds.
The Dockerfile RUN instruction lets you run commands during a build. It’s commonly used to update packages and install dependencies. Every RUN instruction creates a new image layer.
The Dockerfile COPY instruction adds files to images, and you’ll regularly use it to copy your application code into a new image. Every COPY instruction creates an image layer.
The Dockerfile EXPOSE instruction documents an application’s network port.
The Dockerfile ENTRYPOINT and CMD instructions tell Docker how to run the app when starting a new container.
Some other Dockerfile instructions include LABEL, ENV, ONBUILD, HEALTHCHECK and more.


# Multi-container apps with compose 
- a microservices app with the following services

Web front-end
Ordering
Catalog
Back-end datastore
Logging
Authentication
Authorization

Compose lets you describe the application in a simple YAML file called a Compose file. You then use the Compose file with the docker compose command to deploy and manage the entire app.

- installing docker compose 

$ docker compose version
Docker Compose version v2.35.1

- example 

$ git clone https://github.com/nigelpoulton/ddd-book.git

The app folder contains the application code, views, and templates
The Dockerfile describes how to build the image for the web-fe service
The requirements.txt file lists the application dependencies for the web-fe service
The compose.yaml file tells Docker how to deploy the app

- compose files 

compose.yaml or compose.yml. However, you can specify a different filename with the -f flag

    + example 

```
services:                  <<--- Microservices are defined in the "services" block
  web-fe:                  ----┐
    deploy:                    |
      replicas: 1              |
    build: .                   | This block
    command: python app.py     | defines the
    ports:                     | *web-fe*
      - target: 8080           | microservice
        published: 5001        |
    networks:                  |
      - counter-net        ----┘
  redis:                   ----┐
    deploy:                    | 
      replicas: 1              | 
    image: "redis:alpine"      | 
    networks:                  | The *redis*
      counter-net              | service
    volumes:                   |
      - type: volume           |
        source: counter-vol    |
        target: /app       ----┘
networks:                  <<--- Networks are defined in this block
  counter-net:                
volumes:                   <<--- Volumes are defined in this block
  counter-vol:
```

    + the service section 

```
services:                     
  web-fe:                     <<--- Service name. Containers will inherit this name
    deploy:                   
      replicas: 1             <<--- Deploy a single container for this service
    build: .                  <<--- Build from the Dockerfile in the current directory
    command: python app/app.py <<-- Execute this command when starting containers
    ports:                    
      - target: 8080          ----┐ Map port 8080 in the container
        published: 5001       ----┘ to port 5001 on the Docker host
    networks:
      - counter-net           <<--- Attach the service's containers to the "counter-net" network
```

run the python app/app.py command. It will attach to the counter-net network, expose the web service on the host’s port 5001.

    + redis service section 
```
services:
  ..
  redis:                      <<--- Service name. Containers will inherit this name
    image: "redis:alpine"     <<--- Pull the "redis:alpine" image for this service
    deploy:                  
      replicas: 1             <<--- Deploy a single container for this service
    networks:
      counter-net:            <<--- Attach containers to the "counter-net" network
    volumes:
      - type: volume
        source: counter-vol   ----┐ Mount the "counter-vol" volume
        target: /app          ----┘ to "/app" in the containers for this service
```

code extract from the app.py file shows the web app communicating with the redis service by name.
```
import time
import redis
from flask import Flask, render_template
app = Flask(__name__)
cache = redis.Redis(host='redis', port=6379)   <<---- "redis" is the name of the service
<Snip>
```

    + network and volumes blocks are extremely simple and define a network called counter-net and a volume called counter-vol

```
networks:                 <<---- This block defines a new network called "counter-net"
  counter-net:                
volumes:                  <<---- This block defines a new volume called "counter-vol"
  counter-vol:
```

    + deploying apps with compose 

run all commands from the multi-container folder.

$ docker compose up --detach

the following command will deploy the application defined in a Compose file called sample-app.yml in the apps/ddd-book directory.

$ docker compose -f apps/ddd-book/sample-app.yml up --detach

$ docker images
REPOSITORY                TAG        IMAGE ID        CREATED           SIZE
redis                     alpine     f773b35a95e1    8 days ago        61.4MB
multi-container-web-fe    latest     811f22c9edb7    2 minutes ago     99.7MB

    +  the resources for our sample app.

Resource type	Resource	Name
Service	web-fe	multi-container-web-fe-1
Service	redis	multi-container-redis-1
Network	counter-net	multi-container_counter-net
Volume	counter-vol	multi-container_counter-vol

see the counter-net network and counter-vol volume.
$ docker network ls
$ docker volume ls

- managing apps with compose 

shut the app down.
$ docker compose down

docker volumes ls command to see if the volume still exists.
$ docker volumes ls

redeploy the app. it started much faster this time. This is because the images and volume already exist.
$ docker compose up --detach

check the current state of the app with a docker compose ps command.
$ docker compose ps 

Run a docker compose top to list the processes inside each container.
$ docker compose top 

stop the app and recheck its status.
$ docker compose stop

Restart the app with
$ docker compose restart 

Check the status of the app.
$ docker compose ls

- clean up 
Run the following command to stop and delete the app. The --volumes flag will delete all of the app’s volumes. --rmi all will delete all of its images

$ docker-compose down --volumes --rmi all


# Docker and AI 
- Docker Model Runner Architecture
DMR executes models directly on host hardware, exposes them via OpenAI-compatible endpoints, and integrates with the wider Docker toolchain


Docker Hub {model ...}   Containerized apps         Network/local apps      Local apps 
A                                   |                           |               |
|                       http://model-runner.docker.internal/    V http:*/       |/var/run/docker.sock 
push/pull                           -----------> OpenAPI compatible endpoints  <-
|                                           /engines/v1/models (get)
V                                           /engines/v1/chat/completions (post) ...
Docker CLI -model mgt-> Docker Model Runner {Runtime Pluggable}   <-- local model store {model...}

- installing docker model runner 

    + clone the book’s GitHub repo:

$ git clone https://github.com/nigelpoulton/ddd-book.git
$ cd ddd-book/dmr

    + Open Docker Desktop’s Settings page, click the Features in development tab, and check the Enable Docker Model Runner and Enable host-side TCP support checkboxes.

    + enabled DMR switch to the command line and verify it's working 
    $ docker model status

    + Explore Docker Model Runner
    In this section, you’ll learn how to:

    Pull models from Docker Hub
    List and inspect models
    Test models
    Inspect the DMR APIs

    + download a newer quantized version if available.

$ docker model pull ai/gemma3:4B-Q4_K_M

    + list your local models to see the one you downloaded.

$ docker model ls

- inspecting models 

a new type of OCI artifact called a model. This model-spec is currently in draft and may change slightly.

- connects to Docker Hub and inspects the manifest from Docker Hub and not the local copy you pulled.

$ docker manifest inspect ai/gemma3:4B-Q4_K_M | jq

layer files are in your local filesystem below ~/.docker/models/blobs/sha256 and you can inspect them with your favorite tools.

$ ls -lh ~/.docker/models/blobs/sha256
SHA and filename may different on your side 
$ cat ~/.docker/models/blobs/sha256/22273fdf4e6dbaf...af0e6569be41539 | jq

    + see the same information with the docker model inspect command

$ docker model inspect 

- store all of the following:

Container images
Signatures
SBOMs
OPA Policies
Helm charts
Wasm modules
MCP tools

- DMR offers two easy ways to test your models:

The CLI
Docker Desktop UI
    + opens an interactive REPL (Read, Evaluate, Print, Loop)

he responses as they can be quite long.

$ docker model run ai/gemma3:4B-Q4_K_M

- Docker Model Runner exposes a set of native endpoints for model management and a set of OpenAI-compatible endpoints

DMR endpoints

/models                       <<---- GET
/models/create                <<---- POST
/models/{namespace}/{name}    <<---- GET and DELETE

OpenAI-compatible endpoints

/engines/llama.cpp/v1/models             <<---- GET
/engines/llama.cpp/v1/chat/completions   <<---- POST
/engines/llama.cpp/v1/completions        <<---- POST
/engines/llama.cpp/v1/embeddings         <<---- POST

use the following path to explicitly call the llama.cpp runtime:

/engines/llama.cpp/v1/chat/completions

$ docker model pull ai/qwen3:0.6B-Q4_K_M
$ docker model ls


# Docker and wasm 
- Wasm (WebAssembly) is driving the third wave of cloud computing
- divided the chapter as follows:

Pre-reqs
Intro to Wasm
Write a Wasm app
Containerize a Wasm app
Deploy a Wasm app

- Configure Docker Desktop for Wasm

Use containerd for pulling and storing images is selected on the General tab. Next, click the Features in development tab, select the Enable Wasm option and click the blue Apply & restart button.

- Install Rust and configure for Wasm

install the wasm32-wasip1 target so that Rust can compile to Wasm.

$ rustup target add wasm32-wasip1

- Spin is a Wasm framework and runtime that makes building and running Wasm apps easy.

Search the web for how to install Fermyon spin

$ spin --version

Wasm is evolving fast and may become better at other workloads in the future.

- list wasm runtimes 
$ docker run --rm -i --privileged --pid=host jorgeprendes420/docker-desktop-shim-manager:latest

My installation has seven Wasm runtimes, including the io.containerd.spin.v2

    + example create a wasm app 

Wasm app called hello-world. Respond to the prompts as shown in the example.

$ spin new hello-world -t http-rust

tree command

$ cd hello-world

$ tree
.
├── Cargo.toml
├── spin.toml
└── src
    └── lib.rs

This configures the app to display Docker loves Wasm.
```
use spin_sdk::http::{IntoResponse, Request, Response};
<Snip>
    Ok(http::Response::builder()
        .status(200)
        .header("content-type", "text/plain")
        .body("Docker loves Wasm")?)             <<---- Change text inside quotes
        .build())
}
```

$ spin build

running a more complex cargo build command that compiles the app as a Wasm binary.

tree command to see the Wasm binary.

$ tree
<Snip>
└── target
    └── wasm32-wasip1
        └── release
            └── hello_world.wasm

Run a spin up command to start the app using the local spin runtime
$ spin up

- Containerize a Wasm app

Create a new file called Dockerfile in your current directory and populate it with the following three lines.

```
FROM scratch
COPY /target/wasm32-wasip1/release/hello_world.wasm .
COPY spin.tom
```

tag the image with your own Docker Hub username instead of mine.

$ docker build \
  --platform wasi/wasm \
  --provenance=false \
  -t nigelpoulton/ddd-book:wasm .

$ docker run -d --name wasm-ctr \
  --runtime=io.containerd.spin.v2 \
  --platform=wasi/wasm \
  -p 5556:80 \
  nigelpoulton/ddd-book:wasm /

deleting the image.

$ docker rm wasm-ctr -f

# Docker swarm 
- how to deploy a multi-node Swarm cluster 

Swarm primer
Build a swarm
Deploy a Swarm app

- Docker Swarm is two things:

A secure cluster of Docker nodes (swarm with a little “s”)
An intelligent application orchestrator (Swarm with a big “S”)

- build two or more Docker nodes into a highly available swarm.

go to http://https://labs.play-with-docker.com/ and spin up a few Docker nodes to follow along

steps to build your swarm:

Initialize the first swarm manager
Add workers (optional)
Add additional managers

- initialize your swarm 

prompted to use the --advertise-addr flag. If this happens, use the node’s primary IP address.
$ docker swarm init

- Adding worker nodes is optional, as your managers will also run user apps

$ docker swarm join --token SWMTKN-1-2hl6...-...3lqg 172.31.40.192:2377
This node joined a swarm as a worker.

$ docker swarm join --token SWMTKN-1-2hl6...-...3lqg 172.31.40.192:2377
This node joined a swarm as a worker.

$ docker node ls

got one manager and two workers. Managers have either Leader or Reachable in the MANAGER STATUS column,

adding more managers.
$ docker swarm join-token manager

The asterisk (*) indicates the manager you executed the command from.

    + use the previous book services as example 

swarm manager and clone the book’s GitHub repo.

$ git clone https://github.com/nigelpoulton/ddd-book.git

$ cd ddd-book/swarm-new

```
networks:               ----┐
  counter-net:              | 
    driver: overlay         | Deploy an encrypted overlay network called *counter-net*
    driver_opts:            |
      encrypted: 'yes'  ----┘
volumes:                
  counter-vol:          <<--- Create a volume called *counter-vol*
services:
  web-fe:                               ----┐
    image: nigelpoulton/ddd-book:swarm-app  |
    command: python app.py                  w 
    deploy:                                 e
      replicas: 4                           b  <<---- Deploy four replicas
      update_config:                        -  <<---- Next 3 lines define how to update the app
        parallelism: 2                      f  <<---- Update two replicas at a time
        delay: 10s                          e  <<---- Wait 10 seconds after each pair
        failure_action: rollback               <<---- Rollback if there's a failure
      restart_policy:                       s   
        condition: on-failure               e  <<---- Restart replicas if they fail
        delay: 5s                           r  <<---- Wait five seconds between restart attempts
        max_attempts: 3                     v  <<---- Only try three restarts
        window: 120s                        i  <<---- Give up after trying for two minutes
    networks:                               c 
      - counter-net                         e  <<---- Attach to the *counter-net* network
    ports:                                  |
      - "5001:8080"                     ----┘  <<---- Map the app to 5001 on the host
  redis:                                ----┐  <<---- Redis service
    image: "redis:alpine"                   | 
    networks:                               | 
      counter-net:                          |  <<---- Join the *counter-net* network
    volumes:                                |
      - type: volume                        | 
        source: counter-vol                 |  <<---- Mount the *counter-vol* volume to
        target: /app                    ----┘  <<---- /data in the container
```

The networks key defines an encrypted overlay network called counter-net, the volumes key defines a volume called counter-vol

deploy the app and call it ddd.

$ docker stack deploy -c compose.yaml ddd
 confirm this.

$ docker stack ps ddd

Swarm has evenly balanced the four web-fe replicas across your nodes. You can also see the web-fe service is publishing port 5001.

manage Swarm apps in two ways:

Imperatively
Declaratively

- Swarm will roll out a new version of the web-fe service with all 10 replicas running the new image.

$ docker stack deploy -c compose.yaml ddd

- clean up 
$ docker stack rm ddd

Run the following command on the node that hosted the redis replica. It will delete the volume.

$ docker volume rm ddd_counter-vol

- Docker Swarm – The Commands
docker swarm init initializes a new Swarm and makes the node the first manager of the Swarm.
docker stack deploy is the command you’ll run to deploy and update Swarm apps. You need to specify the Compose file and the name of the app.
docker stack ls lists all Swarm apps and shows the number of services in each.
docker stack ps gives you detailed information about a Swarm app. It tells you which node each replica is running on, which images they’re based on, and shows the desired state and current state of each replica.
docker stack services gives you a line of information for each application service and includes useful information such as replication mode, how many replicas, and port mappings.
docker stack rm deletes a Swarm app and doesn’t ask for confirmation.


# Docker networking 
- following sections:

Docker networking – the TLDR
Docker networking theory
Single-host bridge networks
External access via port mappings
Connecting to existing networks and VLANs
Service Discovery
Ingress load balancing

-Docker networking is based on libnetwork, which is the reference implementation of open-source architecture called the Container Network Model (CNM).

- docker networking theory 

three components:

The Container Network Model (CNM), design specification and outlines the fundamental building blocks
Libnetwork, Libnetwork is a real-world implementation of the CNM. 
Drivers, extend the model by implementing specific network topologies such as VXLAN overlay networks

    + specification document 

https://github.com/moby/moby/blob/master/libnetwork/docs/design.md

specification document, but at a high level, it defines three building blocks:

Sandboxes, an isolated network stack inside a container. It includes Ethernet interfaces, ports, routing tables, DNS configuration
Endpoints, virtual network interfaces that look, smell, and feel like regular network interfaces. They connect sandboxes to networks.
Networks, virtual switches (usually software implementations of an 802.1d bridge). As such, they group together and isolate one or more endpoints
    +  CNM terminology, endpoints connect sandboxes to networks. Every container you create will have a sandbox with at least one endpoint connecting it to a network.

The Container Network Model (CNM) example

Sandbox Isolated network stack 
|
V
EndPoint virtual ethernet interface 
|
V
Netowrk Virtual switch(802.1d bridge)

each container gets its own sandbox which hosts the container’s entire network stack, including one or more endpoints that act as Ethernet interfaces and can be connected to networks.

Container A {               Container B {
    {Sandbox                {Sandbox 
    Endpoint}               Endpoint  Endpoint}
                                |       |
}                           }   |       |
|                               |       |
V                               V       V 
[Network A                        ]  [Network B ]

Endpoints can only connect them to a single network. This is why Container B needs two endpoints
Containers network stacks are completely isolated and can only communicate via a network.

- Libnetwork is the reference implementation of the CNM. It’s open-source, cross-platform (Linux and Windows), maintained by the Moby project

- Drivers  relies on drivers to implement the data plane. For example, drivers are responsible for creating networks and ensuring isolation and connectivity.  include bridge, overlay, and macvlan, and they build the most common network topologies
                         Libnetwork 
                         {Sandboxes }
Docker Engine <-{API} -> {Endpoints }                                       Drivers 
                         {Networks }                                       {Bridge driver, such as single host}
                         -----------            {Pluggable interface} <----{Overlay driver, such as mutli host}
                         {Service discovery }                              {MACVLAN driver, such as existing VLANs}
                         {Load balancing}
                         -----------
                         Control plane 

Create an overlay network called prod-fe-cuda, Docker will invoke the overlay driver to create the network and its resources.

- Single-host bridge networks
The simplest type of Docker network is the single-host bridge network.

Single-host tells us the network only spans a single Docker host
Bridge tells us that it’s an implementation of an 802.1d bridge (layer 2 switch)
                        
you run Windows containers you’ll need to use the nat driver, but for all intents and purposes they work the same.                        

- on swarm two Docker hosts with identical local bridge networks, both called mynet. Even though the networks are identical, they are independent and isolated, meaning the containers in the picture cannot communicate

new Docker host gets a default single-host bridge network called bridge that Docker connects new containers to unless you override it with the --network flag.

output of a docker network ls command on Docker installation.
$ docker network ls
can run docker inspect commands to get more information.
$ docker network inspect bridge

All bridge networks are based on the battle-hardened Linux bridge technology that has existed in the Linux kernel for over 20 years. 
The default bridge network on all Linux-based Docker hosts is called bridge and maps to an underlying Linux bridge in the host’s kernel called docker0
You can run a docker network inspect command to confirm that the bridge network is based on the docker0 bridge in the host’s kernel.
on Windows using PowerShell, you’ll need to replace grep with SelectString.
$ docker network inspect bridge | grep bridge.name

inspect the docker 0 bridge from the Linux manually install the brctl utility.
$ brctl show
$ ip link show docker0

Docker {Containers, Docker network}
Linux {Linux bridge, port mapping}
Host {Host interface}

- create a new single-host bridge network called localnet.

$ docker network create -d bridge localnet

 creates a new Linux bridge in the host’s kernel.

Run another brctl show command to see it.
$ brctl show
bridge name        bridge id             STP enabled	   interfaces
br-f918f1bb0602    8000.0242372a886b     no
docker0            8000.024258ee84bc     no


 create a new container called c1 and attach it to the new localnet bridge network.

$ docker run -d --name c1 \
  --network localnet \
  alpine sleep 1d

need the jq utility installed for the command to work. Leave off the "| jq" if it doesn’t work.

$ docker network inspect localnet --format '{{json .Containers}}' | jq

$ docker run -it --name c2 \
  --network localnet \
  alpine sh

Docker’s internal DNS server that holds name-to-IP mappings for all containers started with the --name or --net-alias flag.

- external access via port mapping 

The web container on the right is running a web server on port 80 that is mapped to port 5005 on the Docker host. The client container on the left is sending requests to the Docker host on port 5005 and the external client at the bottom is doing the same. 

Host Node {

    client docker {                 
        send to host *:5005
    }

    server web docker {
        send to *:80 
    } mapping to host *:5005
} send to *:5005 

Create a new container called web running NGINX on port 80 and map it to port 5005 on the Docker host.

$ docker run -d --name web \
  --network localnet \
  --publish 5005:80 \
  nginx

Verify the port mapping.

$ docker port web
80/tcp -> 0.0.0.0:5005
80/tcp -> [::]:5005

You can test external access by pointing a web browser to the Docker host on port 5005. You’ll need to know the IP or DNS name of your Docker host 

create a new container called client on the bridge network.
$ docker run -it --name client --network bridge alpine sh

Install curl 
```
# apk add curl
fetch https://dl-cdn.alpinelinux.org/alpine/v3.19/main/aarch64/APKINDEX.tar.gz
fetch https://dl-cdn.alpinelinux.org/alpine/v3.19/community/aarch64/APKINDEX.tar.gz
(1/8) Installing ca-certificates (20240226-r0)
<Snip>
```

$ curl 192.168.64.69:5005

A common example is partially containerized apps where the parts running in containers need to be able to communicate with the parts not running in containers.

- create a new Docker network with the macvlan driver and configure it with all of the following:

Subnet info
Gateway
Range of IPs it can assign to containers
Which of the host’s interfaces or sub-interfaces to use

$ docker network create -d macvlan \
  --subnet=10.0.0.0/24 \
  --ip-range=10.0.0.0/25 \
  --gateway=10.0.0.1 \
  -o parent=eth0.100 \        <<---- Make sure this matches your system
  macvlan100

we’re connecting to VLAN 100, so we tag the sub-interface with .100 (-o parent=eth0.100). used the --ip-range flag to tell the new network which sub-set of IP addresses it can assign to containers

$ docker network inspect macvlan100

creates a new container called mactainer1 and connects it to the macvlan100 network.

$ docker run -d --name mactainer1 \
  --network macvlan100 \
  alpine sleep 1d

- troubleshooting connectivity problems 

can view them with the journalctl -u docker.service command. If you’re using a different init system

Ubuntu systems running upstart: /var/log/upstart/docker.log
RHEL-based systems: /var/log/messages
Debian: /var/log/daemon.log

- tell docker verbose daemon logging, daemon config file at /etc/docker/daemon.json and set "debug" to "true" and "log-level"

debug – the most verbose option
info – the default value and second-most verbose option
warn – third most verbose option
error – fourth most verbose option
fatal – least verbose option

{
  <Snip>
  "debug":true,
  "log-level":"debug",
  <Snip>
}

json-file and journald are probably the easiest to configure and they both work with the docker logs and docker service logs. a daemon.json shows a Docker host configured to use journald.

{
  "log-driver": "journald"
}

$ docker logs vantage-db

libnetwork also provides service discovery that allows all containers and Swarm services to locate each other by name.

use the --dns flag to start containers and services with a customized list of DNS servers, and you can use the --dns-search flag to add custom search

$ docker run -it --name custom-dns \
  --dns=8.8.8.8 \
  --dns-search=nigelpoulton.com \
  alpine sh

- ingress load balancing 

Swarm supports two ways of publishing services to external clients:

Ingress mode (default)
Host mode

$ docker service create -d --name svc1 \
  --publish published=5005,target=80,mode=host \
  nginx

looks like -p 5005:80 and you’ve seen it a few times already. However, you cannot publish a service in host mode using the short form.

as follows:

published=5005 makes the service available to external clients via port 5005
target=80 makes sure requests hitting the published port get mapped back to port 80 on service replicas
mode=host makes sure requests will only reach the service if they arrive on nodes running a service replica

- command to delete the services you created.

$ docker service rm svc1

delete the standalone containers you created.

$ docker rm c1 c2 client web mactainer1 

delete the networks you created.

$ docker network rm localnet macvlan100

- docker network command summary 

docker network ls lists all the Docker networks available to the host.
docker network create is how you create a new Docker network. You have to give the network a name and you can use the -d flag to specify which driver creates it.
docker network inspect provides detailed configuration information about Docker networks.
docker network prune deletes all unused networks on a Docker host.
docker network rm Deletes specific networks on a Docker host or swarm.

native Linux commands.

brctl show prints a list of all kernel bridges on the Docker host and shows if any containers are connected.
ip link show prints bridge configuration data. You ran an ip link show docker0 to see the configuration of the docker0 bridge on your Docker host.


# Docker overlay networking 
- sections 
Docker overlay networking – The TLDR
Docker overlay networking history
Building and testing overlay networks
Overlay networks explained

- docker overlay networking the TLDR a container networking startup called Socket Plane with two goals 

Bring overlay networking to Docker
Make container networking simple for developers

- build a swarm 

//node 1
$ docker swarm init

//node 2
$ docker swarm join \
  --token SWMTKN-1-0hz2ec...2vye \
  172.31.1.5:2377
This node joined a swarm as a worker.

have a two-node Swarm with node1 as a manager and node2 as a worker.

- create a network overlay 

create a new encrypted overlay network called uber-net.

$ docker network create -d overlay -o encrypted uber-net
vdu1yly429jvt04hgdm0mjqc6

 don’t specify the -o encrypted flag, Docker will still encrypt the control plane (management traffic) but won’t encrypt the data plane 

 $ docker network ls
NETWORK ID     NAME              DRIVER    SCOPE
65585dda7500   bridge            bridge    local
7e368a1105c7   docker_gwbridge   bridge    local
a38083cdab1c   host              host      local
4dsqo7jc36ip   ingress           overlay   swarm
d97e92a23945   none              null      local
vdu1yly429jv   uber-net          overlay   swarm   <<---- New overlay network

- Attach a container to the overlay network

deployed to node1 and the other to node2, causing Docker to extend the overlay network to node2.

$ docker service create --name test \
   --network uber-net \
   --replicas 2 \
   ubuntu sleep infinity

$ docker service ps test
ID          NAME    IMAGE           NODE      DESIRED STATE   CURRENT STATE
sm1...1nw   test.1  ubuntu:latest   node1     Running         Running
tro...kgk   test.2  ubuntu:latest   node2     Running         Running

Switch over to node2 and run a docker network ls to verify it can now see the uber-net network.

$ docker network inspect uber-net

the IP of the first replica called test.1.tro...kgk. If you run the same command on node2, you’ll see the name and IP of the other replica.

$ docker ps
CONTAINER ID   IMAGE           COMMAND           CREATED    STATUS     NAME
d7766923a5a7   ubuntu:latest   "sleep infinity"  2 hrs ago  Up 2 hrs   test.1.tro...kgk

$ docker inspect \
  --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' d7766923a5a7
10.0.0.3

containers and install the ping utility.

$ docker exec -it d7766923a5a7 bash

You’ll need to install traceroute in the container for this to work

- Docker uses VXLAN tunnels to create virtual layer 2 overlay networks. So, let’s do a quick VXLAN primer.

- clean up command to delete the test service.

$ docker service rm test
$ docker network rm uber-net

- Docker overlay networking – The commands
docker network create tells Docker to create a new network. You use the -d overlay flag to use the overlay driver to create an overlay network. You can also pass the -o encrypted flag to tell Docker to encrypt network traffic. However, performance may drop in the region of 10%.
docker network ls lists all the container networks visible to a Docker host. Docker hosts running in swarm mode only see overlay networks if they run containers attached to the network. This keeps network-related management traffic to a minimum.
docker network inspect shows detailed information about a particular container network. You can find out the scope, driver, IPv4 and IPv6 info, subnet configuration, IP addresses of connected containers, VXLAN network ID, encryption state, and more.
docker network rm deletes a network.


# Volumns and persisitent data 
- sections 
Volumes and persistent data – The TLDR
Containers without volumes
Containers with volumes
The commands

- Persistent data is the stuff you care about and need to keep. It includes things like customer records, financial data, research results, audit data, and even some types of logs

- Volumes are separate objects that you mount into containers, and they have their own lifecycles

- Docker creates containers by stacking read-only image layers and placing a thin layer of local storage on top.

if an application needs to update existing files or add new files, it makes the changes in the local storage layer, and Docker merges them into the view of the container.

docker keeps local storage layer on host:
Linux containers: /var/lib/docker/<storage-driver>/...
Windows containers: C:\ProgramData\Docker\windowsfilter\...

- Containers with volumes 

Volumes are independent objects that are not tied to the lifecycle of a container
You can map volumes to specialized external storage systems
Multiple containers on different Docker hosts can use volumes to access and share the same data

    + you can map the volume to an external storage system or a directory on the Docker host.

    + example create a new volume called myvol, You can use the -d flag to specify a different driver

$ docker volume create myvol

    + third party drivers, https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins

     Docker host connected to an external storage system via a plugin (driver).

    + check the volumes list 

$ docker volume ls 

$ docker volume inspect myvol
[
    {
        "CreatedAt": "2024-05-15T12:23:14Z",
        "Driver": "local",
        "Labels": null,
        "Mountpoint": "/var/lib/docker/volumes/myvol/_data",
        "Name": "myvol",
        "Options": null,
        "Scope": "local"
    }
]

scope local means local driver. mount point where the volume exists int the host's file system.

- create volumes 

$ docker volume create myvol

-  two ways to delete Docker volumes:

docker volume prune
docker volume rm

The docker volume prune --all command deletes all volumes not mounted into a container or service replica
The docker volume rm command is more precise and lets you specify which volumes to delete.
$ docker volume prune --all

- by default Docker gives every volume created with the local driver its own directory on the host under /var/lib/docker/volumes.

use the -d flag to specify a different driver, but you’ll need to install the driver first.

- Using volumes with containers

the --mount flag, telling Docker to mount a volume called bizvol into the container at /vol. 

$ docker run -it --name voltainer \
    --mount source=bizvol,target=/vol \
    alpine

specify a volume Docker will use it or created.

$ docker volume ls
$ docker volume rm bizvol

- deploy volumes via Dockerfiles by using the VOLUME instruction. The format is VOLUME <container-mount-point>. Interestingly, you cannot specify a host directory when you define volumes in a Dockerfile.

- use exec to run command in container to write on volumes 
```
$ docker exec -it voltainer sh

# echo "I promise to write a book review on Amazon" > /vol/file1

# ls -l /vol
total 4
-rw-r--r-- 1 root  root   50 May 23 08:49 file1

# cat /vol/file1
I promise to write a book review on Amazon

# exit

$ docker rm voltainer -f
voltainer

deleted the container but kept the volume.

$ docker ps -a

$ docker volume ls
DRIVER              VOLUME NAME
local               bizvol

# Check the file on host 
$ ls -l /var/lib/docker/volumes/bizvol/_data/
total 4
-rw-r--r-- 1 root root 50 Jan 12 14:25 file1

$ cat /var/lib/docker/volumes/bizvol/_data/file1
I promise to write a book review on Amazon

create a new container called newctr that mounts bizvol at /vol.

$ docker run -it \
  --name newctr \
  --mount source=bizvol,target=/vol \
  alpine sh

# cat /vol/file1
I promise to write a book review on Amazon
```

- you need to design applications that share data to coordinate updates to shared volumes. it will avoid shadow writting/reading on shared volumes.

- following command to delete the container.

$ docker rm 

- Volumes and persistent data – The Commands
docker volume create creates new volumes. By default, it creates them with the local driver, but you can use the -d flag to specify a different driver.
docker volume ls lists all volumes on your Docker host.
docker volume inspect shows you detailed volume information. You can use this command to see where a volume exists in the Docker host’s filesystem.
docker volume prune deletes all volumes not in use by a container or service replica. Use with caution!
docker volume rm deletes specific volumes that are not in use.


# Docker security 
- Docker security – The TLDR
Linux security technologies
Kernel namespaces
Control Groups
Capabilities
Mandatory Access Control
seccomp
Docker security technologies
Swarm security
Docker Scout and vulnerability scanning
Docker Content Trust
Docker secrets

- security layers 

Docker security {secrets}
                {docker content trust}
                {vulnerability scanning}
                {swarm mode}
Linux security  {seccomp}
                {mandatory access control, MAC}
                {capbilities}
                {contorl groups, cgroups}
                {kernel nampespace}
- own security technologies, including Docker Scout and Docker Content Trust.                
- Docker Content Trust (DCT) lets you cryptographically sign and verify images.
- namespaces, The host OS has its own collection of namespaces we call the root namespaces, and each container has its own collection of equivalent isolated namespaces.

Process ID (pid)
Network (net), every container gets its own eth0 interface with its own unique IP and range of ports.
Filesystem/mount (mnt)
Inter-process Communication (ipc), Docker uses the ipc namespace for shared memory access within a container.
User (user)
UTS (uts)

-  Linux root user is a combination of a long list of capabilities.

CAP_CHOWN: lets you change file ownership
CAP_NET_BIND_SERVICE: lets you bind a socket to low-numbered network ports
CAP_SETUID: lets you elevate the privilege level of a process
CAP_SYS_BOOT: lets you reboot the system.

- Docker uses seccomp to limit which syscalls a container can make to the host’s kernel.
- Docker Swarm lets you cluster multiple Docker hosts and manage applications declaratively

 include:

Cryptographic node IDs
TLS for mutual authentication
Secure join tokens
CA configuration with automatic certificate rotation
Encrypted cluster store
Encrypted networks

  + configure a secure swarm 

$ docker swarm init

  + Joining new managers is a two-step process:

Extract the secure join token
Execute a docker swarm join command with the join token on the node you’re adding

  + swarm maintains two distinct join tokens:

Manager token
Worker token

  + inspect a node’s client certificate on Linux with the following command.

$ sudo openssl x509 \
  -in /var/lib/docker/swarm/certificates/swarm-node.crt \
  -text

- Docker Scout is Docker’s native scanning platform and works with Docker Hub

- command signs a local image called nigelpoulton/ddd-trust:signed and pushes it to Docker Hub.

$ docker trust sign nigelpoulton/ddd-trust:signed
Signing and pushing trust data for local image nigelpoulton/ddd-trust:signed may...
The push refers to repository [docker.io/nigelpoulton/ddd-trust]
...

$ docker trust inspect nigelpoulton/ddd-trust:signed --pretty

export the DOCKER_CONTENT_TRUST variable with a value of 1 to force a Docker host to sign and verify all images.

$ export DOCKER_CONTENT_TRUST=1

pull an unsigned image.

$ docker pull nigelpoulton/ddd-book:web0.2

Delete the local copy of the image you just signed and pushed so that you can try pulling it from Docker Hub. Your image name will be different.

$ docker rmi nigelpoulton/ddd-trust:signed

pulling the image.

$ docker pull nigelpoulton/ddd-trust:signed

-  Secrets only work in swarm mode as they leverage the cluster store.

can create and manage secrets with the docker secret command and attach them to services by passing the --secret flag to the docker service create

$ docker secret

- clean up disable docker content trust 

$ unset DOCKER_CONTENT_TRUST

Remove the signer from the repository you created. Your signer and repository will have different names.

$ docker trust signer remove nigel nigelpoulton/ddd-trust

 run it on the swarm managers last.

$ docker swarm leave -f