Data Science from Scratch, 2nd Edition=Joel Grus;Note=Erxin

# Crash course in python 


# Visualizing data 
- plot line

from matplotlib import pyplot as plt

years = [1950, 1960, 1970, 1980, 1990, 2000, 2010]
gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3]

# create a line chart, years on x-axis, gdp on y-axis
plt.plot(years, gdp, color='green', marker='o', linestyle='solid')

# add a title
plt.title("Nominal GDP")

# add a label to the y-axis
plt.ylabel("Billions of $")
plt.show()

- bar 

movies = ["Annie Hall", "Ben-Hur", "Casablanca", "Gandhi", "West Side Story"]
num_oscars = [5, 11, 3, 8, 10]

# plot bars with left x-coordinates [0, 1, 2, 3, 4], heights [num_oscars]
plt.bar(range(len(movies)), num_oscars)

plt.title("My Favorite Movies")     # add a title
plt.ylabel("# of Academy Awards")   # label the y-axis

# label x-axis with movie names at bar centers
plt.xticks(range(len(movies)), movies)

plt.show()

- line charts

variance     = [1, 2, 4, 8, 16, 32, 64, 128, 256]
bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1]
total_error  = [x + y for x, y in zip(variance, bias_squared)]
xs = [i for i, _ in enumerate(variance)]

# We can make multiple calls to plt.plot
# to show multiple series on the same chart
plt.plot(xs, variance,     'g-',  label='variance')    # green solid line
plt.plot(xs, bias_squared, 'r-.', label='bias^2')      # red dot-dashed line
plt.plot(xs, total_error,  'b:',  label='total error') # blue dotted line

# Because we've assigned labels to each series,
# we can get a legend for free (loc=9 means "top center")
plt.legend(loc=9)
plt.xlabel("model complexity")
plt.xticks([])
plt.title("The Bias-Variance Tradeoff")
plt.show()

- scatterplots 

friends = [ 70,  65,  72,  63,  71,  64,  60,  64,  67]
minutes = [175, 170, 205, 120, 220, 130, 105, 145, 190]
labels =  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']

plt.scatter(friends, minutes)

# label each point
for label, friend_count, minute_count in zip(labels, friends, minutes):
    plt.annotate(label,
        xy=(friend_count, minute_count), # Put the label with its point
        xytext=(5, -5),                  # but slightly offset
        textcoords='offset points')

plt.title("Daily Minutes vs. Number of Friends")
plt.xlabel("# of friends")
plt.ylabel("daily minutes spent on the site")
plt.show()


# Linear algebra 


# Statistics 
- covariance measures how two variables vary in tandem from their means

- at the correlation, which divides out the standard deviations of both variables:



# Probability 
-  P(E) to mean “the probability of the event E.”
- Dependence and Independence
- Conditional Probability

P(E, F) = P(E|F)P(F)

- bayes's theorem 

P(E|F) = P(E, F)/P(F) = P(F|E) P(E)/P(F)

- random variables 

A random variable is a variable whose possible values have an associated probability distribution. 

- continuous distributions 

A coin flip corresponds to a discrete distribution—one that associates positive probability with discrete outcomes. 

uniform distribution puts equal weight on all the numbers between 0 and 1.

normal distribution 

f(x|__mul__, __delta__) = 1/(2 * pi * __delta__)^0.5 * e^(-(x - __mul__)^2/2*__delta__^2)

central limit theorem 

1/n(x1+...+xn)

(x1+...+xn) - __mul__ * n / (__delta__ * (n)^0.5)



# Hypothesis and inference 
- involves p-values. Instead of choosing bounds based on some probability cutoff, we compute the probability—assuming is true—that we would see a value at least as extreme as the one we actually observed.

- confidence intervals 

- A procedure that erroneously rejects the null hypothesis only 5% of the time will—by definition—5% of the time erroneously reject the null hypothesis



# Simple linear regression 
- example 
```
from typing import Tuple
from scratch.linear_algebra import Vector
from scratch.statistics import correlation, standard_deviation, mean

def least_squares_fit(x: Vector, y: Vector) -> Tuple[float, float]:
    """
    Given two vectors x and y,
    find the least-squares values of alpha and beta
    """
    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)
    alpha = mean(y) - beta * mean(x)
    return alpha, beta
```

- maximum likelihood estimation 

L(__alpha__, __beta__|xi, yi, __delta__) = 1/((2 * pi)^0.5 * __delta__ ) * e^(-(yi - __alpha__ - __beta__ * xi)^2/(2 * __delta__^2))



# Multiple regression 
- example, least squares fit 

import random
import tqdm
from scratch.linear_algebra import vector_mean
from scratch.gradient_descent import gradient_step


def least_squares_fit(xs: List[Vector],
                      ys: List[float],
                      learning_rate: float = 0.001,
                      num_steps: int = 1000,
                      batch_size: int = 1) -> Vector:
    """
    Find the beta that minimizes the sum of squared errors
    assuming the model y = dot(x, beta).
    """
    # Start with a random guess
    guess = [random.random() for _ in xs[0]]

    for _ in tqdm.trange(num_steps, desc="least squares fit"):
        for start in range(0, len(xs), batch_size):
            batch_xs = xs[start:start+batch_size]
            batch_ys = ys[start:start+batch_size]

            gradient = vector_mean([sqerror_gradient(x, y, guess)
                                    for x, y in zip(batch_xs, batch_ys)])
            guess = gradient_step(guess, gradient, -learning_rate)

    return guess


# Logistic regression 
- logistic function 

def logistic(x: float) -> float:
    return 1.0 / (1 + math.exp(-x))

- logistic prime 

def logistic_prime(x: float) -> float:
    y = logistic(x)
    return y * (1 - y)

- applying the model 

from scratch.machine_learning import train_test_split
import random
import tqdm

random.seed(0)
x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)

learning_rate = 0.01

# pick a random starting point
beta = [random.random() for _ in range(3)]

with tqdm.trange(5000) as t:
    for epoch in t:
        gradient = negative_log_gradient(x_train, y_train, beta)
        beta = gradient_step(beta, gradient, -learning_rate)
        loss = negative_log_likelihood(x_train, y_train, beta)
        t.set_description(f"loss: {loss:.3f} beta: {beta}")

- entropy

“how much information” with entropy. You have probably heard this term used to mean disorder

H(S) = -p1 * log2(p1) - ... - pn * log2(pn)

pi is the proportion of data labeled as class ci

term -pi*log2 pi


# Neural networks 


# Clustering 
- cluster 

from typing import List
from scratch.linear_algebra import vector_mean

def cluster_means(k: int,
                  inputs: List[Vector],
                  assignments: List[int]) -> List[Vector]:
    # clusters[i] contains the inputs whose assignment is i
    clusters = [[] for i in range(k)]
    for input, assignment in zip(inputs, assignments):
        clusters[assignment].append(input)

    # if a cluster is empty, just use a random point
    return [vector_mean(cluster) if cluster else random.choice(inputs)
            for cluster in clusters]
- k-means clustering, which can partition the pixels into five clusters in red-green-blue space


# Natural language processing 
- word clouds 
- n-gram language model 
- A different approach to modeling language is with grammars, rules for generating acceptable sentences.
- topic modeling 
- word vector 
- recurrent neural networks 


# Network analysis 
- eigenvector centrality 
- directed graphs and pagerank 
There is a total of 1.0 (or 100%) PageRank in the network.

Initially this PageRank is equally distributed among nodes.


# Recommender systems 
- matrix based similarity 

# Databases and sql 
- sql 

SELECT MIN(user_id) AS min_user_id FROM
(SELECT user_id FROM user_interests WHERE interest = 'SQL') sql_interests;


# map reduce 
- 


